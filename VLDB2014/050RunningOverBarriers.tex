So far, our system design distributes our data and the model we are calculating
so that our system is memory efficient and highly scalable.  We also show how
even in our distributed system, we can answer complex queries through a variety
of machine learning models.  However, the design as described so far has one
weakness - the system processes subepochs synchronously with a barrier between
each subepoch.  While theoretically this may not be a big deal, in real world
clusters some machines may be worse or shared with other jobs resulting in
slower computation for our algorithm.  This results in some machines being
available but being forced to wait around and waste valuable computing time.

To overcome this bottleneck, we developed a technique called \textit{Always-On
SGD}, where we let machines keep running when they hit a barrier.
More specifically, when a machine hits a barrier and needs to wait on other
machines to reach the barrier, the machine can keep making progress on the
problem and performing valuable computation.  At the point of hitting the
barrier, the machine has processed and performed updates from all of the points
in some block $\mathcal{B}$.  But, because it is an iterative algorithm, and
better yet a stochastic one, we can shuffle the points in $\mathcal{B}$ and
process them again.  In processing the points again we continue to take
valuable steps with the gradient toward the solution.  Because we expect we are
getting closer to the solution, we begin to decrease the step size of our
updates, but this is normal behavior as a gradient descent algorithm runs.
We can keep using the points in $\mathcal{B}$ to perform updates until all of
the machines have reached the barrier and are ready to proceed to the next
subepoch.

We have recently proved \cite{aistats} that Always-On SGD still converges and
\ldots.  
We will discuss in Section \ref{sec:eval} the comparison in
convergence time using this method compared to the synchronous version, as well
as show the amount of time wasted when Always-On is not used.
With this set of design decisions, our system is scalable, versatile, and fast.  
\alex{abhi, can you add to the first sentence to this last paragraph about the
impact of our proof.}


%To implement our framework, we build it on top of Apache's Hadoop and Hadoop
%File System (HDFS).  Although this platform is typically not geared towards
%iteratitve distributed machine learning algorithms, we demonstrate that the
%tools in the system offer many advantages such as ease to set up, robustness,
%and ubiquity in industry, while still enabling our framework to work accurately
%and efficiently.
%
%\subsection{Ordering the data} % (fold)
%\label{sub:Ordering the data}
%
%
%\subsection{Computation} % (fold)
%\label{sub:Computation}
%
%
%\subsection{Model Synchronization} % (fold)
%\label{sub:Synchronization}
%
%\paragraph{Parameter Movement} % (fold)
%\label{par:Model Movement}
%
%
%\paragraph{Distributed Projections} % (fold)
%\label{par:Distributed Projections}
%
%
%\subsection{Threading Always-On SGD} % (fold)
%\label{sub:Threading Always-On SGD}
%
%% subsection Threading Always-On SGD (end)
