%So far our system design distributes our data and the model we are calculating
%so that our system is memory efficient, avoids contention over variables, and thus highly scalable.  We also show how
%even in our distributed system, we can answer complex queries through a variety
%of machine learning models.  However, 
The system as described so far has one
weakness - the system processes subepochs synchronously with a barrier between
each subepoch.  While theoretically this may not be a big deal, in real world
clusters some machines may be worse or shared with other jobs resulting in
stragglers and slower computation for our algorithm.  This results in some
machines being under-utilized, forced to wait around and waste valuable
computing time and power.

To overcome this bottleneck, we developed a technique called \textit{Always-On
SGD}, where we let machines keep running when they hit a barrier.
More specifically, when a machine hits a barrier and needs to wait on other
machines to pass the barrier, the machine can keep making progress on the
problem and performing valuable computation.  Under our relational block structure, when a machine hits a
barrier, it has processed and performed updates from all of the points
in some block $\mathcal{B}$.  However, because it is an iterative algorithm, and
better yet a stochastic one, we can shuffle the points in $\mathcal{B}$ and
process them again.  In processing the points again we continue to take
valuable steps with the gradient toward the solution.  Because we expect we are
getting closer to the solution, we begin to decrease the step size of our
updates, but this is normal behavior as a gradient descent algorithm runs.
We can keep using the points in $\mathcal{B}$ to perform updates until all of
the machines have reached the barrier and are ready to proceed to the next
subepoch.

We have recently proved\footnote{Our proof is in a different paper, currently
under double-blind review.  The appropriate citation will be included when
published.} that Always-On SGD still converges and
\ldots.  
We will discuss in Section \ref{sec:eval} the comparison in
convergence time using this method compared to the synchronous version, as well
as show the amount of time wasted when Always-On is not used.
With this set of design decisions, our system is scalable, versatile, and fast.  
\alex{abhi, can you add to the first sentence to this last paragraph about the
impact of our proof.}
