So far, our system can successfully separate computation on different parts
of the data and distribute it over a cluster.  As was described previously,
this design works for simple computation such as matrix
factorization, but most questions we have, such as sparse non-negative matrix
factorization or topic modeling, are more complex and require a more robust
system.  Handling more complex questions is crucial for modern machine
learning, so we must build on our system to make it versatile.

\subsection{Projecting into the solution space} % (fold)
\label{sub:Projecting into the solution space}


In our topic modeling problem, 
%question when we want to calculate
%the probability that a document is about a given topic, 
the probabilities that a document is about different topics must be non-negative
and sum to 1.  
Therefore, in using the iterative learning methods outlined above, we must
continuously project our current model into the appropriate solution
space as we perform updates.
For example, to make sure the model is non-negative we can always project
negative values to 0.  To make sure the probabilities sum to 1, we can
occasionally normalize the values.

%To stay within this space, we can
%always set parameters to 0 if they go negative, and normalize our probabilities
%when they do not sum to 1 after an update.
\textit{Projections} of this form are quite versatile and enable our system to
handle a wide variety of complex questions and learn many classic machine
learning models.
Additionally, we recently proved in \cite{flexifact} that our distributed
parallel system still provably converges even when performing projections
during SGD.
We outline a few examples of useful projections below.

%To handle complex questions in our system, we need to be able to
%perform \textit{projections} during our computation.  

%Projections make sure our
%solutions are of the desired form and cover a wide range of machine learning
%problems.  

%{\bf Non-negative solutions}

%{\bf Sparse models}

%{\bf Simplex constraints}

%{\bf Distributed Simplex Constraints}

\paragraph{Non-negative solutions} % (fold)
\label{par:Non-negative solutions}

% paragraph Non-negative solutions (end)

\paragraph{Sparse Models} % (fold)
\label{par:Sparse Models}

% paragraph Sparse Models (end)

\paragraph{Simplex Constraints} % (fold)
\label{par:Simplex Constraints}

% paragraph Simplex Constraints (end)

\paragraph{Distributed Simplex Constraints} % (fold)
\label{par:Simplex Constraints}

% paragraph Simplex Constraints (end)


\subsection{Distributed Synchronization} % (fold)
\label{sub:Distributed Synchronization}

% subsection Distributed Synchronization (end)



%From sparse non-negative matrix factorization to topic modeling, 


%We now outline the our approach to using SGD for machine learning at a large
%scale while maintaining its speed.
%
%\subsection{Data and Model Partitioning} % (fold)
%\label{sub:partition}
%
%% subsection Data Blocking (end)
%
%\subsection{Movement of Data and Parameters}
%\label{sub:flow}
%
%% subsection Flow of data  (end)
%
%%\subsection{Model Distribution} % (fold)
%%\label{sub:Model Distribution}
%
%%% subsection Model Distribution (end)
%
%\subsection{Synchronization} % (fold)
%\label{sub:Synchronization}
%
%% subsection Synchronization (end)
%
%\subsection{Always-On SGD} % (fold)
%\label{sub:Always-On SGD}
%
%% subsection Always-On SGD (end)

