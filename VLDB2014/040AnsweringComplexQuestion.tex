So far, our system can successfully separate computation on different parts
of the data and distribute it over a cluster.  As was described previously,
this design works for simple computation such as matrix
factorization, but most questions we have, such as sparse non-negative matrix
factorization or topic modeling, are more complex and require a more robust
system.  Handling more complex questions is crucial for modern machine
learning, so we must build on our system to make it versatile.

\subsection{Projecting into the solution space} % (fold)
\label{sub:Projecting into the solution space}


In our topic modeling problem, 
%question when we want to calculate
%the probability that a document is about a given topic, 
the probabilities that a document is about different topics must be non-negative
and sum to 1.  
Therefore, in using the iterative learning methods outlined above, we must
continuously project our current model into the appropriate solution
space as we perform updates.
For example, to make sure the model is non-negative we can always project
negative values to 0.  To make sure the probabilities sum to 1, we can
occasionally normalize the values.

%To stay within this space, we can
%always set parameters to 0 if they go negative, and normalize our probabilities
%when they do not sum to 1 after an update.
\textit{Projections} of this form are quite versatile and enable our system to
handle a wide variety of complex questions and learn many classic machine
learning models.
Additionally, we recently proved in \cite{flexifact} that our distributed
parallel system still provably converges even when performing projections
during SGD.
We outline a few examples of useful projections below.

%To handle complex questions in our system, we need to be able to
%perform \textit{projections} during our computation.  

%Projections make sure our
%solutions are of the desired form and cover a wide range of machine learning
%problems.  

%{\bf Non-negative solutions}

%{\bf Sparse models}

%{\bf Simplex constraints}

%{\bf Distributed Simplex Constraints}

\paragraph{Non-negative solutions} % (fold)
\label{par:Non-negative solutions}
One of the simplest constraints requires that our model be non-negative.  This
is useful for a number of different reasons.  In topic modeling, as described
above, our model represents a set of probabilities and thus should be
non-negative.  More simply, it is common to want a non-negative matrix
factorization to make the results actually interpretable.

As was mentioned above, we can enforce non-negativity constraints by simply
projecting negative values to 0:
\begin{align}
	\Theta(x) =
	\left\{
	\begin{array}{ll}
		x  & \mbox{if } x \geq 0 \\
		0 & \mbox{if } x < 0
	\end{array}
	\right.
\end{align}
Therefore, whenever we update a part of the model, say $\beta_{k,j}$, the
probability that word $j$ is in topic $k$, we would perform the projection on
that parameter, $\beta_{k,j} = \Theta(\beta_{k,j})$.  This is simple as it can
be included in the local update.

% paragraph Non-negative solutions (end)

\paragraph{Sparse Models} % (fold)
\label{par:Sparse Models}
Another common constraint is to desire a sparse model, such that most of our
model is zero.  In the context of topic modeling, this would mean that we want
to push each document to be about only a few topics (and thus the probability
that the document is about most of the topics is 0).  If we have a set of 500
or 1000 topics, this makes it easier to interpret what topics a document is
about.  It is common to enforce sparsity constraints in everything from basic matrix
factorization to more complex topic models.  

To encourage sparsity, mathematically, we include an $\ell_1$-norm penalty on
our model in the objective function.  In more practical terms, this
$\ell_1$-penalty results in a \textit{soft-thresholding} of our parameters.
That is, whenever we update our parameters we push them toward zero with the
soft-thresholder:
\begin{align}
	S_\lambda(x) =
	\left\{
	\begin{array}{ll}
		x-\lambda  & \mbox{if } x > \lambda \\
		x+\lambda  & \mbox{if } x < -\lambda \\
		0 & \mbox{if } -\lambda \leq x \leq \lambda
	\end{array}
	\right.
\end{align}
In our system, we would apply this projection whenever we update any parameters
for which there is an $\ell_1$ penalty on that part of the model.  We will show
later that this is particularly useful for dictionary learning.

% paragraph Sparse Models (end)

\paragraph{Simplex Constraints} % (fold)
\label{par:Simplex Constraints}
In machine learning problems, we are most often calculating probabilities to
estimate other probabilities.  As explained before, we use the probability that
a document is about a topic, and the probability that a word is from topic, to
estimate the probability that a word is in a document.
Unfortunately, many of the learning methods above do
not naturally treat the values they are learning as probabilities, so we need
to continuously enforce the constraints to keep the results focused.

To do this, we enforce simplex constraints - that certain sets of values sum to
1 - as we saw in the topic modeling definition above.  In particular, for topic
modeling for each document $i$, the vector of probabilities of topics for that
document should sum to 1: $\sum_k \pi_{i,k} = 1$. 
Luckily, when using SGD to process data point $Y_{i,j}$ we see that we update the
probabilities for document $i$ and word $j$.  As a result, we can simply
normalize row $\pi_i$ whenever we perform and update on it:
\begin{align}
	\pi_i = \frac{\Theta(\pi_i)}{\|\Theta(\pi_i)\|_1}
\end{align}
As we will show later, this is very useful in a variety of machine learning
applications.

% paragraph Simplex Constraints (end)

\paragraph{Projecting on the unit ball}
A similar, but slightly different constraint is that our result vectors must fall within the unit ball.
As a result, when a vector is updated to a length $>1$ we must project the
vector back onto the unit ball.
This is generally of a similar form as the simplex constraint above, but is
used in dictionary learning.  The projection function here is
\begin{align}
	b(x) = 
	\left\{
	\begin{array}{ll}
		x  & \mbox{if } \|x\|_2 \leq 1 \\
		\frac{x}{\|x\|_2} & \mbox{otherwise}
	\end{array}
	\right.
\end{align}

\paragraph{Distributed Simplex Constraints} % (fold)
\label{par:Simplex Constraints}
So far we have focused on local constraints, where our projections only effect
the part of the model being updated from the current data point.  However, this
is not always the case.  For example, in topic modeling, we also want to know
what is the probability that a word is used when discussing a given topic.
This is coded as a vector of probabilities of words for each topic, $\beta_k$.

When we observe a data point $Y_{i,j}$ and update the
probabilities associated with word $j$, we break the simplex requirement for
all of the topic-word vectors.  As with the previous simplex constraints, we would
like to re-normalize the values in each topic-word vector.
Unfortunately, in our distributed block model, the
probabilities of all of the words for a topic $k$ are distributed over the
entire cluster, thus making normalization a distributed process.  

In \cite{flexifact} we showed that when using SGD, we can normalize these
vectors not even on every updated but every so often and still converge.
%In practice after each subepoch we perform a distributed synchronized normalization.  
We describe the details of how we do this in practice in the following section.

% paragraph Simplex Constraints (end)


\subsection{Distributed Normalization} % (fold)
\label{sub:Distributed Normalization}
As described above, performing local projections is relatively straightforward
and we have proved in the past that our method converges under such
projections.  However, in many cases we need to coordinate our parameters
across our machines, such as when we want to normalize across machines.

In practice, we perform our distributed normalize after every subepoch.  To be
more specific, in a given subepoch we wait for all machines to finish the
updates based on the data in their current block.  Once all machines have
finished their updates, we normally would transfer the appropriate parts of the
factor matrices to the appropriate machines.  When performing a distributed
normalization, each machine takes the sum of their relevant vectors and stores
them separately for communication.  To be more specific, in topic modeling each
machine would want the sum of values for a topic $k$ and thus takes the sum the
values of $\pi_{i,k}$ for all words $i$ stored on that machine.  This results
in a $K$-length vector of sums on each machine.  While the larger factor
matrices are being passed transferred, the machines also distribute their
sum-vector to every machine.  The movement of data after a subepoch can be seen
in Figure \ref{fig:distributed-normalization}.

Therefore, when a machine receives its new factor matrices and {\it all} of the
sum-vectors (one from every machine in the cluster), it can perform the
appropriate normalization.  That is, the machine can sum the sum-vectors
leaving a normalization term $n_k$ for each topic $k$.  As such, in
reading in the new factor matrices, the machine can normalize the terms by
setting $\pi_{i,k} = \pi_{i,k}/n_k$.  As a result, each subepoch starts with
the simplex constraint being met for all topics $k$: $\sum_i \pi_{i,k} = 1$.

As mentioned earlier, this projection is of course less frequent and more
expensive than the local projections described earlier.  However, as proven in
\cite{flexifact} the algorithm still converges appropriately.

By being able to perform projections and still provably converge, our algorithm
can answer a variety of questions and handle a wide range of machine learning
applications, as will be discussed in Section \ref{sec:applications} and
demonstrated in Section \ref{sec:eval}.


% subsection Distributed Synchronization (end)



%From sparse non-negative matrix factorization to topic modeling, 


%We now outline the our approach to using SGD for machine learning at a large
%scale while maintaining its speed.
%
%\subsection{Data and Model Partitioning} % (fold)
%\label{sub:partition}
%
%% subsection Data Blocking (end)
%
%\subsection{Movement of Data and Parameters}
%\label{sub:flow}
%
%% subsection Flow of data  (end)
%
%%\subsection{Model Distribution} % (fold)
%%\label{sub:Model Distribution}
%
%%% subsection Model Distribution (end)
%
%\subsection{Synchronization} % (fold)
%\label{sub:Synchronization}
%
%% subsection Synchronization (end)
%
%\subsection{Always-On SGD} % (fold)
%\label{sub:Always-On SGD}
%
%% subsection Always-On SGD (end)

