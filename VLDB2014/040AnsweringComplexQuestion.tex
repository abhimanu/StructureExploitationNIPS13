So far, our system can successfully separate computation on different parts
of the data and distribute it over a cluster.  As was described previously,
this design works for simple computation such as matrix
factorization, but most questions we have, such as sparse non-negative matrix
factorization or topic modeling, are more complex and require a more robust
system.  Handling more complex questions is crucial for modern machine
learning, so we must build on our system to make it versatile.

\subsection{Projecting into the solution space} % (fold)
\label{sub:Projecting into the solution space}


In our topic modeling problem, 
%question when we want to calculate
%the probability that a document is about a given topic, 
the probabilities that a document is about different topics must be non-negative
and sum to 1.  
Therefore, in using the iterative learning methods outlined above, we must
continuously project our current model into the appropriate solution
space as we perform updates.
For example, to make sure the model is non-negative we can always project
negative values to 0.  To make sure the probabilities sum to 1, we can
occasionally normalize the values.

%To stay within this space, we can
%always set parameters to 0 if they go negative, and normalize our probabilities
%when they do not sum to 1 after an update.
\textit{Projections} of this form are quite versatile and enable our system to
handle a wide variety of complex questions and learn many classic machine
learning models.
Additionally, we recently proved in \cite{flexifact} that our distributed
parallel system still provably converges even when performing projections
during SGD.
We outline a few examples of useful projections below.

%To handle complex questions in our system, we need to be able to
%perform \textit{projections} during our computation.  

%Projections make sure our
%solutions are of the desired form and cover a wide range of machine learning
%problems.  

%{\bf Non-negative solutions}

%{\bf Sparse models}

%{\bf Simplex constraints}

%{\bf Distributed Simplex Constraints}

\paragraph{Non-negative solutions} % (fold)
\label{par:Non-negative solutions}
One of the simplest constraints requires that our model be non-negative.  This
is useful for a number of different reasons.  In topic modeling, as described
above, our model represents a set of probabilities and thus should be
non-negative.  More simply, it is common to want a non-negative matrix
factorization to make the results actually interpretable.

As was mentioned above, we can enforce non-negativity constraints by simply
projecting negative values to 0:
\begin{align}
	\Theta(x) =
	\left\{
	\begin{array}{ll}
		x  & \mbox{if } x \geq 0 \\
		0 & \mbox{if } x < 0
	\end{array}
	\right.
\end{align}
Therefore, whenever we update a part of the model, say $\beta_{k,j}$, the
probability that word $j$ is in topic $k$, we would perform the projection on
that parameter, $\beta_{k,j} = \Theta(\beta_{k,j})$.  This is simple as it can
be included in the local update.

% paragraph Non-negative solutions (end)

\paragraph{Sparse Models} % (fold)
\label{par:Sparse Models}
Another common constraint is to desire a sparse model, such that most of our
model is zero.  In the context of topic modeling, this would mean that we want
to push each document to be about only a few topics (and thus the probability
that the document is about most of the topics is 0).  If we have a set of 500
or 1000 topics, this makes it easier to interpret what topics a document is
about.  It is common to enforce sparsity constraints in everything from basic matrix
factorization to more complex topic models.  

To encourage sparsity, mathematically, we include an $\ell_1$-norm penalty on
our model in the objective function.  In more practical terms, this
$\ell_1$-penalty results in a \textit{soft-thresholding} of our parameters.
That is, whenever we update our parameters we push them toward zero with the
soft-thresholder:
\begin{align}
	S_\lambda(x) =
	\left\{
	\begin{array}{ll}
		x-\lambda  & \mbox{if } x > \lambda \\
		x+\lambda  & \mbox{if } x < -\lambda \\
		0 & \mbox{if } -\lambda \leq x \leq \lambda
	\end{array}
	\right.
\end{align}
In our system, we would apply this projection whenever we update any parameters
for which there is an $\ell_1$ penalty on that part of the model.  We will show
later that this is particularly useful for dictionary learning.

% paragraph Sparse Models (end)

\paragraph{Simplex Constraints} % (fold)
\label{par:Simplex Constraints}
In machine learning problems, we are most often calculating probabilities to
estimate other probabilities.  However, many of the learning methods above do
not naturally treat the values they are learning as probabilities, so we need
to continuously enforce the constraints to keep the results focused.

% paragraph Simplex Constraints (end)

\paragraph{Projecting in the unit ball}

\paragraph{Distributed Simplex Constraints} % (fold)
\label{par:Simplex Constraints}

% paragraph Simplex Constraints (end)


\subsection{Distributed Synchronization} % (fold)
\label{sub:Distributed Synchronization}

% subsection Distributed Synchronization (end)



%From sparse non-negative matrix factorization to topic modeling, 


%We now outline the our approach to using SGD for machine learning at a large
%scale while maintaining its speed.
%
%\subsection{Data and Model Partitioning} % (fold)
%\label{sub:partition}
%
%% subsection Data Blocking (end)
%
%\subsection{Movement of Data and Parameters}
%\label{sub:flow}
%
%% subsection Flow of data  (end)
%
%%\subsection{Model Distribution} % (fold)
%%\label{sub:Model Distribution}
%
%%% subsection Model Distribution (end)
%
%\subsection{Synchronization} % (fold)
%\label{sub:Synchronization}
%
%% subsection Synchronization (end)
%
%\subsection{Always-On SGD} % (fold)
%\label{sub:Always-On SGD}
%
%% subsection Always-On SGD (end)

