The model and data abstraction defined in section~\ref{sec:mdAbstract} is a
convenient view for distributed computing of a large scale machine learning
problem. This abstraction can be separated logically from the way the learning
algortihm of the problem. The partition abstraction is used to distribute the
learning phase over different computig units. For example in the \lda problem
discussed in the abstraction earlier, we can use various learning algorithms to
reach the solution. In particular we can use either of the two general
learning schemes: 1) Sampling based learning, 2) Gradient based learning. These
two learning schemes differ in the way they treat the given problem.  

\subsection{Sampling based learning scheme}
Sampling based approach in general takes a probabilistic view of the problem.
It learns the parameters of the model by approximating the probability
distribution of the true data. This scheme samples
iteratively from a changing distribution that is affected by the samples drawn.
The distribution eventually reaches an equilibrium. The parameters of this
stationary distribution are our solutions (parameters of the model) and are used
further down the pipeline for prediction. In case of \lda such a scheme
progresses iteratively. We start with some initial values of $\pi$ and $\beta$
defined in equation~\ref{eqn:LDA}. We assume that each word occurrence has a hidden
indicator $z$ (an artificial construct of the learning paradigm) that provides
the topic it lies in. We sample values of $z$ for each word occurrence
over all the documents based on $\pi$ and $\beta$ values. These $z$s are
used to obtain new values for $\pi$ and $\beta$. This goes on iteratively until
the distribution that is characterize by $\pi$ and $\beta$ is not changing
anymore.
% \abhi{explain the
% gibbs sampling steps for \lda here. For that we need to discribe the objective
% function for \lda in section 2}.
\subsection{Gradient based learning}
In gradient based learning an objective function is optimized to find the
parameters of the model. The optimization procedure is formulated in a way to
abstract away the probabilistic components of the model if any present. For example
incase of \lda we optimize objective defined in equation~\ref{eqn:LDA} by
computing the derivatives $\nabla L_{\pi}$ and $\nabla L_{\beta}$ with respect
to $\pi$ and $\beta$ respectively. The new values $\pi^{'}$ and $\beta^{'}$ are
\begin{align*}
\pi^{'} = \pi - \eta\nabla L_{\pi} \\ 
\beta^{'} = \beta - \eta\nabla L_{\beta}
\end{align*}
Here beta is the step size of learning which is a constant provided by the
algorithm.
\abhi{again define }

\subsubsection{Stochastic gradient descent}



%In this section, we describe how optimization techniques, and in particular
%stochastic gradient descent, can be used to fit a variety of machine learning
%models.
%
%\subsection{SGD Background} % (fold)
%\label{sub:SGD Background}
%
%% subsection SGD Background (end)
%
%\subsection{Dictionary Learning} % (fold)
%\label{sub:Dictionary Learning}
%
%% subsection Dictionary Learning (end)
%
%\subsection{Topic Modeling} % (fold)
%\label{sub:Topic Modeling}
%
%% subsection Topic Modeling (end)
%
%\subsection{Mixed Membership Stochastic Block Models} % (fold)
%\label{sub:Mixed Membership Stochastic Block Models}
%
%% subsection Mixed Membership Stochastic Block Models (end)
