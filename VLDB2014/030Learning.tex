The model and data abstraction defined in Section~\ref{sec:mdAbstract} is a
convenient view for distributed computing of a large scale machine learning (ML)
problem. 
%This abstraction can be separated logically from the way the learning is achieved in an ML algorithm. 
The partition abstraction is used to distribute the learning phase over different computing units. 
%For example in the \lda problem
%discussed in the abstraction Section~\ref{sec:mdAbstract} earlier, we can use
%different learning algorithms to obtain the solution. 
We can then use either of the two general learning schemes to learn model variables on each block: 1) Sampling based learning, 2) Gradient based learning. 
%These two learning schemes differ in the way they treat the given problem.  

\subsection{Sampling based learning scheme}

Sampling based approaches take a probabilistic view of the problem.
They learn the parameters of the model by approximating the probability
distribution of the true data. This scheme iteratively {\it samples randomly}
from a changing distribution that is affected by the samples drawn.
The distribution eventually reaches an equilibrium. The parameters of this
stationary distribution are our solutions (parameters of the model) and are used
further down the pipeline for prediction. 
%In case of \lda such a scheme
%progresses iteratively. We start with some initial values of $\pi$ and $\beta$
%defined in equation~\ref{eqn:LDA}. We assume that each word occurrence has a hidden
%indicator $z$ (an artificial construct of the learning paradigm) that provides
%the topic it lies in. We sample values of $z$ for each word occurrence
%over all the documents based on $\pi$ and $\beta$ values. These $z$s are
%used to obtain new values for $\pi$ and $\beta$. This goes on iteratively until
%the distribution that is characterize by $\pi$ and $\beta$ is not changing
%anymore.

% \abhi{explain the
% gibbs sampling steps for \lda here. For that we need to discribe the objective
% function for \lda in section 2}.

\subsection{Gradient based learning}
In gradient based learning an objective function is minimized to find the
parameters of the model. The optimization procedure is formulated in a way to
abstract away the probabilistic components of the model, if any present. For
example in the case of \lda we optimize the objective $L$ defined in \eqref{eqn:LDA} by
computing the derivatives $\partial L_{\pi}$ and $\partial L_{\beta}$ with respect
to $\pi$ and $\beta$ respectively. 
The new value $\pi^{'}$ is
%to $\pi$ and $\beta$ respectively. The new values $\pi^{'}$ and $\beta^{'}$ are
\begin{align*}
\pi^{'}_{i,k} = \pi_{i,k} - \eta\partial L_{\pi_{i,k}}  
%\;\;\;\;\;{\rm and }\;\;\;\;\;
%\beta^{'}_{j,k} = \beta_{j,k} - \eta\partial L_{\beta_{j,k}}
%{\rm where}\hspace{1.5cm}&\\
\end{align*}
where for topic modeling
\begin{align*}
\partial L_{\pi_{i,k}} &= 2*\sum_{j}(Y_{i,j} - \sum_{k}\pi_{i_k}\beta_{j,k})\beta_{j,k}. 
%&{\rm and}\\
%\\ \partial L_{\beta_{j,k}} &= 2*\sum_{i}(Y_{i,j} - \sum_{k}\pi_{i,k}\beta_{j,k})\pi_{i,k}
\end{align*}
Here $\eta$ is the step size in the learning scheme which is a constant
provided by the algorithm. (The update equation for $\beta_{j,k}$ can be found
similarly.)
%\abhi{convert $\pi$ and $\beta$ into more databasey terms like relations or
%probabilities}

%\subsubsection{Stochastic gradient descent}
\paragraph*{Stochastic gradient descent}
%A roadblock for gradient based learning for large scale data is the sum over
%all $i$ or $j$ in computation of $\partial L_{\pi_{i,k}}$ and $\partial
%L_{\beta_{k,j}}$ respectively. When the dimensions of the data $Y$ are large, the summation is computationaly very expensive. 
When the dimensions of the data $Y$ are large, the summation over
all $i$ or $j$ in computation of $\partial L_{\pi_{i,k}}$ and $\partial
L_{\beta_{k,j}}$ is computationaly very expensive.  
To overcome this large scale ML algortihms
typically use stochastic gradient descent~\cite{Kushner:yin} that breaks up the
summation. The gradient with respect to $\pi$ or $\beta$ becomes 
\begin{align}
&\partial L_{\pi_{i,k}} = 2*(Y_{i,j} -
\sum_{k}\pi_{i_k}\beta_{j,k})\beta_{j,k} \nonumber\\  
%&\hspace{2cm} {\rm and} \nonumber\\
&\partial L_{\beta_{j,k}} = 2*(Y_{i,j} -
\sum_{k}\pi_{i_k}\beta_{j,k})\pi_{i,k}
\label{eqn:sgd}
\end{align}
This approach takes one single element from the input matrix $Y_{i,j}$ at a time and
applies equation~\ref{eqn:sgd} to get the new values for $\pi$ and $\beta$. It is
theoretically guaranteed to converge~\cite{Kushner:yin} and hence used quite
often in large scale machine learning problems.  (Because our problems are
non-convex, SGD can only be guaranteed to converge to a local minima.)  Further
it can be seen that updates from $Y_{i,j}$ and $Y_{i',j'}$ only inovlve and
update indepdnent variables $\pi_{i,*}$, $\pi_{i',*}$, $\beta_{j,*}$, and
$\beta_{j',*}$.  \method leverages this property in processing relational
blocks in parallel. 


%In this section, we describe how optimization techniques, and in particular
%stochastic gradient descent, can be used to fit a variety of machine learning
%models.
%
%\subsection{SGD Background} % (fold)
%\label{sub:SGD Background}
%
%% subsection SGD Background (end)
%
%\subsection{Dictionary Learning} % (fold)
%\label{sub:Dictionary Learning}
%
%% subsection Dictionary Learning (end)
%
%\subsection{Topic Modeling} % (fold)
%\label{sub:Topic Modeling}
%
%% subsection Topic Modeling (end)
%
%\subsection{Mixed Membership Stochastic Block Models} % (fold)
%\label{sub:Mixed Membership Stochastic Block Models}
%
%% subsection Mixed Membership Stochastic Block Models (end)
