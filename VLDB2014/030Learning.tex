The model and data abstraction defined in section~\ref{sec:mdAbstract} is a
convenient view for distributed computing of a large scale machine learning
problem. This abstraction can be separated logically from the way the learning
algortihm of the problem. The partition abstraction is used to distribute the
learning phase over different computig units. For example in the \lda problem
discussed in the abstraction earlier, we can use various learning algorithms to
reach the solution. In particular we can use either of the two general
learning schemes: 1) Sampling based learning, 2) Gradient based learning. These
two learning schemes differ in the way they treat the given problem.  

\subsection{Sampling based learning scheme}
Sampling based approach in general takes a probabilistic view of the problem.
It learns the parameters of the model by approximating the probability
distribution of the true data. This scheme samples
iteratively from a changing distribution that is affected by the samples drawn.
The distribution eventually reaches an equilibrium. The parameters of this
stationary distribution are our solutions (parameters of the model) and are used
further down the pipeline for prediction. In case of \lda such a scheme
progresses iteratively. We start with some distribution of \abhi{explain the
gibbs sampling steps for \lda here. For that we need to discribe the objective
function for \lda in section 2}.
\subsection{Gradient based learning}
In gradient based learning an objective function is optimized to find the
parameters of the model. The objective is formulated in a way to abstract
away the probabilistic components of the model if any present. A simple example
of such a learning scheme is, in case of \lda \abhi{again define }

\subsubsection{Stochastic gradient descent}



%In this section, we describe how optimization techniques, and in particular
%stochastic gradient descent, can be used to fit a variety of machine learning
%models.
%
%\subsection{SGD Background} % (fold)
%\label{sub:SGD Background}
%
%% subsection SGD Background (end)
%
%\subsection{Dictionary Learning} % (fold)
%\label{sub:Dictionary Learning}
%
%% subsection Dictionary Learning (end)
%
%\subsection{Topic Modeling} % (fold)
%\label{sub:Topic Modeling}
%
%% subsection Topic Modeling (end)
%
%\subsection{Mixed Membership Stochastic Block Models} % (fold)
%\label{sub:Mixed Membership Stochastic Block Models}
%
%% subsection Mixed Membership Stochastic Block Models (end)
