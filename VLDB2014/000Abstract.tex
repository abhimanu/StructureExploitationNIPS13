How can I accurately model a billion node social network like Facebook?  Or how
can I scalably run a toipc model over billions of webpages?
In modern machine learning and data mining using models,
which were designed for thousands of items, on billions of items is a constant
challenge.  The big data revolution pushes our systems to not only be able to
process moredata but also to be able to fit larger and more complex models.  
Our ``documents'' in topic modeling tweets are shorter not longer and our
social circles are not growing exponentially, but we have so many more of them
requiring our models to scale with the data being produced.

In this paper we describe \method, a general framework for performing fast
machine learning on huge models and big data.  Making use of our recent
innovations in \textcolor{red}{statistical something}, we design 
to use stochastic gradient descent in
