In designing any large scale machine learning system, one of the most important
design decisions is the abstraction for the data and problem model.  Recent
distributed ML systems focus primarily on the abstraction for the data, such as
GraphLab's \cite{graphlab} view of each data point as a node.  Here we take a
different approach - partitioning our data and the model we are building of the
data simultaneously.  As we will go on to demonstrate, focusing on both is
advantageous for both scalability and speed.

\subsection{Block structure in relational data} % (fold)
\label{sub:Block structure in relational data}

For many classic machine learning problems, we have relational data between
two sets of items, and we are asking questions about these items.  To be a bit
more concrete, consider the classic topic modeling question: given a large set
of documents, with what probability is each document in a given set of topics,
and with what probability does a word represent a given topic?  In this case, a
data point is the number of times a given word was used in a given document.
Because of the intrinsic relational nature of the data (here between words and
documents), partitioning our data intelligently results in a clean partitioning
of our model of topics for the words and documents.

More specifically, for any given subset of the documents and subset of the
words, there is a unique set of data points that are applicable to items from
both sets.  If we view our data as a very sparse matrix, partitioning the
documents and the words results in the data matrix also being partitioned into
blocks, as seen in Figure \ref{fig:abstraction}.  
Under this partitioning, all data points in block $\mathcal{B}_{i,j}$ describe
a relationship between a word from set $\mathcal{S}^{(1)}_i$ and document from
set $\mathcal{S}^{(2)}_j$.  We focus our computation around these blocks of data.

An interesting property of these blocks is that for some blocks, such as
$\mathcal{B}_{i,j}$ and $\mathcal{B}_{i',j'}$ where $i\neq i'$ and $j \neq j'$,
we see that the blocks are \textit{independent}.  That is, a data point from
$\mathcal{B}_{i,j}$ does not describe any relations to words in
$\mathcal{S}^{(1)}_{i'}$ or documents in $\mathcal{S}^{(2)}_{j'}$ and vice
versa for data in $\mathcal{B}_{i',j'}$.
%Formally defined:
%\alex{add formal definition of independence here? or will that come off too
%mathy}
As has been shown in previous stochastic learning literature \cite{textbook}
and used in simpler data mining problems \cite{gemulla,flexifact}, we can now
process 


\subsection{Distributed Parallel Processing} % (fold)

% subsection Processing the data (end)



%\subsection{Topic Modelling} \abhi{explain the model and data partition using
%this, topic-word and doc-topic matrix}
