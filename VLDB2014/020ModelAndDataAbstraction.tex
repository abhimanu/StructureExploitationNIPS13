In designing any large scale machine learning system, one of the most important
design decisions is the abstraction for the data and problem model.  Recent
distributed ML systems focus primarily on the abstraction for the data, such as
GraphLab's \cite{graphlab} view of each data point as a node.  Here we take a
different approach - partitioning our data and the model we are building of the
data simultaneously.  As we will go on to demonstrate, focusing on both is
advantageous for both scalability and speed.

\subsection{Block structure in relational data} % (fold)
\label{sub:Block structure in relational data}

For many classic machine learning problems, we have relational data between
two sets of items, and we are asking questions about these items.  To be a bit
more concrete, consider the classic topic modeling question: given a large set
of documents, with what probability is each document in a given set of topics,
and with what probability does a word represent a given topic?  In this case, a
data point is the number of times a given word was used in a given document.
Because of the intrinsic relational nature of the data (here between words and
documents), partitioning our data intelligently results in a clean partitioning
of our model of topics for the words and documents.  

More specifically, for any given subset of the documents and subset of the
words, there is a unique set of data points that are applicable to items from
both sets.  If we view our data as a very sparse matrix, partitioning the
documents and the words results in the data matrix also being partitioned into
blocks, as seen in Figure \ref{fig:abstraction}.  
Under this partitioning, all data points in block $\mathcal{B}_{i,j}$ describe
a relationship between a word from set $\mathcal{S}^{(1)}_i$ and document from
set $\mathcal{S}^{(2)}_j$.  We focus our computation around these blocks of data.

An interesting property of these blocks is that for some blocks, such as
$\mathcal{B}_{i,j}$ and $\mathcal{B}_{i',j'}$ where $i\neq i'$ and $j \neq j'$,
we see that the blocks are \textit{independent}.  That is, a data point from
$\mathcal{B}_{i,j}$ does not describe any relations to words in
$\mathcal{S}^{(1)}_{i'}$ or documents in $\mathcal{S}^{(2)}_{j'}$ and vice
versa for data in $\mathcal{B}_{i',j'}$.
%Formally defined:
%\alex{add formal definition of independence here? or will that come off too
%mathy}
As has been shown in previous stochastic learning literature \cite{textbook}
and used in simpler data mining problems \cite{gemulla,flexifact}, this
independence property allows for improved scalability and parallel processing.
We will discuss our system and the strengths of this abstraction primarily in terms of topic modeling, 
%Note, we will primarily discuss
%this system in terms of topic modeling, 
but note the system generalizes to many
machine learning problems as we will demonstrate later.
%We go over the implications for topic modeling and other machine learning
%problems below.

\subsection{Distributing our data} % (fold)
\label{sub:Distributing the Data and Model}

As mentioned above, distributing your data over a cluster is useful, but as
data grows partitioning both the data and your model of that data is
increasingly valuable.  For example, if we would like to know the topic
distribution for 10 million documents over 1000 topics, this would require over
37 gigabytes just to hold all of the answers to the query.  Therefore, it is
crucial that as our data grows we intelligently distribute both the data and our
solution.  This significantly improve memory efficiency and makes it possible
to scale to unprecedented sizes.  Luckily, our blocking abstraction makes this
easy.

In processing a given block $\mathcal{B}_{i,j}$, we only need the data from
that block, and the current information about the words $\mathcal{S}^{(1)}_{i}$
and documents $\mathcal{S}^{(2)}_{j}$.  Therefore, in distributing the problem
over a cluster of machines, we can have each worker only store and process one
block and its corresponding object model at a time.  Additionally, because our
sets of documents and sets of words are each disjoint, we can process our
blocks in such a way that each document and each word is only being worked on
by one worker at a time.  As a result, we do not need to store \textit{any}
duplicate data (about the topic model or blocks) and thus we are perfectly
memory efficient.


% subsection Distributing the Data and Model (end)

\subsection{Parallel processing} % (fold)
The last piece of the general system design is understanding the order in which
we process our data.  Our goal is to reach the optimal memory efficiency
described above and also keep our computation fast and accurate.  To do this,
we must choose a \textit{stratum}, a group of blocks, to process in parallel.  
In order to be memory efficient and keep our computation accurate, each stratum
must only contain blocks that are independent of each other.  From the block
structure, we can create multiple strata such that each block is in exactly one
stratum.
We iterate over the strata, in each case processing each block
in the stratum in parallel on the cluster.  We call processing one stratum a
\textit{subepoch} and processing all of the strata an \textit{epoch}.
A small example can be seen in Figure \ref{fig:abstraction}.

In practice, each machine in the cluster holds one of the blocks being
currently processed as well as the topic model for the corresponding words and
documents. As explained, doing this results in the topic models and blocks
being stored exactly once and thus being memory efficient.  
%We call the processing of all of the data from all of the blocks in a given stratum a \textit{subepoch}.
When a subepoch completes, we load the blocks for the next subepoch and
transfer the necessary pieces of the topic model to the appropriate machines.
\alex{maybe give classic systems graphic of data moving between machines in
cluster?}


% subsection Processing the data (end)



%\subsection{Topic Modelling} \abhi{explain the model and data partition using
%this, topic-word and doc-topic matrix}
