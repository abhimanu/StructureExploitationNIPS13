\begin{figure}[htb]
\begin{centering}
{
	\begin{tikzpicture}[scale=0.75,place/.style={draw,circle,thick,text=white, text badly centered,minimum size=2.0in}]
		\node at (-2.5,-1.6) (M1) {\pgfbox[center,bottom]{\pgfuseimage{machine}}};
		\node at (2.5,-1.6) (M2) {\pgfbox[center,bottom]{\pgfuseimage{machine}}};
		\node at (0,2.9) (M3) {\pgfbox[center,bottom]{\pgfuseimage{machine}}};

		%\draw (0,0) circle (1cm);

		\draw (-4,-3) rectangle (-1.0,-1.5);
		\draw (1,-3) rectangle (4.0,-1.5);
		\draw (-1.5,1.5) rectangle (1.5,3);

		% Overall border
		\draw[draw=white] (-4.5,-3.5) rectangle (4.5,4.5);


		%\draw (-4.5,-3.5) rectangle (4.5,4.5);
		\filldraw[fill=blue,draw=black,opacity=0.2] (-3.8,-2.8) rectangle (-2.9,-1.6);
		\filldraw[gray, very thin,step=0.3cm,xshift=0.1cm,yshift=-0.1cm] (-3.9,-2.7) grid (-3.0,-1.5);
		\draw[black] (-3.8,-2.8) rectangle (-2.9,-1.6);

		\filldraw[fill=blue,draw=black,opacity=0.2] (-2.0,-2.8) rectangle (-1.1,-1.6);
		\filldraw[gray, very thin,step=0.3cm,xshift=0.1cm,yshift=-0.1cm] (-2.1,-2.7) grid (-1.2,-1.5);
		\draw[black] (-2.0,-2.8) rectangle (-1.1,-1.6);

		\node at (-1.45,-2.2) (S1) {$\mathcal{S}^{(2)}_1$};
		\node at (-3.3,-2.2) {$\mathcal{S}^{(1)}_1$};


		\filldraw[fill=red,draw=black,opacity=0.2] (3.8,-2.8) rectangle (2.9,-1.6);
		\filldraw[gray, very thin,step=0.3cm,xshift=-0.1cm,yshift=-0.1cm] (3.0,-2.7) grid (3.9,-1.5);
		\draw[black] (3.8,-2.8) rectangle (2.9,-1.6);

		\filldraw[fill=red,draw=black,opacity=0.2] (2.0,-2.8) rectangle (1.1,-1.6);
		\filldraw[gray, very thin,step=0.3cm,xshift=-0.1cm,yshift=-0.1cm] (1.2,-2.7) grid (2.1,-1.5);
		\draw[black] (2.0,-2.8) rectangle (1.1,-1.6);

		\node at (1.6,-2.2) {$\mathcal{S}^{(1)}_2$};
		\node at (3.4,-2.2) (S2) {$\mathcal{S}^{(2)}_2$};


		\filldraw[fill=green,draw=black,opacity=0.2] (-1.3,1.7) rectangle (-0.4,2.9);
		\filldraw[gray, very thin,step=0.3cm,xshift=0.2cm,yshift=0.2cm] (-1.5,1.5) grid (-0.6,2.7);
		\draw[black] (-1.3,1.7) rectangle (-0.4,2.9);

		\filldraw[fill=green,draw=black,opacity=0.2] (1.3,1.7) rectangle (0.4,2.9);
		\filldraw[gray, very thin,step=0.3cm,xshift=-0.2cm,yshift=0.2cm] (0.6,1.5) grid (1.5,2.7);
		\draw[black] (1.3,1.7) rectangle (0.4,2.9);

		\node at (-0.8,2.35) {$\mathcal{S}^{(1)}_3$};
		\node at  (0.9,2.35) (S3) {$\mathcal{S}^{(2)}_3$};
		%\node at (3.4,-2.2) {$\mathcal{S}^{(2)}_2$};


		\begin{scope}[>=stealth,ultra thick,->]
		\path (S1) edge[bend right=30]  node [left=5pt] { } (S2);
		\path (S2) edge[bend right=30]  node [left=5pt] { } (S3);
		\path (S3) edge[bend right=30]  node [left=5pt] { } (S1);

		%\path (S) edge[red,bend right=10,decorate,decoration={snake,amplitude=1mm}] 
			%node [right=5pt,text=black] {$\beta_1$ } (I1);
		%\path (I1) edge[red,bend right=10,decorate,decoration={snake,amplitude=1mm}] 
			%node [right=5pt,text=black] {$\epsilon \beta_2$ } (I12);

		%\path (I12) edge[bend right=20]     node [left=5pt] {$\delta_2$} (I1);
		
			%\path (S) edge[red,bend left=10,decorate,decoration={snake,amplitude=1mm}] 
				%node [left=5pt,text=black] {$\beta_2$ } (I2);
			%\path (I2) edge[bend left=20]     node [right=5pt] {$\delta_2$} (S);

			%\path (I2) edge[red,bend left=10,decorate,decoration={snake,amplitude=1mm}] 
				%node [left=5pt,text=black] {$\epsilon \beta_1$ } (I12);
			%\path (I12) edge[bend left=20]     node [right=5pt] {$\delta_1$} (I2);


		\end{scope}
	\end{tikzpicture}
}
\caption{Movement of parameters through the cloud between subepochs.
\label{fig:data-movement}
}
\end{centering}
\end{figure}



%%%%% GENEREAL GRID
%\begin{figure}[htb]
%\begin{centering}
%{
	%\begin{tikzpicture}[scale=1.00,place/.style={draw,circle,thick,text=white, text badly centered,minimum size=2.0in}]

		%\filldraw[gray, very thin,step=0.2cm,yshift=0.1cm] (-2.2,2) grid (2,3);
		%\filldraw[gray, very thin,step=0.2cm,xshift=-0.1cm] (-3.2,-2.2) grid (-2.2,2);

		%\filldraw[gray, very thin,step=0.2cm] (-2.2,-2.2) grid (2,2);


		%\draw[black] (-2.2,-2.2) rectangle (2,2);
		%\draw[black] (-3.3,-2.2) rectangle (-2.3,2);
		%\draw[black] (-2.2,2.1) rectangle (2,3.1);


		%\draw[black] (-0.8,-2.2) rectangle (0.6,2);
		%\draw[black] (-2.2,-0.8) rectangle (2.0,0.6);

		%\draw[black] (-3.3,-0.8) rectangle (-2.3,0.6);
		%\draw[black] (-0.8,2.1) rectangle (0.6,3.1);

	%\end{tikzpicture}
%}
%\caption{tikz grid breakdown
%\label{fig:data-movement}
%}
%\end{centering}
%\end{figure}

%\definecolor{lightgray}{gray}{0.85}
%\def\colorOpacity{0.4}
%\def\gridScaling{0.78}
%\begin{figure*}
	%\centering{
	%\begin{minipage}{0.24\textwidth}
		%\centering {
		%\begin{tikzpicture}[scale=\gridScaling,place/.style={draw,circle,thick,text=white, text badly centered,minimum size=2in}]

			%\filldraw[lightgray, very thin,step=0.2cm,yshift=0.1cm] (-2.2,2) grid (2,3);
			%\filldraw[lightgray, very thin,step=0.2cm,xshift=-0.1cm] (-3.2,-2.2) grid (-2.2,2);
			%\filldraw[lightgray, very thin,step=0.2cm] (-2.2,-2.2) grid (2,2);


			%\draw[black] (-2.2,-2.2) rectangle (2,2);
			%\draw[black] (-3.3,-2.2) rectangle (-2.3,2);
			%\draw[black] (-2.2,2.1) rectangle (2,3.1);


			%\draw[black] (-0.8,-2.2) rectangle (0.6,2);
			%\draw[black] (-2.2,-0.8) rectangle (2.0,0.6);

			%\draw[black] (-3.3,-0.8) rectangle (-2.3,0.6);
			%\draw[black] (-0.8,2.1) rectangle (0.6,3.1);

			%\tikzstyle{every node}=[font=\huge]
			%\node at (-0.1,-0.1) {$Y$};
			%\node at (-2.8,-0.1) {$\pi$};
			%\node at (-0.1,2.6) {$\beta$};
		%\end{tikzpicture}
		%}
	%\end{minipage}
	%\begin{minipage}{0.24\textwidth}
		%\centering {
		%\begin{tikzpicture}[scale=\gridScaling,place/.style={draw,circle,thick,text=white, text badly centered,minimum size=2in}]


			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (-3.3,-2.2) rectangle (-2.3,-0.8);
			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (-3.3,-0.8) rectangle (-2.3,0.6);
			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (-3.3,0.6) rectangle (-2.3,2);

			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (-2.2,2.1) rectangle (-0.8,3.1);
			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (-0.8,2.1) rectangle (0.6,3.1);
			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (0.6,2.1) rectangle (2,3.1);

			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (-2.2,0.6) rectangle (-0.8,2);
			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (-0.8,-0.8) rectangle (0.6,0.6);
			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (0.6,-2.2) rectangle (2,-0.8);


			%\filldraw[lightgray, very thin,step=0.2cm,yshift=0.1cm] (-2.2,2) grid (2,3);
			%\filldraw[lightgray, very thin,step=0.2cm,xshift=-0.1cm] (-3.2,-2.2) grid (-2.2,2);
			%\filldraw[lightgray, very thin,step=0.2cm] (-2.2,-2.2) grid (2,2);


			%\draw[black] (-2.2,-2.2) rectangle (2,2);
			%\draw[black] (-3.3,-2.2) rectangle (-2.3,2);
			%\draw[black] (-2.2,2.1) rectangle (2,3.1);


			%\draw[black] (-0.8,-2.2) rectangle (0.6,2);
			%\draw[black] (-2.2,-0.8) rectangle (2.0,0.6);

			%\draw[black] (-3.3,-0.8) rectangle (-2.3,0.6);
			%\draw[black] (-0.8,2.1) rectangle (0.6,3.1);

			%\tikzstyle{every node}=[font=\normalsize]
			%\node at (-1.5,1.3) {$\mathcal{B}_{1,1}$};
			%\node at (-0.1,-0.1) {$\mathcal{B}_{2,2}$};
			%\node at (1.3,-1.5) {$\mathcal{B}_{3,3}$};

			%\node at (-2.8,1.3) {$\mathcal{S}_1^{(1)}$};
			%\node at (-2.8,-0.1) {$\mathcal{S}_2^{(1)}$};
			%\node at (-2.8,-1.5) {$\mathcal{S}_3^{(1)}$};

			%\node at (1.3,2.6) {$\mathcal{S}_3^{(2)}$};
			%\node at (-0.1,2.6) {$\mathcal{S}_2^{(2)}$};
			%\node at (-1.5,2.6) {$\mathcal{S}_1^{(2)}$};

		%\end{tikzpicture}
		%}
	%\end{minipage}
	%\begin{minipage}{0.24\textwidth}
		%\centering {
		%\begin{tikzpicture}[scale=\gridScaling,place/.style={draw,circle,thick,text=white, text badly centered,minimum size=2in}]


			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (-3.3,-2.2) rectangle (-2.3,-0.8);
			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (-3.3,-0.8) rectangle (-2.3,0.6);
			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (-3.3,0.6) rectangle (-2.3,2);

			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (-2.2,2.1) rectangle (-0.8,3.1);
			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (-0.8,2.1) rectangle (0.6,3.1);
			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (0.6,2.1) rectangle (2,3.1);

			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (-0.8,0.6) rectangle (0.6,2);
			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (0.6,-0.8) rectangle (2.0,0.6);
			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (-2.2,-2.2) rectangle (-0.8,-0.8);


			%\filldraw[lightgray, very thin,step=0.2cm,yshift=0.1cm] (-2.2,2) grid (2,3);
			%\filldraw[lightgray, very thin,step=0.2cm,xshift=-0.1cm] (-3.2,-2.2) grid (-2.2,2);
			%\filldraw[lightgray, very thin,step=0.2cm] (-2.2,-2.2) grid (2,2);


			%\draw[black] (-2.2,-2.2) rectangle (2,2);
			%\draw[black] (-3.3,-2.2) rectangle (-2.3,2);
			%\draw[black] (-2.2,2.1) rectangle (2,3.1);


			%\draw[black] (-0.8,-2.2) rectangle (0.6,2);
			%\draw[black] (-2.2,-0.8) rectangle (2.0,0.6);

			%\draw[black] (-3.3,-0.8) rectangle (-2.3,0.6);
			%\draw[black] (-0.8,2.1) rectangle (0.6,3.1);

			%\tikzstyle{every node}=[font=\normalsize]
			%\node at (-0.1,1.3) {$\mathcal{B}_{1,1}$};
			%\node at (1.3,-0.1) {$\mathcal{B}_{2,2}$};
			%\node at (-1.5,-1.5) {$\mathcal{B}_{3,3}$};

			%\node at (-2.8,1.3) {$\mathcal{S}_1^{(1)}$};
			%\node at (-2.8,-0.1) {$\mathcal{S}_2^{(1)}$};
			%\node at (-2.8,-1.5) {$\mathcal{S}_3^{(1)}$};

			%\node at (1.3,2.6) {$\mathcal{S}_2^{(2)}$};
			%\node at (-0.1,2.6) {$\mathcal{S}_1^{(2)}$};
			%\node at (-1.5,2.6) {$\mathcal{S}_3^{(2)}$};

		%\end{tikzpicture}
		%}
	%\end{minipage}
	%\begin{minipage}{0.24\textwidth}
		%\centering {
		%\begin{tikzpicture}[scale=\gridScaling,place/.style={draw,circle,thick,text=white, text badly centered,minimum size=2in}]


			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (-3.3,-2.2) rectangle (-2.3,-0.8);
			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (-3.3,-0.8) rectangle (-2.3,0.6);
			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (-3.3,0.6) rectangle (-2.3,2);

			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (-2.2,2.1) rectangle (-0.8,3.1);
			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (-0.8,2.1) rectangle (0.6,3.1);
			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (0.6,2.1) rectangle (2,3.1);

			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (0.6,0.6) rectangle (2.0,2);
			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (-2.2,-0.8) rectangle (-0.8,0.6);
			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (-0.8,-2.2) rectangle (0.6,-0.8);


			%\filldraw[lightgray, very thin,step=0.2cm,yshift=0.1cm] (-2.2,2) grid (2,3);
			%\filldraw[lightgray, very thin,step=0.2cm,xshift=-0.1cm] (-3.2,-2.2) grid (-2.2,2);
			%\filldraw[lightgray, very thin,step=0.2cm] (-2.2,-2.2) grid (2,2);


			%\draw[black] (-2.2,-2.2) rectangle (2,2);
			%\draw[black] (-3.3,-2.2) rectangle (-2.3,2);
			%\draw[black] (-2.2,2.1) rectangle (2,3.1);


			%\draw[black] (-0.8,-2.2) rectangle (0.6,2);
			%\draw[black] (-2.2,-0.8) rectangle (2.0,0.6);

			%\draw[black] (-3.3,-0.8) rectangle (-2.3,0.6);
			%\draw[black] (-0.8,2.1) rectangle (0.6,3.1);

			%\tikzstyle{every node}=[font=\normalsize]
			%\node at (1.3,1.3) {$\mathcal{B}_{1,1}$};
			%\node at (-1.5,-0.1) {$\mathcal{B}_{2,2}$};
			%\node at (-0.1,-1.5) {$\mathcal{B}_{3,3}$};

			%\node at (-2.8,1.3) {$\mathcal{S}_1^{(1)}$};
			%\node at (-2.8,-0.1) {$\mathcal{S}_2^{(1)}$};
			%\node at (-2.8,-1.5) {$\mathcal{S}_3^{(1)}$};

			%\node at (1.3,2.6) {$\mathcal{S}_1^{(2)}$};
			%\node at (-0.1,2.6) {$\mathcal{S}_3^{(2)}$};
			%\node at (-1.5,2.6) {$\mathcal{S}_2^{(2)}$};

		%\end{tikzpicture}
		%}
	%\end{minipage}
	
	%%$\quad\quad\quad$
	%%\begin{minipage}{0.25\textwidth}
		%%\centering{\includegraphics[page=9,width=0.95\linewidth]{figs/figures.pdf}\\(b)}
	%%\end{minipage}
	%%$\quad\quad\quad$
	%%\begin{minipage}{0.25\textwidth}
		%%\centering{\includegraphics[page=7,width=0.95\linewidth]{figs/figures.pdf}\\(c)
		%%}
	%%\end{minipage}
	%}
%\label{fig:batched}
%\end{figure*}





%\begin{figure}[htb]
%\begin{centering}
%{
	%\begin{tikzpicture}[scale=1,place/.style={draw,circle,thick,text=white, text badly centered,minimum size=0.3in}]
		%%\node[fill=blue!65,place] at (0, -2.5) (S) {\textbf{S}};
		%\node[place] at (0,-2.5) (S) {\pgfbox[center,bottom]{\pgfuseimage{machine}}};
		%\node[fill=gray,place]  at (-2.5, 0)  (I1) {$\mathbf{I_1}$}; 
		%\node[fill=gray, place]   at (2.5, 0)  (I2) {$\mathbf{I_2}$};
		%\node[fill=black, place]   at (0, 2.5)  (I12) {$\mathbf{I_{1,2}}$};
			%%\node[place] at (1, 0) (E) {\textbf{E}};
		%%\node[place] at (2, -0.5) (I) {\textbf{I}};	
		%%\node[place] at (0,-1.5) (V) {\textbf{V}};
		%\begin{scope}[>=stealth,ultra thick,->]

		%\path (S) edge[red,bend right=10,decorate,decoration={snake,amplitude=1mm}] 
			%node [right=5pt,text=black] {$\beta_1$ } (I1);
		%\path (I1) edge[bend right=20]     node [left=5pt] {$\delta_1$} (S);
		%\path (I1) edge[red,bend right=10,decorate,decoration={snake,amplitude=1mm}] 
			%node [right=5pt,text=black] {$\epsilon \beta_2$ } (I12);

		%\path (I12) edge[bend right=20]     node [left=5pt] {$\delta_2$} (I1);
		
			%\path (S) edge[red,bend left=10,decorate,decoration={snake,amplitude=1mm}] 
				%node [left=5pt,text=black] {$\beta_2$ } (I2);
			%\path (I2) edge[bend left=20]     node [right=5pt] {$\delta_2$} (S);

			%\path (I2) edge[red,bend left=10,decorate,decoration={snake,amplitude=1mm}] 
				%node [left=5pt,text=black] {$\epsilon \beta_1$ } (I12);
			%\path (I12) edge[bend left=20]     node [right=5pt] {$\delta_1$} (I2);


		%\end{scope}
	%\end{tikzpicture}
%}
%\caption{State Diagram for a node in the graph under our partial-competition model.
%% $SI_1I_2S$ competing viruses model. The node is in the $S$ state if it doesn't have either  competitor (say iPhone or Android). It is in $I_1$ if it gets the iPhone (virus 1) and is in $I_2$ if it gets the Android (virus 2). The transitions from $S$ to $I_1$ or $I_2$ (red-curvy arrows) depend on the infected neighbors of the node. The remaining transitions,  in contrast, are self-transitions, without the aid of any neighbor.
%\label{fig:siismodel}
%}
%\end{centering}
%\end{figure}



%\begin{tikzpicture}[scale=1.5]
    %% Draw axes
    %\draw [<->,thick] (0,2) node (yaxis) [above] {$y$}
        %|- (3,0) node (xaxis) [right] {$x$};
    %% Draw two intersecting lines
    %\draw (0,0) coordinate (a_1) -- (2,1.8) coordinate (a_2);
    %\draw (0,1.5) coordinate (b_1) -- (2.5,0) coordinate (b_2);
    %% Calculate the intersection of the lines a_1 -- a_2 and b_1 -- b_2
    %% and store the coordinate in c.
    %\coordinate (c) at (intersection of a_1--a_2 and b_1--b_2);
    %% Draw lines indicating intersection with y and x axis. Here we use
    %% the perpendicular coordinate system
    %\draw[dashed] (yaxis |- c) node[left] {$y'$}
        %-| (xaxis -| c) node[below] {$x'$};
    %% Draw a dot to indicate intersection point
    %\fill[red] (c) circle (2pt);
	%%\pgfuseimage{machine}
%\end{tikzpicture}


\begin{figure*}
	\includegraphics[page=1, width=0.24\textwidth]{figs/grid3.pdf} \hfill
	\includegraphics[page=2, width=0.24\textwidth]{figs/grid3.pdf} \hfill
	\includegraphics[page=13,width=0.24\textwidth]{figs/grid3.pdf} \hfill
	\includegraphics[page=17,width=0.24\textwidth]{figs/grid3.pdf} 
	\caption{Example of data and model partitioning along with the three subepochs.}
	\label{fig:partition}
\end{figure*}

In designing any large scale machine learning system, one of the most important
design decisions is the abstraction for the data and problem model.  Recent
distributed ML systems focus primarily on the abstraction for the data, such as
\graphlab's \cite{Low:DGF} view of each data point as a node or 
\psgd's \cite{zinkevich2010parallelized} view of partitioning only on the data.
Here we take a different approach - partitioning our data and the model we are building of the
data simultaneously.  As we will go on to demonstrate, focusing on both is
advantageous for both scalability and speed.

\subsection{Problem formulation}
For many classic machine learning problems, we have data about the
relationships between different types of objects and we would like to
understand these objects and their relationship to each other.  We consider we
have $t$ sets of objects $\mathcal{T}_1, \mathcal{T}_2, \ldots \mathcal{T}_t$,
each with $T_i = |\mathcal{T}_i|$ items which can be indexed from 1 to $T_i$ as $\mathcal{T}_{i,j}$.
From this we have a dataset of relations between these objects defined by
$$\mathcal{X}: \mathcal{T}_1 \times \mathcal{T}_2\times \ldots \times \mathcal{T}_n \rightarrow \mathbb{R}.$$

Given such a dataset, we would like to find a set of values for each object
to describe that object.  Therefore, we have also $t$ sets of variables
$\Theta = \{\theta_1 \ldots \theta_t\}$ where $\theta_{i,j}$ is a set of
variables (often a vector) which describe object $\mathcal{T}_{i,j}$.  For
notational simplicity, we will say that given a vector of indices
$\mathbf{i}=(i_1,i_2,\ldots i_t)$ we can reference the relevant variables by
$\Theta_{\mathbf{i}} = \{\theta_{1,\mathbf{i}_1},\theta_{2,\mathbf{i}_2}, \ldots \theta_{t,\mathbf{i}_t} \}$.

Given these defintions, we can formally define our general problem:
\begin{problem}
	Given dataset $\mathcal{X}$ defiend over $t$ types of objects
	$\mathcal{T}$, our objective is to find $\Theta$ through the following
	minimization:
	\begin{align}
		\arg\min_{\Theta} &\sum_{\mathbf{i} \in \{\mathcal{T}_1 \times \mathcal{T}_2 \times \ldots \times \mathcal{T}_t\}} f(\Theta_\mathbf{i},\mathcal{X}(\mathbf{i}))\\
		& {\rm s.t.}\; C(\Theta) = \mathbf{1},
	\end{align}
	where $f$ is any differentiable function and $C$ is a convex constraint
	across subsets of variables $\Theta$.
\end{problem}
Without loss of generality we can also set $\mathcal{T}_i = \mathcal{T}_j$ such
that we have relations between objects of the same type.  Additionally, we can
combine the loss function across multiple datasets on similar objects by merely
adding their objectives together. 

This problem defintion is very broad covering a variety of common problems as
special cases including topic modeling, dictionary learning, network
decomponsition, matrix and tensor factorization, and even mean field problems.
Throughout this paper, we will describe our system in terms of topic modeling
for clarity.  

When performing topic modeling we have two types of objects ($t=2$) - words
and documents, which we will denote by $\mathcal{T}_1 = \mathcal{W}$ and
$\mathcal{T}_2=\mathcal{D}$.  
For each document, we know the number of times a
word was used; our dataset $\mathcal{X}$ is the normalized version of these values.  
Our objective is to predict which words will be
used in which documents.  In the process, we attempt to find the topics of each
document and the words used when discussing each topic (from $K$ topics). 
To do this, we use the following Bayesian model:
\begin{align*}
	p({\rm Word}\; w | {\rm Doc}\; d) = \sum_{ {\rm Topic}\; k = 1}^K p(w | {\rm Topic} \!=\! k)\cdot  p( {\rm Topic} \!=\! k | d)
\end{align*}
The variables for the probabilities that a word is used in discussing a given
topic are denoted by $\theta_1=\pi$, where $\pi_i$ is the $K$-length vector of
probabilities $p(w_i|{\rm Topic}= k)$ for all $k$.  Similarly the variables for
the probabilities that a document is about a given topic are denoted by
$\theta_2=\beta$ where $\beta_j$ is the $K$-length vector of probabilities
$p(d_j|{\rm Topic}=k)$ for all $k$.

From this, we have a similar problem statement as before but specific to topic modeling:
\begin{problem}
	Given a word $\times$ document dataset $\mathcal{X}$, find
	\begin{align}
		\arg\min_{\pi,\beta} &\sum_{w_i \in \mathcal{W},d_j \in \mathcal{D}} f(\pi_i,\beta_j,\mathcal{X}(i,j))\\
		f(\pi_i,\beta_j,x) &= (x - \pi_i^\top\beta_j)^2\\
		{\rm s.t.}\; &\sum_{k=1}^K \beta_{j,k} = 1,\; \forall j\in \mathcal{D}\\
		&\sum_{i \in \mathcal{W} } \pi_{i,k} = 1,\; \forall k = 1 \ldots K
	\end{align}
\end{problem}
Solving this produces a model of the topics of our documents and the use of our
words.  In particular, we are left with a model of the use of words in topics
given by $\pi$, which can be used to find the topics of new documents. 

In the abstract 
formulation of the problem earlier we introduced types $\mathcal{T}$. It is important to note 
that some of these types are an exclusive construct of the model e.g. the topic-word 
type $\beta$ in topic modelling. These types are called parameters of the model.



\subsection{Relational block structure} % (fold)
\label{sub:Block structure in relational data}

%For many classic machine learning problems, we have relational data between
%two sets of entities, and we are asking questions about these entities. \abhi{my editing is here} 
%Answers to 
%these questions are obtained by estimating a set of unknowns ($\gamma$) that define the entity relations
%concretely. These unknowns can be categorized into two sets: 1) variables ($\gamma_{var}$) and 
%2) parameters ($\gamma_{par}$). 
%Variables are set of unknowns that arise in relation to the data while parameters are set of unknowns that 
%arise in relation to the assumptions that one makes in the entity relationship (i.e. the model). In case of
%topic model, the topic proportions of a document are the variables. Each document (data) has 
%different topic proportion and 
%these unknown proportions are estimated by the model. The probability of a word in a 
%given topic is a parameter and this probability is same for all the documents. The parameters 
%define the model that is learnt.  
%The overall unknowns set (variable and parameters) can be divided into relational blocks ($(v,p)$) of independent set of 
%unknowns($\gamma_{v,p}$) for most problems. Each relational block is indexed by $v$ and $p$ that corresspond to the 
%subsets of variables and parameters present in the block respectively. Taking the topic modelling example,
%if the choose a particular subset of document-topic probability unknowns indexed by $v$ and 
%topic-word probability unknowns indexed by $p$, then inner product (along the common topics) 
%of this set of document-topic unknowns 
%with the corressponding set of topic-word unknowns gives a block of data $Y$ (document-word probabilities). 
%The function $F(\gamma)$ to be optimized can be cleanly 
%broken into sum of smaller functions ($f_{v,p}(Y, \gamma_{v,p})$) that are defined only for each individual relation block.
%\begin{eqnarray}
%F(\gamma) = \sum_{v,p} f_{v,p}(Y, \gamma_{v.p}) \newline
%s.t. \gamma_{v,p} \in C
%\label{eqn:abstractFunction}
%\end{eqnarray}
%All the relational blocks in the end are combined together using a set of constraints $C$. 
%To be a
%bit more concrete, consider again the classic topic modeling question: given a large set
%of $D$ documents, with what probability a given document talks about a given
%topic, and with what probability does a given topic has a given word~\cite{Blei:2003:LDA}? 
%In this case, a raw data point is the number of occurrences of a word in a
%document. Each data point is an element in the document by word data matrix. We
%normalize the data matrix to obtain matrix $Y$ where an element
%$Y_{i,j}$ in column $i$ and row $j$ in the new matrix is the original
%element at $(i,j)$ divided by the row $i$'s sum. In simple terms,
%$Y_{i,j} = p(j|i)$ i.e. the probability of word $j$ lying in
%document $i$.

%\lda (topic modelling) assumes that there are fixed number ($K$) of
%underlying topics in the document corpus. A topic is composed of weighted
%combinations of words in the corpus where different words have different
%weights. The weight of a word $j$ in topic $k$  is $p(j|k)$: probability of
%word $j$ in topic $k$. Each document in the corpus is a weighted combination of
%the topics. The weight of topic $k$ in document $i$ is $p(k|i)$. The probability
%of a word $i$ in document $j$ is 
%\begin{equation}
%p(j|i)=\sum_{k=1}^Kp(j|k)*p(k|i)
%\end{equation}



%We define two matrices $\pi$ and $\beta$ such that $\pi_{i,k}=p(k|i)$ and
%$\beta_{k,j}=p(j|k)$. In that case $Y_{i,j}=\sum_k\pi_{i,k}*\beta_{k,j}$.
%The $\pi$ matrix is the per document topic probability matrix. $\pi_{i,k}$ is
%the probability weight of topic $k$ in document $j$. $\beta$ matrix is the per
%topic word probability. $\beta_{k,j}$ is the probability weight of word $j$
%in topic $k$. 
%\abhi{my edit here}
%Here $\pi$ is the set of variables and $\beta$ is the set of parameters. $\beta$, the 
%topic-word matrix is depedent on the model and is common for all documents. $\pi$, the 
%document-topic matrix is dependent on the documents in the corpus and if documents change 
%over time as in online topic modelling (\cite{online-lda}). The $\pi$ is the set of variables
%as explained earlier.

%In \lda we are solving the following query to find $\pi$ and
%$\beta$.

%% {\small
%\begin{align}
%&\argmin_{\pi,\beta}L(Y,\pi,\beta)
%=\sum_{i,j}(Y_{i,j}-\sum_k\pi_{i,k}\beta_{k,j})^2
%%= \sum_{i,j}(Y_{i,j}-\sum_k \pi_{i,k}\beta_{k,j})_p^p
%\label{eqn:LDA}\\
%&\text{s.t.} \; \forall  i,j,k \quad
%\sum_k\pi_{i,k}=1,
%\sum_j\beta_{k,j}=1, \quad
%\pi_{i,k}\geq 0,
%\beta_{k,j}\geq 0,
%\nonumber
%\end{align}
%% }

%The second line of constraints in equation~\ref{eqn:LDA} is due to the fact that
%$\pi$ and $\beta$ are probability matrices. This corressponds to the constraint set $C$ defined 
%in equation \ref{eqn:abstractFunction}. 

Because of the intrinsic relational nature of the data (here between words and
documents), partitioning our data  intelligently results in a clean
partitioning of our model of topics for the words and documents.

More specifically, for any given subset of the documents and subset of the
words, there is a unique set of data points that are applicable to entities from
both sets.  If we view our data as a very sparse matrix $Y$ (where $Y$ is a
documents by word matrix), partitioning the documents and the words results in
the data matrix $Y$ also being partitioned into blocks, as seen in Figure
\ref{fig:abstraction}.
Under this partitioning, all data points in block $\mathcal{B}_{i,j}$ describe
a relationship between a word from set $\mathcal{S}^{(1)}_i$ and document from
set $\mathcal{S}^{(2)}_j$.  We focus our computation around these blocks of data.

An interesting property of these blocks is that for some blocks, such as
$\mathcal{B}_{i,j}$ and $\mathcal{B}_{i',j'}$ where $i\neq i'$ and $j \neq j'$,
we see that the blocks are \textit{independent}.  That is, a data point from
$\mathcal{B}_{i,j}$ does not describe any relations to words in
$\mathcal{S}^{(1)}_{i'}$ or documents in $\mathcal{S}^{(2)}_{j'}$ and vice
versa for data in $\mathcal{B}_{i',j'}$.
%Formally defined:
%\alex{add formal definition of independence here? or will that come off too
%mathy}
As has been shown in previous stochastic learning literature \cite{textbook}
and used in simpler data mining problems \cite{gemulla,flexifact}, this
independence property allows for improved scalability and parallel processing.
We will discuss our system and the strengths of this abstraction primarily in
terms of topic modeling,
%Note, we will primarily discuss
%this system in terms of topic modeling, 
but note the system generalizes to many
machine learning problems as we will demonstrate later.
%We go over the implications for topic modeling and other machine learning
%problems below.

\subsection{Distributing our data} % (fold)
\label{sub:Distributing the Data and Model}

As mentioned above, distributing your data over a cluster is useful, but as
data grows partitioning both the data and your model of that data is
increasingly valuable.  For example, if we would like to know the topic
distribution for 10 million documents over 1000 topics, this would require over
37 gigabytes just to hold all of the answers to the query.  Therefore, it is
crucial that as our data grows we intelligently distribute both the data and our
solution.  This significantly improve memory efficiency and makes it possible
to scale to unprecedented sizes.  Luckily, our blocking abstraction makes this
easy.

In processing a given block $\mathcal{B}_{i,j}$, we only need the data from
that block, and the current information about the words $\mathcal{S}^{(1)}_{i}$
and documents $\mathcal{S}^{(2)}_{j}$.  Therefore, in distributing the problem
over a cluster of machines, we can have each worker only store and process one
block and its corresponding object model at a time.  Additionally, because our
sets of documents and sets of words are each disjoint, we can process our
blocks in such a way that each document and each word is only being worked on
by one worker at a time.  As a result, we do not need to store \textit{any}
duplicate data (about the topic model or blocks) and thus we are perfectly
memory efficient.


% subsection Distributing the Data and Model (end)

\subsection{Parallel processing} % (fold)
The last piece of the general system design is understanding the order in which
we process our data.  Our goal is to reach the optimal memory efficiency
described above and also keep our computation fast and accurate.  To do this,
we must choose a \textit{stratum}, a group of blocks, to process in parallel.  
In order to be memory efficient and keep our computation accurate, each stratum
must only contain blocks that are independent of each other.  From the block
structure, we can create multiple strata such that each block is in exactly one
stratum.
We iterate over the strata, in each case processing each block
in the stratum in parallel on the cluster.  We call processing one stratum a
\textit{subepoch} and processing all of the strata an \textit{epoch}.
A small example can be seen in Figure \ref{fig:abstraction}.

In practice, each machine in the cluster holds one of the blocks being
currently processed as well as the topic model for the corresponding words and
documents. As explained, doing this results in the topic models and blocks
being stored exactly once and thus being memory efficient.  
%We call the processing of all of the data from all of the blocks in a given stratum a \textit{subepoch}.
When a subepoch completes, we load the blocks for the next subepoch and
transfer the necessary pieces of the topic model to the appropriate machines.
\alex{maybe give classic systems graphic of data moving between machines in
cluster?}


% subsection Processing the data (end)



%\subsection{Topic Modelling} \abhi{explain the model and data partition using
%this, topic-word and doc-topic matrix}
