\documentclass{vldb}
%\usepackage{graphicx}
\usepackage{balance}  % for  \balance command ON LAST PAGE  (only there!)

\input{dfn.tex}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{subfigure,grffile,placeins}
\usepackage{graphicx} 
\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage[lined,boxed]{algorithm2e}
\usepackage{url}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{xkeyval} 
%\usepackage{pax}
%\usepackage{pdfpages}
%\usetikzlibrary{shadows}
%\usetikzlibrary{shapes,arrows,decorations.pathmorphing,backgrounds,fit,positioning}
%\tikzstyle{every picture}+=[remember picture]


\newtheorem{observation}{Observation}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem Definition}
\newtheorem{algo}{Algorithm}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
%\newtheorem{question}{Question}
\newtheorem{example}{Example}
\newcommand{\pushright}[1]{\ifmeasuring@#1\else\omit\hfill$\displaystyle#1$\fi\ignorespaces}
\newcommand{\pushleft}[1]{\ifmeasuring@#1\else\omit$\displaystyle#1$\hfill\fi\ignorespaces}

%\newtheorem{answer}{Answer}
%\newtheorem{proof}{Proof}

\newcommand{\abhi}[1]{\textcolor{orange}{abhi-comment: #1}}
\newcommand{\alex}[1]{\textcolor{red}{alex-comment: #1}}
\newcommand{\qirong}[1][1]{\textcolor{fuschia}{\\ qirong-comment: #1}}
\newcommand{\eric}[1][1]{\textcolor{blue}{\\ eric-comment: #1}}


\pgfdeclareimage[height=0.5in]{machine}{figs/server2.png}


\begin{document}

% ****************** TITLE ****************************************

\title{\methodplain: A General Optimization Framework for Fast, Scalable Machine Learning}

% possible, but not really needed or used for PVLDB:
%\subtitle{[Extended Abstract]
%\titlenote{A full version of this paper is available as\textit{Author's Guide to Preparing ACM SIG Proceedings Using \LaTeX$2_\epsilon$\ and BibTeX} at \texttt{www.acm.org/eaddress.htm}}}

% ****************** AUTHORS **************************************

% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.

\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Abhimanu Kumar\titlenote{\label{note1}The first two authors contributed equally to this work.}\\
       \affaddr{School of Computer Science}\\
       \affaddr{Carnegie Mellon University}\\
       \affaddr{Pittsburgh, PA}\\
       \email{abhimank@cs.cmu.edu}
% 2nd. author
\alignauthor
Alex Beutel\raisebox{9pt}{$\ast$} \\
       \affaddr{School of Computer Science}\\
       \affaddr{Carnegie Mellon University}\\
       \affaddr{Pittsburgh, PA}\\
       \email{abeutel@cs.cmu.edu}
% 3rd. author
\alignauthor 
Qirong Ho\\
       \affaddr{School of Computer Science}\\
       \affaddr{Carnegie Mellon University}\\
       \affaddr{Pittsburgh, PA}\\
       \email{qho@cs.cmu.edu}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor 
Eric P. Xing\\
       \affaddr{School of Computer Science}\\
       \affaddr{Carnegie Mellon University}\\
       \affaddr{Pittsburgh, PA}\\
       \email{epxing@cs.cmu.edu}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%\additionalauthors{Additional authors: John Smith (The Th{\o}rv\"{a}ld Group, {\texttt{jsmith@affiliation.org}}), Julius P.~Kumquat
%(The \raggedright{Kumquat} Consortium, {\small \texttt{jpkumquat@consortium.net}}), and Ahmet Sacan (Drexel University, {\small \texttt{ahmetdevel@gmail.com}})}
%\date{30 July 1999}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.


\maketitle

\begin{abstract}
How can Facebook scalably model their billion-node social network?  How
can Twitter efficiently run a toipc model over billions of tweets?
``Big data'' today does not just mean more data to mine but also more things
to understand - more users, more documents, and more images.
Thus, modern machine learning and data mining faces the challenge of
scaling our classic models to both use all of the data available as well as build
larger and more complex models of our world. 
%In modern machine learning and data mining using models,
%which were designed for thousands of items, on billions of items is a constant
%challenge.  
%The big data revolution pushes our systems to not only be able to
%process moredata but also to be able to fit larger and more complex models.  
%Our ``documents'' in topic modeling tweets are shorter not longer and our
%social circles are not growing exponentially, but we have so many more of them
%requiring our models to scale with the data being produced.

In this paper we describe \method, a general framework for performing fast
machine learning on huge models and big data.  Our system uses a combination
of recent insights in stochastic learning theory to enable our
system innovations.  In particular, our system offers three contributions:
{\bf (1) Scalability:} Our system distributes both the data and model across
the cluster of machines, allowing the system to scale to terrabytes of data as
well as models with billions of parameters, beyond the size that can fit in
memory for each machine. {\bf (2) Versatility:} Our system supports
synchronized parameter updates, such as normalizing across the cluster.
As a result, we can fit a variety of machine learning models, including topic
models, dictionary learning, and mixed membership stochastic block models.
{\bf (3) Speed:} We use a \textcolor{red}{modification} of stochastic gradient
descent, called Always-On SGD, to continously improve our model fit, even if
their are slow workers that would otherwise obstruct model synchronization and
slow computation. 
Finally, we demonstrate that with our new approah, \method can fit ML models at
an unprecedented scale and faster than prior work.
\end{abstract}

\section{Introduction} % (fold)
\label{sec:intro}
%\alex{\textbf{NOTE: The paper outline is still {\it very} rough, and I expect to re-order
%many sections, largely in attempt to keep the focus on the system.}}

\begin{figure}
	\centering
	\includegraphics[width=0.8\columnwidth]{fig2/lda_datasize.eps}
	\caption{Scalability comparison for topic modeling}
	\label{fig:crownjewel}
\end{figure}

In recent years, the amount of data being produced and stored throughout the
world has exploded.  From science to e-commerce to news to social networking,
we, as a culture, are spending more time and money producing uniquely digital
content and recording elements of our physical world online for posterity and
analysis.  With this big data boom, the focus is often on the size of the data
- ``LHC produced 13 petabytes \ldots of data in
2010\footnote{\url{http://www.nature.com/news/2011/110119/full/469282a.html}}''
and ``500+terabytes of new data ingested into [Facebook] databases every
day\footnote{\url{http://gigaom.com/2012/08/22/facebook-is-collecting-your-data-500-terabytes-a-day/}}.''
While handling all of this data is an important challenge and lots of work has
focused on scaling to large datasets, this misses an important part of the
picture - we are also recording more data about {\em more things}: ``300
million photos uploaded per day [to Facebook]'' and there are more than 400
million tweets sent per
day\footnote{\url{http://articles.washingtonpost.com/2013-03-21/business/37889387_1_tweets-jack-dorsey-twitter}}.
The challenge now becomes not just using all of the data, but also
understanding all of these items.

Machine learning has focused on understanding large datasets for many years
now, but the research often ignore the challenge of scaling to models over many
millions or even billions of items, e.g. finding the topics of millions of news
articles, removing noise from the many images upoaded online, or clustering users
in a billion node social network.  In each of these cases, the issue is not
just the size of the data but also the number of items we need to model and
understand.  Even if we distribute our dataset across many machines, the model
of topics for 100 million documents (news articles or webpages) across even 100
topics would take over 35 gigabytes just to store in memory.  

In this paper we describe a new system, \method, for answering such complex
questions about modern massive datasets.  Informally and generally,
the problem we are solving is:\\ 
\textbf{Given:} A database of relations between different objects\\
\textbf{Find:} A model of those objects which best matches those relationionships.\\
As an example, for the special case of topic modelling this can be framed, again informally, as:\\
\textbf{Given:} A database of documents for which we know the use of different words in different documents\\
\textbf{Find:} The probability of each document being about a particular topic,
and for each topic the probability that a word is used when discussing that topic.\\
We define our problem formally as a minimization problem over a class of functions in
Section \ref{sec:mdAbstract}.  

In order to answer a variety of such complex questions, \method focuses on a
new abstraction for relational data that enables easy partitioning and
distribution of both the dataset and model across a cluster of machines.  This
abstraction for relational data is flexible enough to answer a wide variety of
common machine learning queries.  In this paper we describe the following
contributions:
\begin{enumerate}
	\item \textbf{Scalability:} Our system scales to model billions of items
		using massive datasets.  We are particularly efficient in our memory
		usage, enabling us to scale beyond current state of the art systems.
	\item \textbf{Versatility:} \method can answer a wide variety of complex
		machine learning queries, often with even higher accuracy than other
		systems.  We demonstrate our effectiveness on topic modeling,
		dictionary learning for de-noising images, and mixed membership
		stochastic block models for clustering users in a network.
	\item \textbf{Speed:} We use new techniques for running over barriers to
		not waste computational time even when waiting stragglers in the cluster.
		This lets \method converge to the final answer faster than
		competitors.
	\item \textbf{Portability:} Because of Apache's Hadoop
		and HDFS ubiquity in industry, we build our system on top of these
		frameworks.  This makes it extremely easy to run the system without
		having to modify your cluster.
\end{enumerate}

In Section \ref{sec:mdAbstract} we explain our scalable abstraction for the
model and data, and give the high level overview of computation under this
abstraction in a cluster.  In Section \ref{sec:learning} we give the background
on different learning techniques, such as gradient based learning, for modeling
our data.  In Section \ref{sec:complexQues} we describe how our system can
answer a wide variety of complex questions, such as what is the probability of
a webpage being about a set of topics.  In Section \ref{sec:overBarriers} we
explain how we can continue performing valuable computation even while waiting
for stragglers to catch up, thus enabling our system to converge to the solution
quickly.  In Section \ref{sec:implementation} we give the details of how we
implemented the system on Hadoop.  In Section \ref{sec:applications} we
describe a few different applications of our system and in Section
\ref{sec:experiments} we describe our experimental setup.  In Section
\ref{sec:eval} we evaluate our system against other state of the art systems.
Finally in Section \ref{sec:related} we give a survey of the related work, and we
conclude in Section \ref{sec:conclusion}.

\section{Model and Data Abstraction}
\label{sec:mdAbstract}

\begin{figure}[htb]
\begin{centering}
{
	\begin{tikzpicture}[scale=0.75,place/.style={draw,circle,thick,text=white, text badly centered,minimum size=2.0in}]
		\node at (-2.5,-1.6) (M1) {\pgfbox[center,bottom]{\pgfuseimage{machine}}};
		\node at (2.5,-1.6) (M2) {\pgfbox[center,bottom]{\pgfuseimage{machine}}};
		\node at (0,2.9) (M3) {\pgfbox[center,bottom]{\pgfuseimage{machine}}};

		%\draw (0,0) circle (1cm);

		\draw (-4,-3) rectangle (-1.0,-1.5);
		\draw (1,-3) rectangle (4.0,-1.5);
		\draw (-1.5,1.5) rectangle (1.5,3);

		% Overall border
		\draw[draw=white] (-4.5,-3.5) rectangle (4.5,4.5);


		%\draw (-4.5,-3.5) rectangle (4.5,4.5);
		\filldraw[fill=blue,draw=black,opacity=0.2] (-3.8,-2.8) rectangle (-2.9,-1.6);
		\filldraw[gray, very thin,step=0.3cm,xshift=0.1cm,yshift=-0.1cm] (-3.9,-2.7) grid (-3.0,-1.5);
		\draw[black] (-3.8,-2.8) rectangle (-2.9,-1.6);

		\filldraw[fill=blue,draw=black,opacity=0.2] (-2.0,-2.8) rectangle (-1.1,-1.6);
		\filldraw[gray, very thin,step=0.3cm,xshift=0.1cm,yshift=-0.1cm] (-2.1,-2.7) grid (-1.2,-1.5);
		\draw[black] (-2.0,-2.8) rectangle (-1.1,-1.6);

		\node at (-1.45,-2.2) (S1) {$\mathcal{S}^{(2)}_1$};
		\node at (-3.3,-2.2) {$\mathcal{S}^{(1)}_1$};


		\filldraw[fill=red,draw=black,opacity=0.2] (3.8,-2.8) rectangle (2.9,-1.6);
		\filldraw[gray, very thin,step=0.3cm,xshift=-0.1cm,yshift=-0.1cm] (3.0,-2.7) grid (3.9,-1.5);
		\draw[black] (3.8,-2.8) rectangle (2.9,-1.6);

		\filldraw[fill=red,draw=black,opacity=0.2] (2.0,-2.8) rectangle (1.1,-1.6);
		\filldraw[gray, very thin,step=0.3cm,xshift=-0.1cm,yshift=-0.1cm] (1.2,-2.7) grid (2.1,-1.5);
		\draw[black] (2.0,-2.8) rectangle (1.1,-1.6);

		\node at (1.6,-2.2) {$\mathcal{S}^{(1)}_2$};
		\node at (3.4,-2.2) (S2) {$\mathcal{S}^{(2)}_2$};


		\filldraw[fill=green,draw=black,opacity=0.2] (-1.3,1.7) rectangle (-0.4,2.9);
		\filldraw[gray, very thin,step=0.3cm,xshift=0.2cm,yshift=0.2cm] (-1.5,1.5) grid (-0.6,2.7);
		\draw[black] (-1.3,1.7) rectangle (-0.4,2.9);

		\filldraw[fill=green,draw=black,opacity=0.2] (1.3,1.7) rectangle (0.4,2.9);
		\filldraw[gray, very thin,step=0.3cm,xshift=-0.2cm,yshift=0.2cm] (0.6,1.5) grid (1.5,2.7);
		\draw[black] (1.3,1.7) rectangle (0.4,2.9);

		\node at (-0.8,2.35) {$\mathcal{S}^{(1)}_3$};
		\node at  (0.9,2.35) (S3) {$\mathcal{S}^{(2)}_3$};
		%\node at (3.4,-2.2) {$\mathcal{S}^{(2)}_2$};


		\begin{scope}[>=stealth,ultra thick,->]
		\path (S1) edge[bend right=30]  node [left=5pt] { } (S2);
		\path (S2) edge[bend right=30]  node [left=5pt] { } (S3);
		\path (S3) edge[bend right=30]  node [left=5pt] { } (S1);

		%\path (S) edge[red,bend right=10,decorate,decoration={snake,amplitude=1mm}] 
			%node [right=5pt,text=black] {$\beta_1$ } (I1);
		%\path (I1) edge[red,bend right=10,decorate,decoration={snake,amplitude=1mm}] 
			%node [right=5pt,text=black] {$\epsilon \beta_2$ } (I12);

		%\path (I12) edge[bend right=20]     node [left=5pt] {$\delta_2$} (I1);
		
			%\path (S) edge[red,bend left=10,decorate,decoration={snake,amplitude=1mm}] 
				%node [left=5pt,text=black] {$\beta_2$ } (I2);
			%\path (I2) edge[bend left=20]     node [right=5pt] {$\delta_2$} (S);

			%\path (I2) edge[red,bend left=10,decorate,decoration={snake,amplitude=1mm}] 
				%node [left=5pt,text=black] {$\epsilon \beta_1$ } (I12);
			%\path (I12) edge[bend left=20]     node [right=5pt] {$\delta_1$} (I2);


		\end{scope}
	\end{tikzpicture}
}
\caption{Movement of parameters through the cloud between subepochs.
\label{fig:data-movement}
}
\end{centering}
\end{figure}



%%%%% GENEREAL GRID
%\begin{figure}[htb]
%\begin{centering}
%{
	%\begin{tikzpicture}[scale=1.00,place/.style={draw,circle,thick,text=white, text badly centered,minimum size=2.0in}]

		%\filldraw[gray, very thin,step=0.2cm,yshift=0.1cm] (-2.2,2) grid (2,3);
		%\filldraw[gray, very thin,step=0.2cm,xshift=-0.1cm] (-3.2,-2.2) grid (-2.2,2);

		%\filldraw[gray, very thin,step=0.2cm] (-2.2,-2.2) grid (2,2);


		%\draw[black] (-2.2,-2.2) rectangle (2,2);
		%\draw[black] (-3.3,-2.2) rectangle (-2.3,2);
		%\draw[black] (-2.2,2.1) rectangle (2,3.1);


		%\draw[black] (-0.8,-2.2) rectangle (0.6,2);
		%\draw[black] (-2.2,-0.8) rectangle (2.0,0.6);

		%\draw[black] (-3.3,-0.8) rectangle (-2.3,0.6);
		%\draw[black] (-0.8,2.1) rectangle (0.6,3.1);

	%\end{tikzpicture}
%}
%\caption{tikz grid breakdown
%\label{fig:data-movement}
%}
%\end{centering}
%\end{figure}

%\definecolor{lightgray}{gray}{0.85}
%\def\colorOpacity{0.4}
%\def\gridScaling{0.78}
%\begin{figure*}
	%\centering{
	%\begin{minipage}{0.24\textwidth}
		%\centering {
		%\begin{tikzpicture}[scale=\gridScaling,place/.style={draw,circle,thick,text=white, text badly centered,minimum size=2in}]

			%\filldraw[lightgray, very thin,step=0.2cm,yshift=0.1cm] (-2.2,2) grid (2,3);
			%\filldraw[lightgray, very thin,step=0.2cm,xshift=-0.1cm] (-3.2,-2.2) grid (-2.2,2);
			%\filldraw[lightgray, very thin,step=0.2cm] (-2.2,-2.2) grid (2,2);


			%\draw[black] (-2.2,-2.2) rectangle (2,2);
			%\draw[black] (-3.3,-2.2) rectangle (-2.3,2);
			%\draw[black] (-2.2,2.1) rectangle (2,3.1);


			%\draw[black] (-0.8,-2.2) rectangle (0.6,2);
			%\draw[black] (-2.2,-0.8) rectangle (2.0,0.6);

			%\draw[black] (-3.3,-0.8) rectangle (-2.3,0.6);
			%\draw[black] (-0.8,2.1) rectangle (0.6,3.1);

			%\tikzstyle{every node}=[font=\huge]
			%\node at (-0.1,-0.1) {$Y$};
			%\node at (-2.8,-0.1) {$\pi$};
			%\node at (-0.1,2.6) {$\beta$};
		%\end{tikzpicture}
		%}
	%\end{minipage}
	%\begin{minipage}{0.24\textwidth}
		%\centering {
		%\begin{tikzpicture}[scale=\gridScaling,place/.style={draw,circle,thick,text=white, text badly centered,minimum size=2in}]


			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (-3.3,-2.2) rectangle (-2.3,-0.8);
			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (-3.3,-0.8) rectangle (-2.3,0.6);
			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (-3.3,0.6) rectangle (-2.3,2);

			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (-2.2,2.1) rectangle (-0.8,3.1);
			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (-0.8,2.1) rectangle (0.6,3.1);
			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (0.6,2.1) rectangle (2,3.1);

			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (-2.2,0.6) rectangle (-0.8,2);
			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (-0.8,-0.8) rectangle (0.6,0.6);
			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (0.6,-2.2) rectangle (2,-0.8);


			%\filldraw[lightgray, very thin,step=0.2cm,yshift=0.1cm] (-2.2,2) grid (2,3);
			%\filldraw[lightgray, very thin,step=0.2cm,xshift=-0.1cm] (-3.2,-2.2) grid (-2.2,2);
			%\filldraw[lightgray, very thin,step=0.2cm] (-2.2,-2.2) grid (2,2);


			%\draw[black] (-2.2,-2.2) rectangle (2,2);
			%\draw[black] (-3.3,-2.2) rectangle (-2.3,2);
			%\draw[black] (-2.2,2.1) rectangle (2,3.1);


			%\draw[black] (-0.8,-2.2) rectangle (0.6,2);
			%\draw[black] (-2.2,-0.8) rectangle (2.0,0.6);

			%\draw[black] (-3.3,-0.8) rectangle (-2.3,0.6);
			%\draw[black] (-0.8,2.1) rectangle (0.6,3.1);

			%\tikzstyle{every node}=[font=\normalsize]
			%\node at (-1.5,1.3) {$\mathcal{B}_{1,1}$};
			%\node at (-0.1,-0.1) {$\mathcal{B}_{2,2}$};
			%\node at (1.3,-1.5) {$\mathcal{B}_{3,3}$};

			%\node at (-2.8,1.3) {$\mathcal{S}_1^{(1)}$};
			%\node at (-2.8,-0.1) {$\mathcal{S}_2^{(1)}$};
			%\node at (-2.8,-1.5) {$\mathcal{S}_3^{(1)}$};

			%\node at (1.3,2.6) {$\mathcal{S}_3^{(2)}$};
			%\node at (-0.1,2.6) {$\mathcal{S}_2^{(2)}$};
			%\node at (-1.5,2.6) {$\mathcal{S}_1^{(2)}$};

		%\end{tikzpicture}
		%}
	%\end{minipage}
	%\begin{minipage}{0.24\textwidth}
		%\centering {
		%\begin{tikzpicture}[scale=\gridScaling,place/.style={draw,circle,thick,text=white, text badly centered,minimum size=2in}]


			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (-3.3,-2.2) rectangle (-2.3,-0.8);
			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (-3.3,-0.8) rectangle (-2.3,0.6);
			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (-3.3,0.6) rectangle (-2.3,2);

			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (-2.2,2.1) rectangle (-0.8,3.1);
			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (-0.8,2.1) rectangle (0.6,3.1);
			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (0.6,2.1) rectangle (2,3.1);

			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (-0.8,0.6) rectangle (0.6,2);
			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (0.6,-0.8) rectangle (2.0,0.6);
			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (-2.2,-2.2) rectangle (-0.8,-0.8);


			%\filldraw[lightgray, very thin,step=0.2cm,yshift=0.1cm] (-2.2,2) grid (2,3);
			%\filldraw[lightgray, very thin,step=0.2cm,xshift=-0.1cm] (-3.2,-2.2) grid (-2.2,2);
			%\filldraw[lightgray, very thin,step=0.2cm] (-2.2,-2.2) grid (2,2);


			%\draw[black] (-2.2,-2.2) rectangle (2,2);
			%\draw[black] (-3.3,-2.2) rectangle (-2.3,2);
			%\draw[black] (-2.2,2.1) rectangle (2,3.1);


			%\draw[black] (-0.8,-2.2) rectangle (0.6,2);
			%\draw[black] (-2.2,-0.8) rectangle (2.0,0.6);

			%\draw[black] (-3.3,-0.8) rectangle (-2.3,0.6);
			%\draw[black] (-0.8,2.1) rectangle (0.6,3.1);

			%\tikzstyle{every node}=[font=\normalsize]
			%\node at (-0.1,1.3) {$\mathcal{B}_{1,1}$};
			%\node at (1.3,-0.1) {$\mathcal{B}_{2,2}$};
			%\node at (-1.5,-1.5) {$\mathcal{B}_{3,3}$};

			%\node at (-2.8,1.3) {$\mathcal{S}_1^{(1)}$};
			%\node at (-2.8,-0.1) {$\mathcal{S}_2^{(1)}$};
			%\node at (-2.8,-1.5) {$\mathcal{S}_3^{(1)}$};

			%\node at (1.3,2.6) {$\mathcal{S}_2^{(2)}$};
			%\node at (-0.1,2.6) {$\mathcal{S}_1^{(2)}$};
			%\node at (-1.5,2.6) {$\mathcal{S}_3^{(2)}$};

		%\end{tikzpicture}
		%}
	%\end{minipage}
	%\begin{minipage}{0.24\textwidth}
		%\centering {
		%\begin{tikzpicture}[scale=\gridScaling,place/.style={draw,circle,thick,text=white, text badly centered,minimum size=2in}]


			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (-3.3,-2.2) rectangle (-2.3,-0.8);
			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (-3.3,-0.8) rectangle (-2.3,0.6);
			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (-3.3,0.6) rectangle (-2.3,2);

			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (-2.2,2.1) rectangle (-0.8,3.1);
			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (-0.8,2.1) rectangle (0.6,3.1);
			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (0.6,2.1) rectangle (2,3.1);

			%\filldraw[fill=blue,draw=black,opacity=\colorOpacity] (0.6,0.6) rectangle (2.0,2);
			%\filldraw[fill=red,draw=black,opacity=\colorOpacity] (-2.2,-0.8) rectangle (-0.8,0.6);
			%\filldraw[fill=green,draw=black,opacity=\colorOpacity] (-0.8,-2.2) rectangle (0.6,-0.8);


			%\filldraw[lightgray, very thin,step=0.2cm,yshift=0.1cm] (-2.2,2) grid (2,3);
			%\filldraw[lightgray, very thin,step=0.2cm,xshift=-0.1cm] (-3.2,-2.2) grid (-2.2,2);
			%\filldraw[lightgray, very thin,step=0.2cm] (-2.2,-2.2) grid (2,2);


			%\draw[black] (-2.2,-2.2) rectangle (2,2);
			%\draw[black] (-3.3,-2.2) rectangle (-2.3,2);
			%\draw[black] (-2.2,2.1) rectangle (2,3.1);


			%\draw[black] (-0.8,-2.2) rectangle (0.6,2);
			%\draw[black] (-2.2,-0.8) rectangle (2.0,0.6);

			%\draw[black] (-3.3,-0.8) rectangle (-2.3,0.6);
			%\draw[black] (-0.8,2.1) rectangle (0.6,3.1);

			%\tikzstyle{every node}=[font=\normalsize]
			%\node at (1.3,1.3) {$\mathcal{B}_{1,1}$};
			%\node at (-1.5,-0.1) {$\mathcal{B}_{2,2}$};
			%\node at (-0.1,-1.5) {$\mathcal{B}_{3,3}$};

			%\node at (-2.8,1.3) {$\mathcal{S}_1^{(1)}$};
			%\node at (-2.8,-0.1) {$\mathcal{S}_2^{(1)}$};
			%\node at (-2.8,-1.5) {$\mathcal{S}_3^{(1)}$};

			%\node at (1.3,2.6) {$\mathcal{S}_1^{(2)}$};
			%\node at (-0.1,2.6) {$\mathcal{S}_3^{(2)}$};
			%\node at (-1.5,2.6) {$\mathcal{S}_2^{(2)}$};

		%\end{tikzpicture}
		%}
	%\end{minipage}
	
	%%$\quad\quad\quad$
	%%\begin{minipage}{0.25\textwidth}
		%%\centering{\includegraphics[page=9,width=0.95\linewidth]{figs/figures.pdf}\\(b)}
	%%\end{minipage}
	%%$\quad\quad\quad$
	%%\begin{minipage}{0.25\textwidth}
		%%\centering{\includegraphics[page=7,width=0.95\linewidth]{figs/figures.pdf}\\(c)
		%%}
	%%\end{minipage}
	%}
%\label{fig:batched}
%\end{figure*}





%\begin{figure}[htb]
%\begin{centering}
%{
	%\begin{tikzpicture}[scale=1,place/.style={draw,circle,thick,text=white, text badly centered,minimum size=0.3in}]
		%%\node[fill=blue!65,place] at (0, -2.5) (S) {\textbf{S}};
		%\node[place] at (0,-2.5) (S) {\pgfbox[center,bottom]{\pgfuseimage{machine}}};
		%\node[fill=gray,place]  at (-2.5, 0)  (I1) {$\mathbf{I_1}$}; 
		%\node[fill=gray, place]   at (2.5, 0)  (I2) {$\mathbf{I_2}$};
		%\node[fill=black, place]   at (0, 2.5)  (I12) {$\mathbf{I_{1,2}}$};
			%%\node[place] at (1, 0) (E) {\textbf{E}};
		%%\node[place] at (2, -0.5) (I) {\textbf{I}};	
		%%\node[place] at (0,-1.5) (V) {\textbf{V}};
		%\begin{scope}[>=stealth,ultra thick,->]

		%\path (S) edge[red,bend right=10,decorate,decoration={snake,amplitude=1mm}] 
			%node [right=5pt,text=black] {$\beta_1$ } (I1);
		%\path (I1) edge[bend right=20]     node [left=5pt] {$\delta_1$} (S);
		%\path (I1) edge[red,bend right=10,decorate,decoration={snake,amplitude=1mm}] 
			%node [right=5pt,text=black] {$\epsilon \beta_2$ } (I12);

		%\path (I12) edge[bend right=20]     node [left=5pt] {$\delta_2$} (I1);
		
			%\path (S) edge[red,bend left=10,decorate,decoration={snake,amplitude=1mm}] 
				%node [left=5pt,text=black] {$\beta_2$ } (I2);
			%\path (I2) edge[bend left=20]     node [right=5pt] {$\delta_2$} (S);

			%\path (I2) edge[red,bend left=10,decorate,decoration={snake,amplitude=1mm}] 
				%node [left=5pt,text=black] {$\epsilon \beta_1$ } (I12);
			%\path (I12) edge[bend left=20]     node [right=5pt] {$\delta_1$} (I2);


		%\end{scope}
	%\end{tikzpicture}
%}
%\caption{State Diagram for a node in the graph under our partial-competition model.
%% $SI_1I_2S$ competing viruses model. The node is in the $S$ state if it doesn't have either  competitor (say iPhone or Android). It is in $I_1$ if it gets the iPhone (virus 1) and is in $I_2$ if it gets the Android (virus 2). The transitions from $S$ to $I_1$ or $I_2$ (red-curvy arrows) depend on the infected neighbors of the node. The remaining transitions,  in contrast, are self-transitions, without the aid of any neighbor.
%\label{fig:siismodel}
%}
%\end{centering}
%\end{figure}



%\begin{tikzpicture}[scale=1.5]
    %% Draw axes
    %\draw [<->,thick] (0,2) node (yaxis) [above] {$y$}
        %|- (3,0) node (xaxis) [right] {$x$};
    %% Draw two intersecting lines
    %\draw (0,0) coordinate (a_1) -- (2,1.8) coordinate (a_2);
    %\draw (0,1.5) coordinate (b_1) -- (2.5,0) coordinate (b_2);
    %% Calculate the intersection of the lines a_1 -- a_2 and b_1 -- b_2
    %% and store the coordinate in c.
    %\coordinate (c) at (intersection of a_1--a_2 and b_1--b_2);
    %% Draw lines indicating intersection with y and x axis. Here we use
    %% the perpendicular coordinate system
    %\draw[dashed] (yaxis |- c) node[left] {$y'$}
        %-| (xaxis -| c) node[below] {$x'$};
    %% Draw a dot to indicate intersection point
    %\fill[red] (c) circle (2pt);
	%%\pgfuseimage{machine}
%\end{tikzpicture}


\begin{figure*}
	\includegraphics[page=1, width=0.24\textwidth]{figs/grid3.pdf} \hfill
	\includegraphics[page=2, width=0.24\textwidth]{figs/grid3.pdf} \hfill
	\includegraphics[page=13,width=0.24\textwidth]{figs/grid3.pdf} \hfill
	\includegraphics[page=17,width=0.24\textwidth]{figs/grid3.pdf} 
	\caption{Example of data and model partitioning along with the three subepochs.}
	\label{fig:partition}
\end{figure*}

In designing any large scale machine learning system, one of the most important
design decisions is the abstraction for the data and problem model.  Recent
distributed ML systems focus primarily on the abstraction for the data, such as
\graphlab's \cite{Low:DGF} view of each data point as a node or 
\psgd's \cite{zinkevich2010parallelized} view of partitioning only on the data.
Here we take a different approach - partitioning our data and the model we are building of the
data simultaneously.  As we will go on to demonstrate, focusing on both is
advantageous for both scalability and speed.

\subsection{Problem formulation}
For many classic machine learning problems, we have data about the
relationships between different types of objects and we would like to
understand these objects and their relationship to each other.  We consider we
have $t$ sets of objects $\mathcal{T}_1, \mathcal{T}_2, \ldots \mathcal{T}_t$,
each with $T_i = |\mathcal{T}_i|$ items which can be indexed from 1 to $T_i$ as $\mathcal{T}_{i,j}$.
From this we have a dataset of relations between these objects defined by
$$\mathcal{X}: \mathcal{T}_1 \times \mathcal{T}_2\times \ldots \times \mathcal{T}_n \rightarrow \mathbb{R}.$$

Given such a dataset, we would like to find a set of values for each object
to describe that object.  Therefore, we have also $t$ sets of variables
$\Theta = \{\theta_1 \ldots \theta_t\}$ where $\theta_{i,j}$ is a set of
variables (often a vector) which describe object $\mathcal{T}_{i,j}$.  For
notational simplicity, we will say that given a vector of indices
$\mathbf{i}=(i_1,i_2,\ldots i_t)$ we can reference the relevant variables by
$\Theta_{\mathbf{i}} = \{\theta_{1,\mathbf{i}_1},\theta_{2,\mathbf{i}_2}, \ldots \theta_{t,\mathbf{i}_t} \}$.

Given these defintions, we can formally define our general problem:
\begin{problem}
	Given dataset $\mathcal{X}$ defiend over $t$ types of objects
	$\mathcal{T}$, our objective is to find $\Theta$ through the following
	minimization:
	\begin{align}
		\arg\min_{\Theta} &\sum_{\mathbf{i} \in \{\mathcal{T}_1 \times \mathcal{T}_2 \times \ldots \times \mathcal{T}_t\}} f(\Theta_\mathbf{i},\mathcal{X}(\mathbf{i}))\\
		& {\rm s.t.}\; C(\Theta) = \mathbf{1},
	\end{align}
	where $f$ is any differentiable function and $C$ is a convex constraint
	across subsets of variables $\Theta$.
\end{problem}
Without loss of generality we can also set $\mathcal{T}_i = \mathcal{T}_j$ such
that we have relations between objects of the same type.  Additionally, we can
combine the loss function across multiple datasets on similar objects by merely
adding their objectives together. 

This problem defintion is very broad covering a variety of common problems as
special cases including topic modeling, dictionary learning, network
decomponsition, matrix and tensor factorization, and even mean field problems.
Throughout this paper, we will describe our system in terms of topic modeling
for clarity.  

When performing topic modeling we have two types of objects ($t=2$) - words
and documents, which we will denote by $\mathcal{T}_1 = \mathcal{W}$ and
$\mathcal{T}_2=\mathcal{D}$.  
For each document, we know the number of times a
word was used; our dataset $\mathcal{X}$ is the normalized version of these values.  
Our objective is to predict which words will be
used in which documents.  In the process, we attempt to find the topics of each
document and the words used when discussing each topic (from $K$ topics). 
To do this, we use the following Bayesian model:
\begin{align*}
	p({\rm Word}\; w | {\rm Doc}\; d) = \sum_{ {\rm Topic}\; k = 1}^K p(w | {\rm Topic} \!=\! k)\cdot  p( {\rm Topic} \!=\! k | d)
\end{align*}
The variables for the probabilities that a word is used in discussing a given
topic are denoted by $\theta_1=\pi$, where $\pi_i$ is the $K$-length vector of
probabilities $p(w_i|{\rm Topic}= k)$ for all $k$.  Similarly the variables for
the probabilities that a document is about a given topic are denoted by
$\theta_2=\beta$ where $\beta_j$ is the $K$-length vector of probabilities
$p(d_j|{\rm Topic}=k)$ for all $k$.

From this, we have a similar problem statement as before but specific to topic modeling:
\begin{problem}
	Given a word $\times$ document dataset $\mathcal{X}$, find
	\begin{align}
		\arg\min_{\pi,\beta} &\sum_{w_i \in \mathcal{W},d_j \in \mathcal{D}} f(\pi_i,\beta_j,\mathcal{X}(i,j))\\
		f(\pi_i,\beta_j,x) &= (x - \pi_i^\top\beta_j)^2\\
		{\rm s.t.}\; &\sum_{k=1}^K \beta_{j,k} = 1,\; \forall j\in \mathcal{D}\\
		&\sum_{i \in \mathcal{W} } \pi_{i,k} = 1,\; \forall k = 1 \ldots K
	\end{align}
\end{problem}
Solving this produces a model of the topics of our documents and the use of our
words.  In particular, we are left with a model of the use of words in topics
given by $\pi$, which can be used to find the topics of new documents. 

In the abstract 
formulation of the problem earlier we introduced types $\mathcal{T}$. It is important to note 
that some of these types are an exclusive construct of the model e.g. the topic-word 
type $\beta$ in topic modelling. These types are called parameters of the model.



\subsection{Relational block structure} % (fold)
\label{sub:Block structure in relational data}

%For many classic machine learning problems, we have relational data between
%two sets of entities, and we are asking questions about these entities. \abhi{my editing is here} 
%Answers to 
%these questions are obtained by estimating a set of unknowns ($\gamma$) that define the entity relations
%concretely. These unknowns can be categorized into two sets: 1) variables ($\gamma_{var}$) and 
%2) parameters ($\gamma_{par}$). 
%Variables are set of unknowns that arise in relation to the data while parameters are set of unknowns that 
%arise in relation to the assumptions that one makes in the entity relationship (i.e. the model). In case of
%topic model, the topic proportions of a document are the variables. Each document (data) has 
%different topic proportion and 
%these unknown proportions are estimated by the model. The probability of a word in a 
%given topic is a parameter and this probability is same for all the documents. The parameters 
%define the model that is learnt.  
%The overall unknowns set (variable and parameters) can be divided into relational blocks ($(v,p)$) of independent set of 
%unknowns($\gamma_{v,p}$) for most problems. Each relational block is indexed by $v$ and $p$ that corresspond to the 
%subsets of variables and parameters present in the block respectively. Taking the topic modelling example,
%if the choose a particular subset of document-topic probability unknowns indexed by $v$ and 
%topic-word probability unknowns indexed by $p$, then inner product (along the common topics) 
%of this set of document-topic unknowns 
%with the corressponding set of topic-word unknowns gives a block of data $Y$ (document-word probabilities). 
%The function $F(\gamma)$ to be optimized can be cleanly 
%broken into sum of smaller functions ($f_{v,p}(Y, \gamma_{v,p})$) that are defined only for each individual relation block.
%\begin{eqnarray}
%F(\gamma) = \sum_{v,p} f_{v,p}(Y, \gamma_{v.p}) \newline
%s.t. \gamma_{v,p} \in C
%\label{eqn:abstractFunction}
%\end{eqnarray}
%All the relational blocks in the end are combined together using a set of constraints $C$. 
%To be a
%bit more concrete, consider again the classic topic modeling question: given a large set
%of $D$ documents, with what probability a given document talks about a given
%topic, and with what probability does a given topic has a given word~\cite{Blei:2003:LDA}? 
%In this case, a raw data point is the number of occurrences of a word in a
%document. Each data point is an element in the document by word data matrix. We
%normalize the data matrix to obtain matrix $Y$ where an element
%$Y_{i,j}$ in column $i$ and row $j$ in the new matrix is the original
%element at $(i,j)$ divided by the row $i$'s sum. In simple terms,
%$Y_{i,j} = p(j|i)$ i.e. the probability of word $j$ lying in
%document $i$.

%\lda (topic modelling) assumes that there are fixed number ($K$) of
%underlying topics in the document corpus. A topic is composed of weighted
%combinations of words in the corpus where different words have different
%weights. The weight of a word $j$ in topic $k$  is $p(j|k)$: probability of
%word $j$ in topic $k$. Each document in the corpus is a weighted combination of
%the topics. The weight of topic $k$ in document $i$ is $p(k|i)$. The probability
%of a word $i$ in document $j$ is 
%\begin{equation}
%p(j|i)=\sum_{k=1}^Kp(j|k)*p(k|i)
%\end{equation}



%We define two matrices $\pi$ and $\beta$ such that $\pi_{i,k}=p(k|i)$ and
%$\beta_{k,j}=p(j|k)$. In that case $Y_{i,j}=\sum_k\pi_{i,k}*\beta_{k,j}$.
%The $\pi$ matrix is the per document topic probability matrix. $\pi_{i,k}$ is
%the probability weight of topic $k$ in document $j$. $\beta$ matrix is the per
%topic word probability. $\beta_{k,j}$ is the probability weight of word $j$
%in topic $k$. 
%\abhi{my edit here}
%Here $\pi$ is the set of variables and $\beta$ is the set of parameters. $\beta$, the 
%topic-word matrix is depedent on the model and is common for all documents. $\pi$, the 
%document-topic matrix is dependent on the documents in the corpus and if documents change 
%over time as in online topic modelling (\cite{online-lda}). The $\pi$ is the set of variables
%as explained earlier.

%In \lda we are solving the following query to find $\pi$ and
%$\beta$.

%% {\small
%\begin{align}
%&\argmin_{\pi,\beta}L(Y,\pi,\beta)
%=\sum_{i,j}(Y_{i,j}-\sum_k\pi_{i,k}\beta_{k,j})^2
%%= \sum_{i,j}(Y_{i,j}-\sum_k \pi_{i,k}\beta_{k,j})_p^p
%\label{eqn:LDA}\\
%&\text{s.t.} \; \forall  i,j,k \quad
%\sum_k\pi_{i,k}=1,
%\sum_j\beta_{k,j}=1, \quad
%\pi_{i,k}\geq 0,
%\beta_{k,j}\geq 0,
%\nonumber
%\end{align}
%% }

%The second line of constraints in equation~\ref{eqn:LDA} is due to the fact that
%$\pi$ and $\beta$ are probability matrices. This corressponds to the constraint set $C$ defined 
%in equation \ref{eqn:abstractFunction}. 

Because of the intrinsic relational nature of the data (here between words and
documents), partitioning our data  intelligently results in a clean
partitioning of our model of topics for the words and documents.

More specifically, for any given subset of the documents and subset of the
words, there is a unique set of data points that are applicable to entities from
both sets.  If we view our data as a very sparse matrix $Y$ (where $Y$ is a
documents by word matrix), partitioning the documents and the words results in
the data matrix $Y$ also being partitioned into blocks, as seen in Figure
\ref{fig:abstraction}.
Under this partitioning, all data points in block $\mathcal{B}_{i,j}$ describe
a relationship between a word from set $\mathcal{S}^{(1)}_i$ and document from
set $\mathcal{S}^{(2)}_j$.  We focus our computation around these blocks of data.

An interesting property of these blocks is that for some blocks, such as
$\mathcal{B}_{i,j}$ and $\mathcal{B}_{i',j'}$ where $i\neq i'$ and $j \neq j'$,
we see that the blocks are \textit{independent}.  That is, a data point from
$\mathcal{B}_{i,j}$ does not describe any relations to words in
$\mathcal{S}^{(1)}_{i'}$ or documents in $\mathcal{S}^{(2)}_{j'}$ and vice
versa for data in $\mathcal{B}_{i',j'}$.
%Formally defined:
%\alex{add formal definition of independence here? or will that come off too
%mathy}
As has been shown in previous stochastic learning literature \cite{textbook}
and used in simpler data mining problems \cite{gemulla,flexifact}, this
independence property allows for improved scalability and parallel processing.
We will discuss our system and the strengths of this abstraction primarily in
terms of topic modeling,
%Note, we will primarily discuss
%this system in terms of topic modeling, 
but note the system generalizes to many
machine learning problems as we will demonstrate later.
%We go over the implications for topic modeling and other machine learning
%problems below.

\subsection{Distributing our data} % (fold)
\label{sub:Distributing the Data and Model}

As mentioned above, distributing your data over a cluster is useful, but as
data grows partitioning both the data and your model of that data is
increasingly valuable.  For example, if we would like to know the topic
distribution for 10 million documents over 1000 topics, this would require over
37 gigabytes just to hold all of the answers to the query.  Therefore, it is
crucial that as our data grows we intelligently distribute both the data and our
solution.  This significantly improve memory efficiency and makes it possible
to scale to unprecedented sizes.  Luckily, our blocking abstraction makes this
easy.

In processing a given block $\mathcal{B}_{i,j}$, we only need the data from
that block, and the current information about the words $\mathcal{S}^{(1)}_{i}$
and documents $\mathcal{S}^{(2)}_{j}$.  Therefore, in distributing the problem
over a cluster of machines, we can have each worker only store and process one
block and its corresponding object model at a time.  Additionally, because our
sets of documents and sets of words are each disjoint, we can process our
blocks in such a way that each document and each word is only being worked on
by one worker at a time.  As a result, we do not need to store \textit{any}
duplicate data (about the topic model or blocks) and thus we are perfectly
memory efficient.


% subsection Distributing the Data and Model (end)

\subsection{Parallel processing} % (fold)
The last piece of the general system design is understanding the order in which
we process our data.  Our goal is to reach the optimal memory efficiency
described above and also keep our computation fast and accurate.  To do this,
we must choose a \textit{stratum}, a group of blocks, to process in parallel.  
In order to be memory efficient and keep our computation accurate, each stratum
must only contain blocks that are independent of each other.  From the block
structure, we can create multiple strata such that each block is in exactly one
stratum.
We iterate over the strata, in each case processing each block
in the stratum in parallel on the cluster.  We call processing one stratum a
\section{Learning}
\label{sec:learning}

The model and data abstraction defined in section~\ref{sec:mdAbstract} is a
convenient view for distributed computing of a large scale machine learning (ML)
problem. This abstraction can be separated logically from the way the learning
is achieved in an ML algorithm. The partition abstraction is used
to distribute the learning phase over different computing units. For example in the 
\lda problem
discussed in the abstraction section~\ref{sec:mdAbstract} earlier, we can use
different learning algorithms to obtain the solution. In particular we can use
either of the two general learning schemes: 1) Sampling based learning, 2) Gradient based learning. These
two learning schemes differ in the way they treat the given problem.  

\subsection{Sampling based learning scheme}

Sampling based approach in general takes a probabilistic view of the problem.
It learns the parameters of the model by approximating the probability
distribution of the true data. This scheme samples
iteratively from a changing distribution that is affected by the samples drawn.
The distribution eventually reaches an equilibrium. The parameters of this
stationary distribution are our solutions (parameters of the model) and are used
further down the pipeline for prediction. In case of \lda such a scheme
progresses iteratively. We start with some initial values of $\pi$ and $\beta$
defined in equation~\ref{eqn:LDA}. We assume that each word occurrence has a hidden
indicator $z$ (an artificial construct of the learning paradigm) that provides
the topic it lies in. We sample values of $z$ for each word occurrence
over all the documents based on $\pi$ and $\beta$ values. These $z$s are
used to obtain new values for $\pi$ and $\beta$. This goes on iteratively until
the distribution that is characterize by $\pi$ and $\beta$ is not changing
anymore.

% \abhi{explain the
% gibbs sampling steps for \lda here. For that we need to discribe the objective
% function for \lda in section 2}.

\subsection{Gradient based learning}
In gradient based learning an objective function is optimized to find the
parameters of the model. The optimization procedure is formulated in a way to
abstract away the probabilistic components of the model, if any present. For
example incase of \lda we optimize objective defined in equation~\ref{eqn:LDA} by
computing the derivatives $\partial L_{\pi}$ and $\partial L_{\beta}$ with respect
to $\pi$ and $\beta$ respectively. The new values $\pi^{'}$ and $\beta^{'}$ are
\begin{align*}
&\pi^{'}_{i,k} = \pi_{i,k} - \eta\partial L_{\pi_{i,k}} \\ 
&\beta^{'}_{k,j} = \beta_{k,j} - \eta\partial L_{\beta_{k,j}}\\
&where~\partial L_{\pi_{i,k}} = 2*\sum_{j}(Y_{i,j} -
\sum_{k}\pi_{i_k}\beta_{k,j})\beta_{k,j} \\ 
&\hspace{2cm} and\\
&\partial L_{\beta_{k,j}} = 2*\sum_{i}(Y_{i,j} -
\sum_{k}\pi_{i_k}\beta_{k,j})\pi_{i,k}
\end{align*}
Here $\eta$ is the step size in the learning scheme which is a constant
provided by the algorithm.

\abhi{convert $\pi$ and $\beta$ into more databasey terms like relations or
probabilities}

\subsubsection{Stochastic gradient descent}
A roadblock for gradient based learning for large scale data is the sum over
all $i$ or $j$ in computation of $\partial L_{\pi_{i,k}}$ or $\partial
L_{\beta_{k,j}}$. When the dimensions of data $Y$ is large, the summation is
computationaly very expensive. To overcome this large scale ML algortihms
typically use stochastic gradient descent~\cite{Kushner:yin} thats removes the
summation. The gradient with respect to $\pi$ or $\beta$ becomes 

\begin{align}
&\partial L_{\pi_{i,k}} = 2*(Y_{i,j} -
\sum_{k}\pi_{i_k}\beta_{k,j})\beta_{k,j} \nonumber\\  
&\hspace{2cm} and \nonumber\\
&\partial L_{\beta_{k,j}} = 2*(Y_{i,j} -
\sum_{k}\pi_{i_k}\beta_{k,j})\pi_{i,k}
\label{eqn:sgd}
\end{align}

This approach takes one single element from the input matrix at a time and
applies equation~\ref{eqn:sgd} to get the new values for $\pi$ and $\beta$. It is
theoretically guaranteed to converge~\cite{Kushner:yin} and hence used quiet
often in large scale machine learning problems. Further it can be seen
that it is easily parallelizable across two different $\pi$-rows $\pi_{i,*}$
and $\pi_{j,*}$. \method leverages this property to distribute in model as well
as data space.


%In this section, we describe how optimization techniques, and in particular
%stochastic gradient descent, can be used to fit a variety of machine learning
%models.
%
%\subsection{SGD Background} % (fold)
%\label{sub:SGD Background}
%
%% subsection SGD Background (end)
%
%\subsection{Dictionary Learning} % (fold)
%\label{sub:Dictionary Learning}
%
%% subsection Dictionary Learning (end)
%
%\subsection{Topic Modeling} % (fold)
%\label{sub:Topic Modeling}
%
%% subsection Topic Modeling (end)
%
%\subsection{Mixed Membership Stochastic Block Models} % (fold)
%\label{sub:Mixed Membership Stochastic Block Models}
%
%% subsection Mixed Membership Stochastic Block Models (end)

\section{Answering Complex Questions}
\label{sec:complexQues}
So far, our system can successfully separate computation on different parts
of the data and distribute it over a cluster.  As was described previously,
this design works for simple computation such as matrix
factorization, but most questions we have, such as sparse non-negative matrix
factorization or topic modeling, are more complex and require a more robust
system.  Handling more complex questions is crucial for modern machine
learning, so we must build on our system to make it versatile.

\subsection{Projecting into the solution space} % (fold)
\label{sub:Projecting into the solution space}


In our topic modeling problem, 
%question when we want to calculate
%the probability that a document is about a given topic, 
the probabilities that a document is about different topics must be non-negative
and sum to 1.  
Therefore, in using the iterative learning methods outlined above, we must
continuously project our current model into the appropriate solution
space as we perform updates.
For example, to make sure the model is non-negative we can always project
negative values to 0.  To make sure the probabilities sum to 1, we can
occasionally normalize the values.

%To stay within this space, we can
%always set parameters to 0 if they go negative, and normalize our probabilities
%when they do not sum to 1 after an update.
\textit{Projections} of this form are quite versatile and enable our system to
handle a wide variety of complex questions and learn many classic machine
learning models.
Additionally, we recently proved in \cite{flexifact} that our distributed
parallel system still provably converges even when performing projections
during SGD.
We outline a few examples of useful projections below.

%To handle complex questions in our system, we need to be able to
%perform \textit{projections} during our computation.  

%Projections make sure our
%solutions are of the desired form and cover a wide range of machine learning
%problems.  

%{\bf Non-negative solutions}

%{\bf Sparse models}

%{\bf Simplex constraints}

%{\bf Distributed Simplex Constraints}

\paragraph{Non-negative solutions} % (fold)
\label{par:Non-negative solutions}
One of the simplest constraints requires that our model be non-negative.  This
is useful for a number of different reasons.  In topic modeling, as described
above, our model represents a set of probabilities and thus should be
non-negative.  More simply, it is common to want a non-negative matrix
factorization to make the results actually interpretable.

As was mentioned above, we can enforce non-negativity constraints by simply
projecting negative values to 0:
\begin{align}
	\Theta(x) =
	\left\{
	\begin{array}{ll}
		x  & \mbox{if } x \geq 0 \\
		0 & \mbox{if } x < 0
	\end{array}
	\right.
\end{align}
Therefore, whenever we update a part of the model, say $\beta_{k,j}$, the
probability that word $j$ is in topic $k$, we would perform the projection on
that parameter, $\beta_{k,j} = \Theta(\beta_{k,j})$.  This is simple as it can
be included in the local update.

% paragraph Non-negative solutions (end)

\paragraph{Sparse Models} % (fold)
\label{par:Sparse Models}
Another common constraint is to desire a sparse model, such that most of our
model is zero.  In the context of topic modeling, this would mean that we want
to push each document to be about only a few topics (and thus the probability
that the document is about most of the topics is 0).  If we have a set of 500
or 1000 topics, this makes it easier to interpret what topics a document is
about.  It is common to enforce sparsity constraints in everything from basic matrix
factorization to more complex topic models.  

To encourage sparsity, mathematically, we include an $\ell_1$-norm penalty on
our model in the objective function.  In more practical terms, this
$\ell_1$-penalty results in a \textit{soft-thresholding} of our parameters.
That is, whenever we update our parameters we push them toward zero with the
soft-thresholder:
\begin{align}
	S_\lambda(x) =
	\left\{
	\begin{array}{ll}
		x-\lambda  & \mbox{if } x > \lambda \\
		x+\lambda  & \mbox{if } x < -\lambda \\
		0 & \mbox{if } -\lambda \leq x \leq \lambda
	\end{array}
	\right.
\end{align}
In our system, we would apply this projection whenever we update any parameters
for which there is an $\ell_1$ penalty on that part of the model.  We will show
later that this is particularly useful for dictionary learning.

% paragraph Sparse Models (end)

\paragraph{Simplex Constraints} % (fold)
\label{par:Simplex Constraints}
In machine learning problems, we are most often calculating probabilities to
estimate other probabilities.  As explained before, we use the probability that
a document is about a topic, and the probability that a word is from topic, to
estimate the probability that a word is in a document.
Unfortunately, many of the learning methods above do
not naturally treat the values they are learning as probabilities, so we need
to continuously enforce the constraints to keep the results focused.

To do this, we enforce simplex constraints - that certain sets of values sum to
1 - as we saw in the topic modeling definition above.  In particular, for topic
modeling for each document $i$, the vector of probabilities of topics for that
document should sum to 1: $\sum_k \pi_{i,k} = 1$. 
Luckily, when using SGD to process data point $Y_{i,j}$ we see that we update the
probabilities for document $i$ and word $j$.  As a result, we can simply
normalize row $\pi_i$ whenever we perform and update on it:
\begin{align}
	\pi_i = \frac{\Theta(\pi_i)}{\|\Theta(\pi_i)\|_1}
\end{align}
As we will show later, this is very useful in a variety of machine learning
applications.

% paragraph Simplex Constraints (end)

\paragraph{Projecting on the unit ball}
A similar, but slightly different constraint is that our result vectors must fall within the unit ball.
As a result, when a vector is updated to a length $>1$ we must project the
vector back onto the unit ball.
This is generally of a similar form as the simplex constraint above, but is
used in dictionary learning.  The projection function here is
\begin{align}
	b(x) = 
	\left\{
	\begin{array}{ll}
		x  & \mbox{if } \|x\|_2 \leq 1 \\
		\frac{x}{\|x\|_2} & \mbox{otherwise}
	\end{array}
	\right.
\end{align}

\paragraph{Distributed Simplex Constraints} % (fold)
\label{par:Simplex Constraints}
So far we have focused on local constraints, where our projections only effect
the part of the model being updated from the current data point.  However, this
is not always the case.  For example, in topic modeling, we also want to know
what is the probability that a word is used when discussing a given topic.
This is coded as a vector of probabilities of words for each topic, $\beta_k$.

When we observe a data point $Y_{i,j}$ and update the
probabilities associated with word $j$, we break the simplex requirement for
all of the topic-word vectors.  As with the previous simplex constraints, we would
like to re-normalize the values in each topic-word vector.
Unfortunately, in our distributed block model, the
probabilities of all of the words for a topic $k$ are distributed over the
entire cluster, thus making normalization a distributed process.  

In \cite{flexifact} we showed that when using SGD, we can normalize these
vectors not even on every updated but every so often and still converge.
%In practice after each subepoch we perform a distributed synchronized normalization.  
We describe the details of how we do this in practice in the following section.

% paragraph Simplex Constraints (end)


\subsection{Distributed Normalization} % (fold)
\label{sub:Distributed Normalization}
As described above, performing local projections is relatively straightforward
and we have proved in the past that our method converges under such
projections.  However, in many cases we need to coordinate our parameters
across our machines, such as when we want to normalize across machines.

In practice, we perform our distributed normalize after every subepoch.  To be
more specific, in a given subepoch we wait for all machines to finish the
updates based on the data in their current block.  Once all machines have
finished their updates, we normally would transfer the appropriate parts of the
factor matrices to the appropriate machines.  When performing a distributed
normalization, each machine takes the sum of their relevant vectors and stores
them separately for communication.  To be more specific, in topic modeling each
machine would want the sum of values for a topic $k$ and thus takes the sum the
values of $\pi_{i,k}$ for all words $i$ stored on that machine.  This results
in a $K$-length vector of sums on each machine.  While the larger factor
matrices are being passed transferred, the machines also distribute their
sum-vector to every machine.  The movement of data after a subepoch can be seen
in Figure \ref{fig:distributed-normalization}.

Therefore, when a machine receives its new factor matrices and {\it all} of the
sum-vectors (one from every machine in the cluster), it can perform the
appropriate normalization.  That is, the machine can sum the sum-vectors
leaving a normalization term $n_k$ for each topic $k$.  As such, in
reading in the new factor matrices, the machine can normalize the terms by
setting $\pi_{i,k} = \pi_{i,k}/n_k$.  As a result, each subepoch starts with
the simplex constraint being met for all topics $k$: $\sum_i \pi_{i,k} = 1$.

As mentioned earlier, this projection is of course less frequent and more
expensive than the local projections described earlier.  However, as proven in
\cite{flexifact} the algorithm still converges appropriately.

By being able to perform projections and still provably converge, our algorithm
can answer a variety of questions and handle a wide range of machine learning
applications, as will be discussed in Section \ref{sec:applications} and
demonstrated in Section \ref{sec:eval}.


% subsection Distributed Synchronization (end)



%From sparse non-negative matrix factorization to topic modeling, 


%We now outline the our approach to using SGD for machine learning at a large
%scale while maintaining its speed.
%
%\subsection{Data and Model Partitioning} % (fold)
%\label{sub:partition}
%
%% subsection Data Blocking (end)
%
%\subsection{Movement of Data and Parameters}
%\label{sub:flow}
%
%% subsection Flow of data  (end)
%
%%\subsection{Model Distribution} % (fold)
%%\label{sub:Model Distribution}
%
%%% subsection Model Distribution (end)
%
%\subsection{Synchronization} % (fold)
%\label{sub:Synchronization}
%
%% subsection Synchronization (end)
%
%\subsection{Always-On SGD} % (fold)
%\label{sub:Always-On SGD}
%
%% subsection Always-On SGD (end)

\section{Running over Barriers}
\label{sec:overBarriers}
So far, our system design distributes our data and the model we are calculating
so that our system is memory efficient and highly scalable.  We also show how
even in our distributed system, we can answer complex queries through a variety
of machine learning models.  However, the design as described so far has one
weakness - the system processes subepochs synchronously with a barrier between
each subepoch.  While theoretically this may not be a big deal, in real world
clusters some machines may be worse or shared with other jobs resulting in
slower computation for our algorithm.  This results in some machines being
available but being forced to wait around and waste valuable computing time.

To overcome this bottleneck, we developed a technique called \textit{Always-On
SGD}, where we let machines keep running when they hit a barrier.
More specifically, when a machine hits a barrier and needs to wait on other
machines to reach the barrier, the machine can keep making progress on the
problem and performing valuable computation.  At the point of hitting the
barrier, the machine has processed and performed updates from all of the points
in some block $\mathcal{B}$.  But, because it is an iterative algorithm, and
better yet a stochastic one, we can shuffle the points in $\mathcal{B}$ and
process them again.  In processing the points again we continue to take
valuable steps with the gradient toward the solution.  Because we expect we are
getting closer to the solution, we begin to decrease the step size of our
updates, but this is normal behavior as a gradient descent algorithm runs.
We can keep using the points in $\mathcal{B}$ to perform updates until all of
the machines have reached the barrier and are ready to proceed to the next
subepoch.

We have recently proved \cite{aistats} that Always-On SGD still converges and
\ldots.  
We will discuss in Section \ref{sec:eval} the comparison in
convergence time using this method compared to the synchronous version, as well
as show the amount of time wasted when Always-On is not used.
With this set of design decisions, our system is scalable, versatile, and fast.  
\alex{abhi, can you add to the first sentence to this last paragraph about the
impact of our proof.}

%\section{Related Work} % (fold)
%\label{sec:related}
%\input{090Related}
%
%
%\section{Optimization Approach to ML Models} % (fold)
%\label{sec:optimization}
%\input{030Optimization}
%
%
%\section{Our Approach} % (fold)
%\label{sec:approach}
%\input{040Approach}
%
%
%\section{System Design and Implementation} % (fold)
%\label{sec:system}
%\input{050System}


\section{Implementation} % (fold)
\label{sec:implementation}
So far we have primarily discussed our sytem in terms of the high level design.
The design choices made so far are largely indepdent of the impelmentation
decisions and should make for an efficient, scalable system, regardless of
whether it is implemented with MPI or Hadoop.

For portability reasons, we build our framework on top of Apache's Hadoop and
Hadoop File System (HDFS).  Although this platform is typically not geared
towards iteratitve distributed machine learning algorithms, we demonstrate that
the tools in the system offer many advantages such as ease to set up,
robustness, and ubiquity in industry, while still enabling our framework to
work accurately and efficiently.  Our implementation uses \textit{stock
Hadoop}, enabling it to be run on any standard Hadoop cluster without requiring
additional software to be installed on the cluster.

At a high level, each epoch is one MapReduce job.  We use Hadoop's mappers to
partition our data and the reducers to perform most of the SGD computation.
Additionally, we use direct access to HDFS to communicate between different
reducers.

\subsection{Distributing the data} % (fold)
\label{sub:Ordering the data}
As was described in Section \ref{sec:mdAbstract}, our system works by dividing
the data into blocks, grouping the blocks into strata, distributing the strata
across the cluster of machines, and processing the strata in order.  To do this
in Hadoop, we use the mappers to (1) read in our data, which is unorganized,
(2) determine the block that the data points fall in, (3) send the blocks to the
appropriate reducer, and (4) order the blocks in each reducer correctly.

In order to set up the system appropriately
We assume we have a data matrix $P$ of size $N \times M$ and we are setting
Hadoop to have $d$ reducers, such that our computation will run over $d$
machines.
Blocks are of size $\lceil N/d \rceil \times \lceil M/d \rceil$.
Blocks with index $b_i$ cover $i\in[b_i\lceil N/d \rceil\ldots (b_i+1),\, N/d
\rceil)$ and blocks with index $b_j$ cover $j \in [b_j\lceil M/d \rceil,\,
(b_j+1)\lceil M/d \rceil)$.
The data comes in as a triplet $\langle i,j,P_{i,j}\rangle$.  We can calculate
which block $\mathcal{B}_{b_i,b_j}$ the point falls into by setting
\begin{align}
	b_i = \left\lfloor \frac{i}{\lceil N/d \rceil} \right\rfloor
	\;\;\;\;\;\;
	\mbox{and}
	\;\;\;\;\;\;
	b_j = \left\lfloor \frac{j}{\lceil M/d \rceil} \right\rfloor.
\end{align}




We design our strata by beginning with the blocks along the diagonal and then
rotating the diagonal around the matrix, as can be seen in Figure
\ref{fig:blocks}.  This maintains the property that all blocks in each stratum
are independent and the order is also easy to calculate.  Additionally, we can
then assign one row of blocks to each reducer.  Doing this is both simple and
means that we do not need to ever transfer the factors for the left side of the
matrix (e.g. the document model when doing topic modeling) between reducers.

Within the row of blocks to be received by each reducer, we can order the
blocks by finding the stratum number they correspond to.
The stratum order $s$ can be calculated as:
\begin{align}
	s= (b_j - b_i + d) \bmod{d}.
\end{align}
\alex{I am thinking we should move much of the above description to section 2.
it will make the problem much more concrete and we can even show it visually.}

\alex{include detailed description of what do we set as the key and value?}

Based on this desired processing the data, we would like each reducer to
receive the data for one row of blocks, and for the data going to that reducer
to be ordered by the stratum number $s$.  Hadoop makes this quite easy.  To
send the data to the correct reducer, we modify Hadoop's default partitioner to
use the value of $b_i$ as the partition number.  To order the points by $s$ we
can then modify the KeyComparator.  As such, each reducer only receives points
for the same $b_i$ and the blocks are ordered by the stratum number.
\alex{check this}

\subsection{Main Computation} % (fold)
\label{sub:Computation}

Within each reducer, the computation is relatively straightforward.  As we see
in Algorithm \ref{algo:reducer}, we process the data points in a streaming
fashion from using the reducer's iterator.  In most cases, processing a point
consists of performing the SGD update on the relevant portions of our model and
then performing any local projections that go with the update.  

When using our Always-On SGD, we also keep a linked list of the points in the
current block.  Once we have gone through all of the points in the block, we
shuffle the linked list and again process all of the points from the list.  We
do this repeatedly until all reducers have processed the points in their
current block at least once.

%\subsection{Model Synchronization} % (fold)
%\label{sub:Synchronization}

\paragraph{Parameter Communication} % (fold)
\label{par:Model Movement}
Besides processing the points, the reducers are also responsible for passing
the necessary parameters among the reducers between subepochs.  
To do this requires two steps.  First, each reducer must be able to check when
the other reducers have finished processing their points at least once.  To
accomplish this, when reducer $r$ has
processed all of the points in its current stratum $s$ at least once, it
touches on HDFS a unique file named by $\langle r,s \rangle$.   After this, the
reducer polls HDFS every 3 seconds to check if there is a file for each reduer
for the current stratum $s$.  If so, then the subepoch is over and the reducers
can sync their paramters.

To sync their parameters, each reducer writes its paremeters from
$\mathcal{S}^{(2)}$ to HDFS in a unique file (again named by the reducer $r$
and the stratum $s$).  Each file will contain a portion of the matrix
$\mathcal{S}^{(2)}$, so when serialized appropriately, each file is fairly
small (\alex{5kb?}).  After its data has been written, the reducer polls HDFS
waiting for the block of $\mathcal{S}^{(2)}$ it needs for stratum $s+1$.  Once
the file appears, the reducer can read it into memory and begin with the next
subepoch.  This passing of parameters can be seen in Figure
\ref{fig:data-movement}.

\begin{figure}[htb]
\begin{centering}
{
	\begin{tikzpicture}[scale=0.75,place/.style={draw,circle,thick,text=white, text badly centered,minimum size=2.0in}]

		\begin{scope}[>=stealth,ultra thick,->]
		\path (S1) edge[lightgray, very thick,bend right=30]  node [left=5pt] { } (S2);
		\path (S2) edge[lightgray, very thick,bend right=30]  node [left=5pt] { } (S3);
		\path (S3) edge[lightgray, very thick,bend right=30]  node [left=5pt] { } (S1);
		\end{scope}


		\node at (-2.5,-0.9) (M1) {\pgfbox[center,center]{\pgfuseimage{machine}}};
		\node at (2.5,-0.9) (M2) {\pgfbox[center,center]{\pgfuseimage{machine}}};
		\node at (0,3.6) (M3) {\pgfbox[center,center]{\pgfuseimage{machine}}};

		%\draw (0,0) circle (1cm);

		\draw (-4,-3) rectangle (-1.0,-1.5);
		\draw (1,-3) rectangle (4.0,-1.5);
		\draw (-1.5,1.5) rectangle (1.5,3);

		% Overall border
		\draw[draw=white] (-4.5,-3.5) rectangle (4.5,4.5);


		%\draw (-4.5,-3.5) rectangle (4.5,4.5);
		\filldraw[fill=blue,draw=black,opacity=0.2] (-3.8,-2.8) rectangle (-2.9,-1.6);
		\filldraw[gray, very thin,step=0.3cm,xshift=0.1cm,yshift=-0.1cm] (-3.9,-2.7) grid (-3.0,-1.5);
		\draw[black] (-3.8,-2.8) rectangle (-2.9,-1.6);

		\filldraw[fill=blue,draw=black,opacity=0.2] (-2.0,-2.8) rectangle (-1.1,-1.6);
		\filldraw[gray, very thin,step=0.3cm,xshift=0.1cm,yshift=-0.1cm] (-2.1,-2.7) grid (-1.2,-1.5);
		\draw[black] (-2.0,-2.8) rectangle (-1.1,-1.6);

		\node at (-1.45,-2.2) (S1) {$\mathcal{S}^{(2)}_1$};
		\node at (-3.3,-2.2) {$\mathcal{S}^{(1)}_1$};


		\filldraw[fill=red,draw=black,opacity=0.2] (3.8,-2.8) rectangle (2.9,-1.6);
		\filldraw[gray, very thin,step=0.3cm,xshift=-0.1cm,yshift=-0.1cm] (3.0,-2.7) grid (3.9,-1.5);
		\draw[black] (3.8,-2.8) rectangle (2.9,-1.6);

		\filldraw[fill=red,draw=black,opacity=0.2] (2.0,-2.8) rectangle (1.1,-1.6);
		\filldraw[gray, very thin,step=0.3cm,xshift=-0.1cm,yshift=-0.1cm] (1.2,-2.7) grid (2.1,-1.5);
		\draw[black] (2.0,-2.8) rectangle (1.1,-1.6);

		\node at (1.6,-2.2) {$\mathcal{S}^{(1)}_2$};
		\node at (3.4,-2.2) (S2) {$\mathcal{S}^{(2)}_2$};


		\filldraw[fill=green,draw=black,opacity=0.2] (-1.3,1.7) rectangle (-0.4,2.9);
		\filldraw[gray, very thin,step=0.3cm,xshift=0.2cm,yshift=0.2cm] (-1.5,1.5) grid (-0.6,2.7);
		\draw[black] (-1.3,1.7) rectangle (-0.4,2.9);

		\filldraw[fill=green,draw=black,opacity=0.2] (1.3,1.7) rectangle (0.4,2.9);
		\filldraw[gray, very thin,step=0.3cm,xshift=-0.2cm,yshift=0.2cm] (0.6,1.5) grid (1.5,2.7);
		\draw[black] (1.3,1.7) rectangle (0.4,2.9);

		\node at (-0.8,2.35) {$\mathcal{S}^{(1)}_3$};
		\node at  (0.9,2.35) (S3) {$\mathcal{S}^{(2)}_3$};
		%\node at (3.4,-2.2) {$\mathcal{S}^{(2)}_2$};


		\begin{scope}[>=stealth,ultra thick,->]
			%\tikzstyle{every node}=[font=\large]
			\path (M1) edge[very thick,bend right=-10]  node [left=0pt,above=-2pt] {$\sigma^{(1)}$} (M2);
			\path (M1) edge[very thick,bend right=-10]  node [left=2pt] {$\sigma^{(1)}$} (M3);
			\path (M2) edge[very thick,bend right=-10]  node [left=0pt,below=-2pt] {$\sigma^{(2)}$} (M1);
			\path (M2) edge[very thick,bend left=-10]  node [right=2pt] {$\sigma^{(2)}$} (M3);
			\path (M3) edge[very thick,bend right=-10]  node [below=9pt,right=-5.0pt] {$\sigma^{(3)}$} (M1);
			\path (M3) edge[very thick,bend left=-10]  node  [below=9pt,left=-5.0pt] {$\sigma^{(3)}$} (M2);
		\end{scope}
	\end{tikzpicture}
}
\caption{Communication for distributed synchronization. \alex{how should we graphically show this gets written to HDFS?}
\label{fig:distributeProjection}
}
\end{centering}
\end{figure}


\paragraph{Distributed Projections} % (fold)
\label{par:Distributed Projections}
In the case of a distributed projection, such as normalizing across all words
in the topic modeling example, we perform the projection between subepochs and
require a bit extra computation.  As before, each reducer waits until all of
the other reducers have processed their points from the current stratum to
begin the synchonization process.  If we are to normalize the factors in $\pi$,
we must calculate the sum $\sigma_k = \sum_{i} \pi_{i,k}$ for all $k$.
Therefore, for a reducer processing $\mathcal{S}^{(1)}_{b_i}$, we can calculate
$$\sigma_k^{(b_i)} = \sum_{i = b_i\lceil N/d\rceil}^{(b_i+1)\lceil N/d\rceil
-1} \pi_{i,k}.$$ Doing this for all $k$ gives each reducer a vector
$\sigma^{(b_i)}$.

Similar to the inter-reducer communication before, each reducer writes its
$\sigma^{(b_i)}$ to HDFS and reads $\sigma^{(b)}$ from HDFS for $b=1\ldots d$.
From this, each reducer can construct $\sigma = \sum_{b=1}^d \sigma^{(b)}$, and
then for each $\pi_{i,k}$ in its new block of parameters set $\pi_{i,k} =
\pi_{i,k} / \sigma_k$.  We will show below that this experimentally takes very
little time \alex{NEED EXPERIMENT OF TIME FOR EACH PART}.

\subsection{Threading Always-On SGD} % (fold)
\label{sub:Threading Always-On SGD}
After a reducer has processed all of its points for the first time, it needs to
both poll HDFS to see if the other reducers are ready to sync, and also
continue processing its data points with Always-On SGD.  Because polling can be
time consuming, we create a separate thread to do the polling while the main
thread continues to process the points.  We then only stop processing the
points when the polling thread finds that all reducers are complete.  This
allows us to spend as much computational time as possible on improving our
model and as little time as possible on overhead.


\section{Applications} % (fold)
\label{sec:applications}
We demonstrate that \method is a generic large scale machine learning system by
applying it over diverse set of real world problems that are non-trivial to
solve. We describe here a set of problems in machine learning from the sub-areas
of graphical models, natural language processing, computer vision and
computational social sciences. These applications are real world problems that
involve non-trivial complexity described in section~\ref{sec:complexQues}

\subsection{Latent dirichlet allocation (\lda)}
We described \lda in detail in section~\ref{sec:mdAbstract}. We further
elaborate on this model and its application here. The assumption that there
are a fixed set of topics and each document is composed of topics with certain
probabilistic weights is helpful in search engine queries and information
retrieval. The search engine can use topics as part of their indices to keep
similar documents together. This helps in retrieving faster and accurate results for
search queries~\cite{Wei:LDM}. A similar strtegy is used by libraries for
efficient storage of documents~\cite{Newman:ETM} in digital form. Besides prevalent in
text mining and natural language processing, it is one of the most common
building blocks of complex graphical models such as nested
chinese restaurant process~\cite{Blei:NCR} used in genetics for clustering
micro-array data~\cite{Qin:CMG}, hierarchical dirichlet process~\cite{hdp:2006} 
used for tracking trending topics~\cite{Gao:TCT}, among other things.

This model uses non-negativity, simplex as well as distributed simplex
constraints defined in section~\ref{par:Simplex Constraints}. We will see in
section~\ref{sec:eval} as to how these constraints affect run time
and convergence quality.

\subsection{Sparse Dictionary learning (\sdl)}
Dictionary Learning is a classical model in computer vision used for image
denoising, restoration~\cite{Mairal07sparserepresentation} and
classification~\cite{RamirezSS10}. Given a signal matrix $Y_{m,n}$ where each
column of $Y$ is an observation of a signal, with $n$ such observation. We
would like represent each observed signal (column of $Y$) $Y_j$ as weighted
combination of $K$ basis vectors $D_k$ (or columns). The basis vectors $D_k$s
form the columns of the matrix $D$ called dictionary. The underlying assumption
is that there is an inherent set of basis vectors that is the building block
of any signal observed. This helps in image classification as the incoming
unlabeled image query can be matched with labled image's weights of basis vector
to predict a label. Mathematically we are solving the following query:

% {\small
\begin{align}
&\argmin_{\alpha,D} L(Y,\alpha,D) =\frac{1}{2}||Y-D\alpha||_2^2 + \lambda||\alpha||_1
\label{eqn:dictL}\\
&\text{s.t.} \; \forall j, D_j^TD_j \leq 1 \nonumber
\end{align}
% }
$\alpha_j$, the $j$th column of $\alpha$, is the weight coefficient for basis
vectors in signal $j$. The $\lambda||\alpha||_1$ in equation~\ref{eqn:dictL} as
discussed in section~\ref{par:Sparse Models}, helps in removing the noise from the
signal~\cite{Mairal07sparserepresentation}.
 
\subsection{Mixed membership stochastic block models (\mmsb)}
\mmsb models or multi-role models are very useful in social sciences..

\section{Experimental Setup} % (fold)
\label{sec:experiments}
To compare our \method to \psgd (data partition), \dsgd (sync barrier) and
\graphlab over speed of convergence and convergence quality we run
them over a collection of large real world dataset. To further demonstrate the
scalability of the approach we replicate and stack up real dataset to
artificially create datasets of terabytes scale. 

\subsection{Dataset}
We use four public datasets, two for \lda, \nytimes and
\pubmed~\footnote{\url{http://archive.ics.uci.edu/ml/datasets/Bag+of+Words}},
and one each for \dl and \mmsb,
\imagenet\footnote{\url{http://www.image-net.org/challenges/LSVRC/2010/download-public}}
and \twitter\footnote{\url{http://konect.uni-koblenz.de/networks/twitter}} respectively.
\paragraph{\nytimes} It is a collection of 300,000 Ny Times news articles that
contain 102,660 distinct words and 100,000,000 tokens (word occurrences) in the.
\paragraph{\pubmed} This set contains 8,200,000 PubMed abstracts, that have in
total 730,000,000 word occurrences and 141,043 unique words.
\paragraph{\imagenet} This dataset was originally used for large scale visual
recognition challenge in 2010~\cite{imagenet_cvpr09}. The set contains 1,261,406
images each with 1,000 features and has in total 389,080,708 non-zero pixels.
\paragraph{\twitter} This is a follower network from twitter that stores
directed edges from followers to followee~\cite{konect:twitter1}. It consists of
1,468,365,182 edges distributed among 41,652,230 vertices (users). 
\paragraph{\scaleblenytimes(\snytimes{N})} We replicate the documents in
\nytimes to create a \lda datasets that are in scales of hundreds of gigabytes. For example
\snytimes{4} is a datset that has each news article in \nytimes dataset
replicated 4 times. We create \snytimes{4}, \snytimes{16}, \snytimes{64} and
\snytimes{256} that have 1,200,000, 4,800,000, 19,200,000 and 76,800,000
documents with data sizes 6.08, 25.12, 103.4, and 421.42 gbs respectively.
Table~\ref{tab:dataset} shows the concise statistics of the dataset used in
all the experiments.

% \begin{table}
% \centering
% \begin{tabular}{c|c|c|c|} %\hline
% Dataset  & Dimensions & Non-zeros & Size \\ \hline
% \nytimes  & 300,000$\times$102,660 & 100,000,000 &  1.49 Gbs \\ \hline
% \pubmed & 8,200,000$\times$141,043 &  730,000,000 & 11.19 Gbs \\ \hline
% \imagenet & 1,261,406$\times$1,000 & 389,080,708 & 5.06 Gbs \\ \hline
% \twitter & 41,652,230$\times$41,652,230 & 1,468,365,182 & 23.99 Gbs \\ \hline
% \snytimes{4} & 1,200,000$\times$102,660 & 400,000,000 &  6.08 Gbs \\ \hline
% \snytimes{4} & 4,800,000$\times$102,660 & 1,600,000,000 &  25.12 Gbs \\ \hline
% \snytimes{4} & 19,200,000$\times$102,660 & 6,400,000,000 &  103.4 Gbs \\ \hline
% \snytimes{4} & 76.800,000$\times$102,660 & 25,600,000,000 &  421.42 Gbs \\
% \hline
% \end{tabular}
% \end{table}


\begin{table}
\centering
\scalebox{0.95}{
\begin{tabular}{c|c|c|c|} %\hline
Dataset  & Dimensions & Nonzeros & Size(GB) \\ \hline
\nytimes  & $0.3*10^6\times$102,660 & $0.1*10^9$ &  1.49  \\ \hline
\pubmed & $8.2*10^6\times$141,043 &  $0.73*10^9$ & 11.19  \\ \hline
\imagenet & $1.26*10^6\times$1,000 & $0.39*10^9$ & 5.06  \\ \hline
\twitter & $41.6*10^6\times 41.6*10^6$ & $1.5*10^9$ & 23.99 \\ \hline
\snytimes{4} & $1,2*10^6\times$102,660 & $0.4*10^9$ &  6.08  \\ \hline
\snytimes{16} & $4.8*10^6\times$102,660 & $1.6*10^9$ &  25.12  \\ \hline
\snytimes{64} & $19.2*10^6\times$102,660 & $6.4*10^9$ &  103.4  \\ \hline
\snytimes{256} & $76.8*10^6\times$102,660 & $25.6*10^9$ &  421.42  \\
\hline
\end{tabular}
}
\caption{Dimension, size and nonzero statistics for different datasets. The
exact figures are rounded off for simplicity. Size is the file size in
gigabytes. The biggest dataset (\snytimes{256}) is of size approximately 0.5
terabytes.}
\label{tab:dataset}
\end{table}


\subsection{\graphlab based solver}
We modify \graphlab's collaborative filtering toolkit to add the constraints
defined in equation~\ref{eqn:constraints} \abhi{put in the constraints
equation}, section~\ref{sec:applications}. We modify \sgd based learner of the toolkit as
it is eaisly pralleizable in the data space\abhi{Do we need to justify why we
use SGD based solver for graphlab?}.
We use its public APIs ( \textit{transform\_vertices(), periodic aggregator} and
\\\textit{map\_reduce\_vertices()}) to put normalization constraints. We take
the simplest and most efficient way of normalizing accross vertices. A periodic
aggregator is called after every fixed interval to compute the normalization
factor using \textit{map\_reduce\_vertices()} after which
we apply the computed factor to each vertex using \textit{transform\_vertices()}.


\begin{figure}
\centering
\includegraphics[width=0.46\textwidth]{results/tm_cvg.pdf} 
\label{fig:convergenceNytimes4}
\end{figure}



\begin{figure*}[t]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\bf Topic Modeling} \\
\hline
Convergence Plots & \# of Topics & \# of Processors & \# of Docs \\
\hline
\includegraphics[width=0.23\textwidth]{results/tm_cvg.pdf} &
\includegraphics[width=0.23\textwidth]{results/tm_rank.pdf} &
\includegraphics[width=0.23\textwidth]{results/tm_cores.pdf} &
\includegraphics[width=0.23\textwidth]{results/tm_data.pdf} \\
\hline
\multicolumn{4}{|c|}{\bf Dictionary Learning} \\
\hline
Convergence Plots & \# of Dictionary Bases & \# of Processors & \# of Images \\
\hline
TODO&&&\\
\hline
\multicolumn{4}{|c|}{\bf Mixed Membership Network Decomposition} \\
\hline
Convergence Plots & \# of Network Roles & \# of Processors & \# of Network Nodes \\
\hline
TODO&&&\\
\hline
\end{tabular}
\caption{\small Convergence (Left) and scalability (in rank, processor cores and data size)
of all methods, on topic modeling, dictionary learning and mixed-membership network decomposition.
The convergence plot reveals the solution trajectory of each method, revealing pathological behavior such as oscillation.
The scalability plots show how each method fares as the problem rank, number of processor cores, and data
size is increased.}
\label{fig:results}
\end{figure*}

\section{Evaluation} % (fold)
\label{sec:eval}
\subsection{Scalability}
\vspace{-0.4cm}
\section{Experiments}
\vspace{-0.3cm}

\begin{figure*}[t]
\vspace{-0.4cm}
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{\bf Topic Modeling} & {\bf Dictionary Learning} & {\bf MMND} \\
\hline
Convergence Plots & Scaling in \# Cores & Convergence Plots &  Convergence Plots \\
%\multicolumn{4}{|c|}{\bf Topic Modeling} \\
%\hline
%Convergence Plots & \# of Topics & \# of Processors & \# of Docs \\
\hline
\includegraphics[width=0.23\textwidth]{fig2/lda_convergence.eps} 
& \includegraphics[width=0.23\textwidth]{fig2/lda_machines.eps} 
& \includegraphics[width=0.23\textwidth]{fig2/dict_convergence.eps} 
& \includegraphics[width=0.23\textwidth]{fig2/mmsb_convergence.eps}  
\\
\hline
%{\bf Topic Modeling} & {\bf Dictionary Learning} & \multicolumn{2}{|c|}{\bf Mixed Membership Network Decomposition} \\
\multicolumn{4}{|c|}{\bf Topic Modeling} \\
\hline
Machines Needed & Scaling in \# Docs & \multicolumn{2}{|c|}{Scaling in \# Topics} \\
\hline
\includegraphics[width=0.23\textwidth]{fig2/lda_machines_failing.eps}
& \includegraphics[width=0.23\textwidth]{fig2/lda_datasize.eps}
& \multicolumn{2}{|c|}{\includegraphics[width=0.46\textwidth]{fig2/lda_rankv2.eps}} \\
\hline
\end{tabular}
\vspace{-0.3cm}
\caption{\small Convergence and scalability plots for the three models (\lda, \dl, \mmsb),
under our \ourmethod{} and baselines (\dsgd, \psgd, \graphlab). Unless otherwise stated in the plot, all methods were run with 16 cores
and rank $K=25$. For all topic modeling plots except ``\# of Docs" and ``Machines Required", we used the NyTimes4 dataset (Table \ref{tab:dataset}).
The convergence plots reveal the objective trajectory and final value of each method,
while the scalability plots show how each method fares (on topic modeling) as we increase the problem rank, number of processor cores, and data size.
In the bottom left, we also show the minimum number of machines required for a given topic modeling dataset size, for
\ourmethod{} and \graphlab.}
\vspace{-0.5cm}
\label{fig:results}
\end{figure*}
\paragraph{Dataset size}
\begin{table}
\vspace{-0.2cm}
\centering
\scriptsize
\begin{tabular}{c|c|c|c|} %\hline
{\bf Dataset} & {\bf Dimensions} & {\bf Nonzeros} & {\bf Size (GB)} \\ \hline\hline
\nytimes  & $0.3*10^6\times$102,660 & $0.1*10^9$ &  1.49  \\ \hline
%\pubmed & $8.2*10^6\times$141,043 &  $0.73*10^9$ & 11.19  \\ \hline
\imagenet & $0.63*10^6\times$1,000 & $0.63*10^9$ & 7.99  \\ \hline
%\twitter & $41.6*10^6\times 41.6*10^6$ & $1.5*10^9$ & 23.99 \\ \hline
\webgraph & $0.28*10^6\times 0.28*10^6$ & $0.31*10^9$ & 4.46 \\ \hline
\snytimes{4} & $1.2*10^6\times$102,660 & $0.4*10^9$ &  6.08  \\ \hline
\snytimes{16} & $4.8*10^6\times$102,660 & $1.6*10^9$ &  25.12  \\ \hline
\snytimes{64} & $19.2*10^6\times$102,660 & $6.4*10^9$ &  103.4  \\ \hline
\snytimes{256} & $76.8*10^6\times$102,660 & $25.6*10^9$ &  421.42  \\
\hline
\end{tabular}
\vspace{-0.2cm}
\caption{\small Dimension, filesize and nonzero statistics for our datasets.
The biggest dataset (\snytimes{256}) is approximately 0.5
terabytes. Note that the \imagenet dataset is 100\% dense. }
\label{tab:dataset}
\vspace{-0.5cm}
\end{table}
data dimension
\paragraph{Model parameters}
rank
\paragraph{Processors}
machines, \method plateaus
\subsection{Convergence speed}
Figure~\ref{fig:speed} shows convergence times 
for \ourmethod{}, \dsgd, \psgd, and \graphlab over the three models: \lda, \mmsb and \dl. \ourmethod{} is faster
by anywhere between $2.6\times$ (vs \graphlab on \lda) to $26.2\times$ (vs \graphlab on \sdl).
\begin{figure}[t]
\vspace{-0.4cm}
\centering
\begin{tabular}{|c|c|}
\hline
 \multicolumn{2}{|c|} {\bf Time taken to converge} \\
\hline
\includegraphics[width=0.46\columnwidth]{fig2/speedup.eps} &
\includegraphics[width=0.46\columnwidth]{fig2/speedup2.eps} \\\hline
\end{tabular}
\vspace{-0.3cm}
\caption{\small Time taken by all methods to converge on the
three ML models, on an absolute scale (left) as well as a relative scale (right).
The methods plateau at these values in the respective plots shown
in figure~\ref{fig:results}. The bar for \psgd is absent in the figure as it never reaches $0.059$ and stops around 
objective value $0.092$.  }
\label{fig:speed}
\end{figure}

\subsection{Convergence quality}
\graphlab oscillation reasons and patterns, \dsgd and \psgd converges to a poor
quality,
\subsection{Why \method wins }
Put a plot of waiting times in different concstraints case and argue why it
helps here.

\section{Related Work} % (fold)
\label{sec:related}

\section{Conclusion} % (fold)
\label{sec:conclusion}

%{
%\small
\bibliographystyle{abbrv}
\bibliography{biblio}
%}


\end{document}
