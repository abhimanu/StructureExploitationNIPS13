%\alex{\textbf{NOTE: The paper outline is still {\it very} rough, and I expect to re-order
%many sections, largely in attempt to keep the focus on the system.}}

In recent years, the amount of data being produced and stored throughout the
world has exploded.  From science to e-commerce to news to social networking,
we, as a culture, are spending more time and money producing uniquely digital
content and recording elements of our physical world online for posterity and
analysis.  With this big data boom, the focus is often on the size of the data
- ``LHC produced 13 petabytes \ldots of data in
2010\footnote{\url{http://www.nature.com/news/2011/110119/full/469282a.html}}''
and ``500+terabytes of new data ingested into [Facebook] databases every
day\footnote{\url{http://gigaom.com/2012/08/22/facebook-is-collecting-your-data-500-terabytes-a-day/}}.''
While handling all of this data is an important challenge and lots of work has
focused on scaling to large datasets, this misses an important part of the
picture - we are also recording more data about {\em more things}: ``300
million photos uploaded per day [to Facebook]'' and there are more than 400
million tweets sent per
day\footnote{\url{http://articles.washingtonpost.com/2013-03-21/business/37889387_1_tweets-jack-dorsey-twitter}}.
The challenge now becomes not just using all of the data, but also
understanding all of these items.

Machine learning has focused on understanding large datasets for many years
now, but the research often ignore the challenge of scaling to models over many
millions or even billions of items, e.g. finding the topics of millions of news
articles, removing noise from the many images upoaded online, or clusters users
in a billion node social network.  In each of these cases, the issue is not
just the size of the data but also the number of items we need to model and
understand.  Even if we distribute our dataset across many machines, the model
of topics for 100 million documents (news articles or webpages) across even 100
topics would take over 35 gigabytes just to store in memory.  

In this paper we describe a new system for answering such complex questions
about modern massive datasets.



