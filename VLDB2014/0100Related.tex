We started with the premise of solving big models over big data, 
but in the current state of the art there are very few works 
that handle both. The big data focus is mainly on i.i.d (identical
and independently distributed) 
nature of the datasets. This informally means that there is no probabilistic 
difference between any two randomly picked samples of the data.
\psgd ~\cite{zinkevich2010parallelized} exploits this property by 
partitioning the data randomly, computing variables on each partition 
independently, and finally averaging them in the end. 
This has an advantage that there is no communication 
needed between the machines until the final phase when they all have converged 
and are ready to synchronize.
But this does not work well when the problem size
is bigger (e.g. when we increase the number of topics 
in \lda). The other class of algorithms in this category are 
fixed-delay algorithms~\cite{agarwal2012distributed,langford2009slow}. 
Here, there is a central server that facilitates communication between machines 
in a fixed ordering. But this suffers with the slow worker bottleneck problem
where slower machines or failed machines dictate the system speed.

In most ML problems each variable is dependent on 
a small set of other variables. When faced with a big model 
the general approach in the current literature is to exploit 
this proeprty. This is the philosophy behind \graphlab
~\cite{low2012distributed} as well as Google Brain project
~\cite{dean2012large}. \graphlab defines every variable as a vertex 
in a graph and partitions them into sets for distributed processing.
But as we have shown, the speed up gained in this case is sub-optimal 
since these partitions may not correspond to an efficient partition
along data and model dimensions. In case of Google Brain project 
the corresponding paper does not detail any generic partitioning 
strategy. Moreover none of the above are
theoretically well grounded. Though Hogwild paper~\cite{niu2011hogwild} provides
theoretical analysis of parallel updates of the same variable in two different 
processors, it does not provide a generic partitioning strategy or any analysis 
in that direction. Besides their experiments only cover single machine
with multiple cores. 


In the present ML systems literature there are no existing works that achieve 
scaling in data as well as model for general ML probelms.
Even for specific problems there are very few cases such as distributed matrix factroization 
by Gemulla et al.~\cite{Gemulla:2011:LMF:2020408.2020426}. 
This is the work that is closest to \ourmethod, but they are restricted to
matrix factorization.
We also differ from this work in providing a 
a generic partitioning strategy for arbitrary machine learning problems and 
allowing workers to run over barriers to avoid the negative impact of stragglers. 

Another useful class of ML systems are parameter servers that are based on distributed
shared-memory interface and is helpful for large number of model parameters
~\cite{ahmed2012scalable,power2010piccolo}. However they lack a generic variable scheduling 
strategy. Recent works on parameter servers have come up with stale synchronous parallel (SSP) 
schemes~\cite{ho2013more,cipar2013solving} that reduce inter-machine communication
mitigating the effects of slow workers. The difference between SSP and \ourmethod is that 
SSP reduces communication regardless of the scheduling strategy, where as \ourmethod intelligently 
partitions and schedules variable updates to mitigate slow workers. 
\ourmethod can be used in conjuntion with SSP to provide further improved answers to 
bigger data and model questions. 

%\ourmethod does not scale well in number of machines if the data 
%size is small. In such a scenario the synchonization time dominates every 
%thing else (Figure~\ref{fig:piechart}). This stems from the 
%limitation of using Hadoop. A system that can ensure few inter-machine 
%communications (e.g. SSP) can help \ourmethod overcome this. We plan to 
%explore this as future work.  
%Another problem is: in case of pathalogically skewed load distribution 
%between the machines or in case a mahine dies altogether, the guarantess 
%of \ourmethod may not hold. These can be solved by sorting the data for 
%fair load scheduling prior to 
%running \ourmethod. 

\ourmethod provides a generic partitioning and schedulng strategy
that is distributed on data as well as model size. Its firm theoretical
standing ensures better annswers whereas its ability to distribute both the
data and model guarantees faster answers. These attributes have been justified
We have demonstrated empirically that \ourmethod successfully converges faster
and to better results than state of the art competitors on multiple complex
machine learning applications.
%emprically (Section ~\ref{sec:eval}).
%But there are certain areas that need further improvement such as machine 
%scalability over smaller datasets and load balancing. 
