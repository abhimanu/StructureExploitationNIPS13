We started with the premise of solving big models over big data, 
but in the current state of the art there are very few works 
that handle both. The big data focus is mainly on i.i.d (identical
and independently distributed) 
nature of the datasets. This informally means that there is no probabilistic 
difference between any two randomly picked samples of the dataset.
\psgd ~\cite{zinkevich2010parallelized} exploits this property by 
partitioning the data randomly, computing variables on each partition 
independently, and finally averaging them in the end. 
This has an advantage that there is no communication 
needed between the machines until the final phase when they all have converged 
and are ready to synchronize.
But this does not work well when the problem size
is bigger (e.g. when we increase the number of topics 
in \lda). The other class of algorithms in this category are 
fixed-delay algorithms~\cite{agarwal2012distributed,langford2009slow}. 
Here, there is a central server that facilitate communication between machines 
in a fixed ordering. But this suffers with the bottleneck problem
where slower machines or failed machines dictate the system.
Compared to this \ourmethod is less prone to slower machine problem.
Besides unlike these it distributes over data as well as model. 

In most machine learning probelms each variable is dependent on 
a small set of other variables. When faced with big model 
the general approach in the current literature is to exploit 
this proeprty. This is the philosophy behind \graphlab
~\cite{low2012distributed} as well as Google Brain project
~\cite{dean2012large}. \graphlab defines every variable as a vertex 
in a graph and partitions them into sets for distributed processing.
But as we have shown, the speed up gained in this case is sub-optimal 
since these partitions may not corresspond to an efficient partition
along data and model dimensions. In case of Google Brain project 
the corrssponding paper does not detail any generic partitioning 
strategy. Moreover none of these works are
theoretically well grounded. Though Hogwild paper~\cite{niu2011hogwild} provides
theoretical analysis of parallel updates of the same variable at two different 
processors, it does not provide a generic partitioning strategy or any analysis 
in that direction. Besides the experiments in the paper only cover single machines
with multiple cores. 


The third type of work is one that covers both big data and big models.
In ther present ML systems literature there are no works that achieve this 
for general machine learning probelms.
Even for specific problems there are very few such as distributed matrix factroization 
by Gemulla et al.~\cite{Gemulla:2011:LMF:2020408.2020426}. 
This is the work that is closest to 
\ourmethod, but they are restricted to matrix factorization.
Besides they lack a generic partitioning strategy for arbitrary machine learning problems. 
They do not allow workers to run over barriers and suffer from slow worker problem 
compared to \ourmethod.    

A useful class of ML systems are parameter server that are based off distributed
shared-memory interface which is helpful for large number of model parameters
~\cite{ahmed2012scalable,power2010piccolo}. But they lack a generic variable scheduling 
strategy. Recent works on parameters have come up with stale synchronous parallel (SSP) 
schemes~\cite{ho2013more,cipar2013solving}. This scheme reduces inter-machine communication
mitigating the effects of slow workers. The difference between SSP and \ourmethod is that 
SSP reduces communication regardless of scheduling strategy, where as \ourmethod intelligently 
partitions and schedules variable updates to mitigate slow workers. \ourmethod can be used to 
provide scheduling strategy over SSP. This can further help to solve bigger data and model 
problems. 

