We started with the premise of solving big models over big data, 
but in the current state of the art there are very few works 
that handle both. The big data focus is mainly on i.i.d (identical
and independently distributed) 
nature of the datasets. This informally means that there is no probabilistic 
difference between any two randomly picked samples of the data.
\psgd ~\cite{zinkevich2010parallelized} exploits this property by 
partitioning the data randomly, computing variables on each partition 
independently, and finally averaging them in the end. 
This has an advantage that there is no communication 
needed between the machines until the final phase when they all have converged 
and are ready to synchronize.
But this does not work well when the problem size
is bigger (e.g. when we increase the number of topics 
in \lda). The other class of algorithms in this category are 
fixed-delay algorithms~\cite{agarwal2012distributed,langford2009slow}. 
Here, there is a central server that facilitate communication between machines 
in a fixed ordering. But this suffers with the slow worker bottleneck problem
where slower machines or failed machines dictate the system speed.

In most ML problems each variable is dependent on 
a small set of other variables. When faced with big model 
the general approach in the current literature is to exploit 
this proeprty. This is the philosophy behind \graphlab
~\cite{low2012distributed} as well as Google Brain project
~\cite{dean2012large}. \graphlab defines every variable as a vertex 
in a graph and partitions them into sets for distributed processing.
But as we have shown, the speed up gained in this case is sub-optimal 
since these partitions may not corresspond to an efficient partition
along data and model dimensions. In case of Google Brain project 
the corrssponding paper does not detail any generic partitioning 
strategy. Moreover none of the above are
theoretically well grounded. Though Hogwild paper~\cite{niu2011hogwild} provides
theoretical analysis of parallel updates of the same variable in two different 
processors, it does not provide a generic partitioning strategy or any analysis 
in that direction. Besides their experiments only cover single machine
with multiple cores. 


In the present ML systems literature there are no existing works that achieve 
scaling in data as well as model for general ML probelms.
Even for specific cases there are very few such as distributed matrix factroization 
by Gemulla et al.~\cite{Gemulla:2011:LMF:2020408.2020426}. 
This is the work that is closest to 
\ourmethod, but they are restricted to matrix factorization.
Besides they lack a generic partitioning strategy for arbitrary machine learning problems. 
They do not allow workers to run over barriers and suffer from slow worker problem 
compared to \ourmethod.    

Another useful class of ML systems are parameter servers that are based on distributed
shared-memory interface that is helpful for large number of model parameters
~\cite{ahmed2012scalable,power2010piccolo}. But they lack a generic variable scheduling 
strategy. Recent works on parameters have come up with stale synchronous parallel (SSP) 
schemes~\cite{ho2013more,cipar2013solving} that reduce inter-machine communication
mitigating the effects of slow workers. The difference between SSP and \ourmethod is that 
SSP reduces communication regardless of the scheduling strategy, where as \ourmethod intelligently 
partitions and schedules variable updates to mitigate slow workers. 
\ourmethod can be used in conjuntion with SSP to provide further improved answers to 
bigger data and model questions. 
\ourmethod does not scale well in number of machines if the data 
size is small. In such a scenario the synchonization time dominates every 
thing else (figure~\ref{fig:piechart}). This stems from the 
limitation of using hadoop . A system that can ensure few inter-machine 
communications (e.g. SSP) can help \ourmethod overcome this. We plan to 
explore this as future work.  Another problem is: in case of pathalogically skewed load distribution 
between the machines or in case a mahine dies altogether, the guarantess 
of \ourmethod may not hold. These can be solved by sorting the data for 
fair load scheduling prior to 
running \ourmethod. 

\ourmethod provides a generic partitioning and schedulng strategy
that is distributed on data as well as model size. Its firm theoretical
standing ensures better annswers whereas its distributivity over data and model guarantees
faster answers. These attributes have been justified emprically ( 
section ~\ref{sec:eval}) as well as theoretically.
But there are certain areas that need further improvement such as machine 
scalability over smaller datasets and load balancing. 
