So far we have primarily discussed our sytem in terms of the high level design.
The design choices made so far are largely indepdent of the impelmentation
decisions and should make for an efficient, scalable system, regardless of
whether it is implemented with MPI or Hadoop.  In building this system, we
focus on ease of use and portability.

For portability reasons, we build our framework on top of Apache's Hadoop and
Hadoop File System (HDFS).  Although this platform is typically not geared
towards iteratitve distributed machine learning algorithms, we demonstrate that
the tools in the system offer many advantages such as ease to set up,
robustness, and ubiquity in industry, while still enabling our framework to
work accurately and efficiently.  Our implementation uses \textbf{stock
Hadoop}, enabling it to be run on any standard Hadoop cluster without requiring
additional software to be installed on the cluster.

At a high level, each epoch is one MapReduce job.  We use Hadoop's mappers to
partition our data and the reducers to perform the SGD computation.
Additionally, we use direct access to HDFS to communicate between different
reducers.

\subsection{System interface} % (fold)
\label{sub:System interface}
Our system works by assuming objects between which we have relations are
indexed from 1 to $T_i$.  (In our implementation we can handle up to 3-way
relations and up to paired objective functions.)
For ease of use, our system takes as input plain text, tab delimited data.
Because our input data is sparse is the number of relations relative to the
number of possible relations, each line contains the indices of the objects and
the numeric value of the relationship.  While we make no assumption about the
ordering of the input data, we typically make sure to randomly index our
relational indices.  If this is not done some relational blocks will have far
more data points than others and this results in extremely uneven computation.
We consider this a pre-processing step and do not consider it in our future
experiments.

% subsection System interface (end)

\subsection{Distributing the data} % (fold)
\label{sub:Ordering the data}
As was described in Section \ref{sec:mdAbstract}, our system works by dividing
the data into blocks, grouping the blocks into strata, distributing the strata
across the cluster of machines, and processing the strata in order.  To do this
in Hadoop, we use the mappers to (1) read in our data, which is unorganized,
(2) determine the block that the data points fall in, (3) send the blocks to the
appropriate reducer, and (4) order the blocks in each reducer correctly.

In order to set up the system appropriately
We assume we have a data matrix $P$ of size $N \times M$ and we are setting
Hadoop to have $d$ reducers, such that our computation will run over $d$
machines.
Blocks are of size $\lceil N/d \rceil \times \lceil M/d \rceil$.
Blocks with index $b_i$ cover $i\in[b_i\lceil N/d \rceil\ldots (b_i+1),\, N/d
\rceil)$ and blocks with index $b_j$ cover $j \in [b_j\lceil M/d \rceil,\,
(b_j+1)\lceil M/d \rceil)$.
The data comes in as a triplet $\langle i,j,P_{i,j}\rangle$.  We calculate
which block $\mathcal{B}_{b_i,b_j}$ the point falls into by setting
\begin{align}
	b_i = \left\lfloor \frac{i}{\lceil N/d \rceil} \right\rfloor
	\;\;\;\;\;\;
	\mbox{and}
	\;\;\;\;\;\;
	b_j = \left\lfloor \frac{j}{\lceil M/d \rceil} \right\rfloor.
\end{align}


We design our strata by beginning with the blocks along the diagonal and then
rotating the diagonal around the matrix, as was seen in Figure
\ref{fig:partition}.  This maintains the property that all blocks in each stratum
are independent and the order is also easy to calculate.  Additionally, we 
then assign one row of blocks to each reducer.  Doing this is both simple and
means that we do not need to ever transfer the factors for the left side of the
matrix (e.g. the document model when doing topic modeling) between reducers.

Within the row of blocks to be received by each reducer, we order the
blocks by finding the stratum number they correspond to.
The stratum order $s$ can be calculated as:
\begin{align}
	s= (b_j - b_i + d) \bmod{d}.
\end{align}
%\alex{I am thinking we should move much of the above description to section 2.
%it will make the problem much more concrete and we can even show it visually.}
\alex{include detailed description of what do we set as the key and value?}

Based on this desired processing the data, we would like each reducer to
receive the data for one row of blocks, and for the data going to that reducer
to be ordered by the stratum number $s$.  Hadoop makes this quite easy.  To
send the data to the correct reducer, we modify Hadoop's default partitioner to
use the value of $b_i$ as the partition number.  To order the points by $s$ we
modify the KeyComparator and GroupComparator.  As such, each reducer only
receives points for the same $b_i$ and the blocks are ordered by the stratum
number.
%\alex{check this}

\subsection{Main Computation} % (fold)
\label{sub:Computation}

Within each reducer, the computation is relatively straightforward.  As we see
in Algorithm \ref{algo:reducer}, we process the data points in a streaming
fashion from using the reducer's iterator.  In most cases, processing a point
consists of performing the SGD update on the relevant portions of our model and
then performing any local projections that go with the update.  

When using our Always-On SGD, we also keep a linked list of the points in the
current block.  Once we have gone through all of the points in the block, we
shuffle the linked list and again process all of the points from the list.  We
do this repeatedly until all reducers have processed the points in their
current block at least once.

%\subsection{Model Synchronization} % (fold)
%\label{sub:Synchronization}

\paragraph*{Parameter Communication} % (fold)
\label{par:Model Movement}
Besides processing the points, the reducers are also responsible for passing
the necessary parameters among the reducers between subepochs.  
To do this requires two steps.  First, each reducer must be able to check when
the other reducers have finished processing their points at least once.  To
accomplish this, when reducer $r$ has
processed all of the points in its current stratum $s$ at least once, it
touches on HDFS a unique file named by $\langle r,s \rangle$.   After this, the
reducer polls HDFS every 3 seconds to check if there is a file for each reducer
for the current stratum $s$.  If so, then the subepoch is over and the reducers
can sync their parameters.

To sync their parameters, each reducer writes its parameters from
$\mathcal{S}^{(2)}$ to HDFS in a unique file (again named by the reducer $r$
and the stratum $s$).  Each file will contain a portion of the matrix
$\mathcal{S}^{(2)}$, so when serialized appropriately, each file is fairly
small (\alex{5kb?}).  After its data has been written, the reducer polls HDFS
waiting for the block of $\mathcal{S}^{(2)}$ it needs for stratum $s+1$.  Once
the file appears, the reducer can read it into memory and begin with the next
subepoch.  This passing of parameters can be seen in Figure
\ref{fig:data-movement}.


\paragraph*{Distributed Projections} % (fold)
\label{par:Distributed Projections}
In the case of a distributed projection, such as normalizing across all words
in the topic modeling example, we perform the projection between subepochs and
require a bit extra computation.  As explained in Section
\ref{sub:Distributed_Normalization}, each reducer $b_i$ calculates a vector
$\mathbf{\sigma}^{(b_i)}$.
%As before, each reducer waits until all of
%the other reducers have processed their points from the current stratum to
%begin the synchonization process.  If we are to normalize the factors in $\pi$,
%we must calculate the sum $\sigma_k = \sum_{i} \pi_{i,k}$ for all $k$.
%Therefore, for a reducer processing $\mathcal{S}^{(1)}_{b_i}$, we can calculate
%$$\sigma_k^{(b_i)} = \sum_{i = b_i\lceil N/d\rceil}^{(b_i+1)\lceil N/d\rceil
%-1} \pi_{i,k}.$$ Doing this for all $k$ gives each reducer a vector
%$\sigma^{(b_i)}$.
Similar to the inter-reducer communication before, each reducer writes its
$\sigma^{(b_i)}$ to HDFS and reads $\sigma^{(b)}$ from HDFS for $b=1\ldots d$.
From this, each reducer can construct $\sigma = \sum_{b=1}^d \sigma^{(b)}$, and
then for each $\beta_{j,k}$ in its new block of parameters set $\beta_{j,k} =
\beta_{j,k} / \sigma_k$.  We will show below that this experimentally takes very
little time. 
\alex{NEED EXPERIMENT OF TIME FOR EACH PART}
%\alex{clean this up as it has been mostly moved earlier}

\subsection{Threading Always-On SGD} % (fold)
\label{sub:Threading Always-On SGD}
After a reducer has processed all of its points for the first time, it needs to
both poll HDFS to see if the other reducers are ready to sync, and also
continue processing its data points with Always-On SGD.  Because polling can be
time consuming, we create a separate thread to do the polling while the main
thread continues to process the points.  We then only stop processing the
points when the polling thread finds that all reducers are complete.  This
allows us to spend as much computational time as possible on improving our
model and as little time as possible on overhead.

% subsection Threading Always-On SGD (end)
