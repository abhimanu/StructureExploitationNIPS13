So far we have primarily discussed our sytem in terms of the high level design.
The design choices made so far are largely indepdent of the impelmentation
decisions and should make for an efficient, scalable system, regardless of
whether it is implemented with MPI or Hadoop.

For portability reasons, we build our framework on top of Apache's Hadoop and
Hadoop File System (HDFS).  Although this platform is typically not geared
towards iteratitve distributed machine learning algorithms, we demonstrate that
the tools in the system offer many advantages such as ease to set up,
robustness, and ubiquity in industry, while still enabling our framework to
work accurately and efficiently.  Our implementation uses \textit{stock
Hadoop}, enabling it to be run on any standard Hadoop cluster without requiring
additional software to be installed on the cluster.

At a high level, each epoch is one MapReduce job.  We use Hadoop's mappers to
partition our data and the reducers to perform most of the SGD computation.
Additionally, we use direct access to HDFS to communicate between different
reducers.

\subsection{Distributing the data} % (fold)
\label{sub:Ordering the data}
As was described in Section \ref{sec:mdAbstract}, our system works by dividing
the data into blocks, grouping the blocks into strata, distributing the strata
across the cluster of machines, and processing the strata in order.  To do this
in Hadoop, we use the mappers to (1) read in our data, which is unorganized,
(2) determine the block that the data points fall in, (3) send the blocks to the
appropriate reducer, and (4) order the blocks in each reducer correctly.

In order to set up the system appropriately
We assume we have a data matrix $P$ of size $N \times M$ and we are setting
Hadoop to have $d$ reducers, such that our computation will run over $d$
machines.
Blocks are of size $\lceil N/d \rceil \times \lceil M/d \rceil$.
Blocks with index $b_i$ cover $i\in[b_i\lceil N/d \rceil\ldots (b_i+1),\, N/d
\rceil)$ and blocks with index $b_j$ cover $j \in [b_j\lceil M/d \rceil,\,
(b_j+1)\lceil M/d \rceil)$.
The data comes in as a triplet $\langle i,j,P_{i,j}\rangle$.  We can calculate
which block $\mathcal{B}_{b_i,b_j}$ the point falls into by setting
\begin{align}
	b_i = \left\lfloor \frac{i}{\lceil N/d \rceil} \right\rfloor
	\;\;\;\;\;\;
	\mbox{and}
	\;\;\;\;\;\;
	b_j = \left\lfloor \frac{j}{\lceil M/d \rceil} \right\rfloor.
\end{align}




We design our strata by beginning with the blocks along the diagonal and then
rotating the diagonal around the matrix, as can be seen in Figure
\ref{fig:blocks}.  This maintains the property that all blocks in each stratum
are independent and the order is also easy to calculate.  Additionally, we can
then assign one row of blocks to each reducer.  Doing this is both simple and
means that we do not need to ever transfer the factors for the left side of the
matrix (e.g. the document model when doing topic modeling) between reducers.

Within the row of blocks to be received by each reducer, we can order the
blocks by finding the stratum number they correspond to.
The stratum order $s$ can be calculated as:
\begin{align}
	s= (b_j - b_i + d) \bmod{d}.
\end{align}
\alex{I am thinking we should move much of the above description to section 2.
it will make the problem much more concrete and we can even show it visually.}

\alex{include detailed description of what do we set as the key and value?}

Based on this desired processing the data, we would like each reducer to
receive the data for one row of blocks, and for the data going to that reducer
to be ordered by the stratum number $s$.  Hadoop makes this quite easy.  To
send the data to the correct reducer, we modify Hadoop's default partitioner to
use the value of $b_i$ as the partition number.  To order the points by $s$ we
can then modify the KeyComparator.  As such, each reducer only receives points
for the same $b_i$ and the blocks are ordered by the stratum number.
\alex{check this}

\subsection{Main Computation} % (fold)
\label{sub:Computation}

Within each reducer, the computation is relatively straightforward.  As we see
in Algorithm \ref{algo:reducer}, we process the data points in a streaming
fashion from using the reducer's iterator.  In most cases, processing a point
consists of performing the SGD update on the relevant portions of our model and
then performing any local projections that go with the update.  When using our
Always-On SGD, we also keep a linked list of the points in the current block.

%\subsection{Model Synchronization} % (fold)
%\label{sub:Synchronization}

\paragraph{Parameter Communication} % (fold)
\label{par:Model Movement}
Besides processing the points, the reducers are also responsible 


\paragraph{Distributed Projections} % (fold)
\label{par:Distributed Projections}


\subsection{Threading Always-On SGD} % (fold)
\label{sub:Threading Always-On SGD}

% subsection Threading Always-On SGD (end)
