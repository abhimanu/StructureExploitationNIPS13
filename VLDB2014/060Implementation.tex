So far we have primarily discussed our sytem in terms of the high level design.
The design choices made so far are largely indepdent of the impelmentation
decisions and should make for an efficient, scalable system, regardless of
whether it is implemented with MPI or Hadoop.

For portability reasons, we build our framework on top of Apache's Hadoop and
Hadoop File System (HDFS).  Although this platform is typically not geared
towards iteratitve distributed machine learning algorithms, we demonstrate that
the tools in the system offer many advantages such as ease to set up,
robustness, and ubiquity in industry, while still enabling our framework to
work accurately and efficiently.  Our implementation uses \textit{stock
Hadoop}, enabling it to be run on any standard Hadoop cluster without requiring
additional software to be installed on the cluster.

At a high level, each epoch is one MapReduce job.  We use Hadoop's mappers to
partition our data and the reducers to perform most of the SGD computation.
Additionally, we use direct access to HDFS to communicate between different
reducers.

\subsection{Distributing the data} % (fold)
\label{sub:Ordering the data}
As was described in Section \ref{sec:mdAbstract}, our system works by dividing
the data into blocks, grouping the blocks into strata, distributing the strata
across the cluster of machines, and processing the strata in order.  To do this
in Hadoop, we use the mappers to (1) read in our data, which is unorganized,
(2) determine the block that the data points fall in, (3) send the blocks to the
appropriate reducer, and (4) order the blocks in each reducer correctly.

In order to set up the system appropriately
We assume we have a data matrix $P$ of size $N \times M$ and we are setting
Hadoop to have $d$ reducers, such that our computation will run over $d$
machines.
Blocks are of size $\lceil N/d \rceil \times \lceil M/d \rceil$.
Blocks with index $b_i$ cover $i\in[b_i\lceil N/d \rceil\ldots (b_i+1),\, N/d
\rceil)$ and blocks with index $b_j$ cover $j \in [b_j\lceil M/d \rceil,\,
(b_j+1)\lceil M/d \rceil)$.
The data comes in as a triplet $\langle i,j,P_{i,j}\rangle$.  We can calculate
which block $\mathcal{B}_{b_i,b_j}$ the point falls into by setting
\begin{align}
	b_i = \left\lfloor \frac{i}{\lceil N/d \rceil} \right\rfloor
	\;\;\;\;\;\;
	\mbox{and}
	\;\;\;\;\;\;
	b_j = \left\lfloor \frac{j}{\lceil M/d \rceil} \right\rfloor.
\end{align}




We design our strata by beginning with the blocks along the diagonal and then
rotating the diagonal around the matrix, as can be seen in Figure
\ref{fig:blocks}.  This maintains the property that all blocks in each stratum
are independent and the order is also easy to calculate.  Additionally, we can
then assign one row of blocks to each reducer.  Doing this is both simple and
means that we do not need to ever transfer the factors for the left side of the
matrix (e.g. the document model when doing topic modeling) between reducers.

Within the row of blocks to be received by each reducer, we can order the
blocks by finding the stratum number they correspond to.
The stratum order $s$ can be calculated as:
\begin{align}
	s= (b_j - b_i + d) \bmod{d}.
\end{align}
\alex{I am thinking we should move much of the above description to section 2.
it will make the problem much more concrete and we can even show it visually.}

\alex{include detailed description of what do we set as the key and value?}

Based on this desired processing the data, we would like each reducer to
receive the data for one row of blocks, and for the data going to that reducer
to be ordered by the stratum number $s$.  Hadoop makes this quite easy.  To
send the data to the correct reducer, we modify Hadoop's default partitioner to
use the value of $b_i$ as the partition number.  To order the points by $s$ we
can then modify the KeyComparator.  As such, each reducer only receives points
for the same $b_i$ and the blocks are ordered by the stratum number.
\alex{check this}

\subsection{Main Computation} % (fold)
\label{sub:Computation}

Within each reducer, the computation is relatively straightforward.  As we see
in Algorithm \ref{algo:reducer}, we process the data points in a streaming
fashion from using the reducer's iterator.  In most cases, processing a point
consists of performing the SGD update on the relevant portions of our model and
then performing any local projections that go with the update.  

When using our Always-On SGD, we also keep a linked list of the points in the
current block.  Once we have gone through all of the points in the block, we
shuffle the linked list and again process all of the points from the list.  We
do this repeatedly until all reducers have processed the points in their
current block at least once.

%\subsection{Model Synchronization} % (fold)
%\label{sub:Synchronization}

\paragraph{Parameter Communication} % (fold)
\label{par:Model Movement}
Besides processing the points, the reducers are also responsible for passing
the necessary parameters among the reducers between subepochs.  
To do this requires two steps.  First, each reducer must be able to check when
the other reducers have finished processing their points at least once.  To
accomplish this, when reducer $r$ has
processed all of the points in its current stratum $s$ at least once, it
touches on HDFS a unique file named by $\langle r,s \rangle$.   After this, the
reducer polls HDFS every 3 seconds to check if there is a file for each reduer
for the current stratum $s$.  If so, then the subepoch is over and the reducers
can sync their paramters.

To sync their parameters, each reducer writes its paremeters from
$\mathcal{S}^{(2)}$ to HDFS in a unique file (again named by the reducer $r$
and the stratum $s$).  Each file will contain a portion of the matrix
$\mathcal{S}^{(2)}$, so when serialized appropriately, each file is fairly
small (\alex{5kb?}).  After its data has been written, the reducer polls HDFS
waiting for the block of $\mathcal{S}^{(2)}$ it needs for stratum $s+1$.  Once
the file appears, the reducer can read it into memory and begin with the next
subepoch.  This passing of parameters can be seen in Figure
\ref{fig:data-movement}.

\begin{figure}[htb]
\begin{centering}
{
	\begin{tikzpicture}[scale=0.75,place/.style={draw,circle,thick,text=white, text badly centered,minimum size=2.0in}]

		\begin{scope}[>=stealth,ultra thick,->]
		\path (S1) edge[lightgray, very thick,bend right=30]  node [left=5pt] { } (S2);
		\path (S2) edge[lightgray, very thick,bend right=30]  node [left=5pt] { } (S3);
		\path (S3) edge[lightgray, very thick,bend right=30]  node [left=5pt] { } (S1);
		\end{scope}


		\node at (-2.5,-0.9) (M1) {\pgfbox[center,center]{\pgfuseimage{machine}}};
		\node at (2.5,-0.9) (M2) {\pgfbox[center,center]{\pgfuseimage{machine}}};
		\node at (0,3.6) (M3) {\pgfbox[center,center]{\pgfuseimage{machine}}};

		%\draw (0,0) circle (1cm);

		\draw (-4,-3) rectangle (-1.0,-1.5);
		\draw (1,-3) rectangle (4.0,-1.5);
		\draw (-1.5,1.5) rectangle (1.5,3);

		% Overall border
		\draw[draw=white] (-4.5,-3.5) rectangle (4.5,4.5);


		%\draw (-4.5,-3.5) rectangle (4.5,4.5);
		\filldraw[fill=blue,draw=black,opacity=0.2] (-3.8,-2.8) rectangle (-2.9,-1.6);
		\filldraw[gray, very thin,step=0.3cm,xshift=0.1cm,yshift=-0.1cm] (-3.9,-2.7) grid (-3.0,-1.5);
		\draw[black] (-3.8,-2.8) rectangle (-2.9,-1.6);

		\filldraw[fill=blue,draw=black,opacity=0.2] (-2.0,-2.8) rectangle (-1.1,-1.6);
		\filldraw[gray, very thin,step=0.3cm,xshift=0.1cm,yshift=-0.1cm] (-2.1,-2.7) grid (-1.2,-1.5);
		\draw[black] (-2.0,-2.8) rectangle (-1.1,-1.6);

		\node at (-1.45,-2.2) (S1) {$\mathcal{S}^{(2)}_1$};
		\node at (-3.3,-2.2) {$\mathcal{S}^{(1)}_1$};


		\filldraw[fill=red,draw=black,opacity=0.2] (3.8,-2.8) rectangle (2.9,-1.6);
		\filldraw[gray, very thin,step=0.3cm,xshift=-0.1cm,yshift=-0.1cm] (3.0,-2.7) grid (3.9,-1.5);
		\draw[black] (3.8,-2.8) rectangle (2.9,-1.6);

		\filldraw[fill=red,draw=black,opacity=0.2] (2.0,-2.8) rectangle (1.1,-1.6);
		\filldraw[gray, very thin,step=0.3cm,xshift=-0.1cm,yshift=-0.1cm] (1.2,-2.7) grid (2.1,-1.5);
		\draw[black] (2.0,-2.8) rectangle (1.1,-1.6);

		\node at (1.6,-2.2) {$\mathcal{S}^{(1)}_2$};
		\node at (3.4,-2.2) (S2) {$\mathcal{S}^{(2)}_2$};


		\filldraw[fill=green,draw=black,opacity=0.2] (-1.3,1.7) rectangle (-0.4,2.9);
		\filldraw[gray, very thin,step=0.3cm,xshift=0.2cm,yshift=0.2cm] (-1.5,1.5) grid (-0.6,2.7);
		\draw[black] (-1.3,1.7) rectangle (-0.4,2.9);

		\filldraw[fill=green,draw=black,opacity=0.2] (1.3,1.7) rectangle (0.4,2.9);
		\filldraw[gray, very thin,step=0.3cm,xshift=-0.2cm,yshift=0.2cm] (0.6,1.5) grid (1.5,2.7);
		\draw[black] (1.3,1.7) rectangle (0.4,2.9);

		\node at (-0.8,2.35) {$\mathcal{S}^{(1)}_3$};
		\node at  (0.9,2.35) (S3) {$\mathcal{S}^{(2)}_3$};
		%\node at (3.4,-2.2) {$\mathcal{S}^{(2)}_2$};


		\begin{scope}[>=stealth,ultra thick,->]
			%\tikzstyle{every node}=[font=\large]
			\path (M1) edge[very thick,bend right=-10]  node [left=0pt,above=-2pt] {$\sigma^{(1)}$} (M2);
			\path (M1) edge[very thick,bend right=-10]  node [left=2pt] {$\sigma^{(1)}$} (M3);
			\path (M2) edge[very thick,bend right=-10]  node [left=0pt,below=-2pt] {$\sigma^{(2)}$} (M1);
			\path (M2) edge[very thick,bend left=-10]  node [right=2pt] {$\sigma^{(2)}$} (M3);
			\path (M3) edge[very thick,bend right=-10]  node [below=9pt,right=-5.0pt] {$\sigma^{(3)}$} (M1);
			\path (M3) edge[very thick,bend left=-10]  node  [below=9pt,left=-5.0pt] {$\sigma^{(3)}$} (M2);
		\end{scope}
	\end{tikzpicture}
}
\caption{Communication for distributed synchronization. \alex{how should we graphically show this gets written to HDFS?}
\label{fig:distributeProjection}
}
\end{centering}
\end{figure}


\paragraph{Distributed Projections} % (fold)
\label{par:Distributed Projections}
In the case of a distributed projection, such as normalizing across all words
in the topic modeling example, we perform the projection between subepochs and
require a bit extra computation.  As before, each reducer waits until all of
the other reducers have processed their points from the current stratum to
begin the synchonization process.  If we are to normalize the factors in $\pi$,
we must calculate the sum $\sigma_k = \sum_{i} \pi_{i,k}$ for all $k$.
Therefore, for a reducer processing $\mathcal{S}^{(1)}_{b_i}$, we can calculate
$$\sigma_k^{(b_i)} = \sum_{i = b_i\lceil N/d\rceil}^{(b_i+1)\lceil N/d\rceil
-1} \pi_{i,k}.$$ Doing this for all $k$ gives each reducer a vector
$\sigma^{(b_i)}$.

Similar to the inter-reducer communication before, each reducer writes its
$\sigma^{(b_i)}$ to HDFS and reads $\sigma^{(b)}$ from HDFS for $b=1\ldots d$.



\subsection{Threading Always-On SGD} % (fold)
\label{sub:Threading Always-On SGD}

% subsection Threading Always-On SGD (end)
