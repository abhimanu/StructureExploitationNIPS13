For many classic machine learning problems, we have data about the
relationships between different types of objects and we would like to
understand these objects and their relationship to each other.  We consider we
have $t$ sets of objects $\mathcal{T}_1, \mathcal{T}_2, \ldots \mathcal{T}_t$,
each with $T_i = |\mathcal{T}_i|$ items which can be indexed from $j=1$ to $T_i$ as $\mathcal{T}_{i,j}$.
From this we have a dataset of relations between these objects defined by
$$\mathcal{X}: \mathcal{T}_1 \times \mathcal{T}_2\times \ldots \times \mathcal{T}_n \rightarrow \mathbb{R}.$$

Given such a dataset, we would like to find a set of values for each object
to describe that object.  Therefore, we have also $t$ sets of variables
$\Theta = \{\theta_1 \ldots \theta_t\}$ where $\theta_{i,j}$ is a set of
variables (often a vector) which describe object $\mathcal{T}_{i,j}$.  For
notational simplicity, we will say that given a vector of indices
$\mathbf{i}=(i_1,i_2,\ldots i_t)$ we can reference the relevant variables by
$\Theta_{\mathbf{i}} = \{\theta_{1,\mathbf{i}_1},\theta_{2,\mathbf{i}_2}, \ldots \theta_{t,\mathbf{i}_t} \}$.

We can now formally define our general problem:
\vspace{-2mm}
\begin{problem}
	Given dataset $\mathcal{X}$ defined over $t$ types of objects
	$\mathcal{T}$, our objective is to find $\Theta$ through the following
	minimization:
	\begin{align}
		%\arg\min_{\Theta} &\sum_{\mathbf{i} \in \{\mathcal{T}_1 \times \mathcal{T}_2 \times \ldots \times \mathcal{T}_t\}} f(\Theta_\mathbf{i},\mathcal{X}(\mathbf{i}))\\
		\arg\min_{\Theta} &\sum_{\mathbf{i} \in \mathcal{X}} f(\Theta_\mathbf{i},\mathcal{X}(\mathbf{i}))\\
		& {\rm s.t.}\; C(\Theta) = \mathbf{1},
	\end{align}
	where $f$ is any differentiable function and $C$ is a convex constraint
	across subsets of variables in $\Theta$.
\end{problem}
\vspace{-2mm}
Without loss of generality we can also set $\mathcal{T}_i = \mathcal{T}_j$ such
that we have relations between objects of the same type.  
Additionally, we can combine the loss function across multiple datasets on
similar objects by merely adding their objectives together. 

This problem defintion is very broad covering a variety of common problems as
special cases including topic modeling, dictionary learning, network
decomponsition, matrix and tensor factorization, and even mean field problems.
Throughout this paper, we will describe our system in terms of topic modeling
for clarity.  

\paragraph*{Special Case: Topic Modeling}
When performing topic modeling we have two types of objects ($t=2$) - words
and documents, which we will denote by $\mathcal{T}_1 = \mathcal{W}$ and
$\mathcal{T}_2=\mathcal{D}$.  
For each document $d\in\mathcal{D}$, we know the number of times a
word $w\in\mathcal{W}$ was used; our dataset $\mathcal{X}$ is the normalized version of these values.  
Our objective is to predict which words will be
used in which documents.  In the process, we attempt to find the topics of each
document and the words used when discussing each topic (from $K$ topics). 
To do this, we use the following model:
\begin{align*}
	p({\rm Word}\; w | {\rm Doc}\; d) = \sum_{ {\rm Topic}\; k = 1}^K p(w | {\rm Topic} \!=\! k)\cdot  p( {\rm Topic} \!=\! k | d)
\end{align*}
The variables for the probabilities that a word is used in discussing a given
topic are denoted by $\theta_1=\beta$, where $\beta_j$ is the $K$-length vector of
probabilities $p(w_j|{\rm Topic}= k)$ for all $k$.  Similarly the variables for
the probabilities that a document is about a given topic are denoted by
$\theta_2=\pi$ where $\pi_i$ is the $K$-length vector of probabilities
$p(d_i|{\rm Topic}=k)$ for all $k$.

From this, we have a similar problem statement as before but specific to topic modeling:
\begin{problem}
	Given a word $\times$ document dataset $\mathcal{X}$, find
	\begin{align}
		\arg\min_{\pi,\beta} &\sum_{d_i \in \mathcal{D},w_j \in \mathcal{W}} f(\pi_i,\beta_j,\mathcal{X}(i,j))\\
		f(\pi_i,\beta_j,x) &= (x - \pi_i\beta_j^\top)^2\\
		{\rm s.t.}\; &\sum_{k=1}^K \pi_{i,k} = 1,\; \forall d_i\in \mathcal{D}\\
		&\sum_{w_j \in \mathcal{W} } \beta_{j,k} = 1,\; \forall k = 1 \ldots K
	\end{align}
\end{problem}
Solving this produces a model of the topics of our documents and the use of our
words.  In particular, we are left with a model of the use of words in topics
given by $\beta$, which can be used to find the topics of new documents. 

In this topic model, the variables $\beta$ are an exclusive construct of the
model and are typically referred to as the model parameters.  These parameters
define our model of documents and can be used to model new documents once we
have learned our parameters.
In the general formulation of the problem earlier, for which we have a variety
object types $\mathcal{T}$, some subset of the variables in $\Theta$ are
typically considered to be model parameters.
%In the abstract 
%formulation of the problem earlier we introduced types $\mathcal{T}$. It is important to note 
%that some of these types are an exclusive construct of the model e.g. the topic-word 
%type $\beta$ in topic modelling. These types are called parameters of the model.
%\alex{ABHI: is this definition of parameters ok?}
