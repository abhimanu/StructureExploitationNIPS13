
\begin{figure}[t]
\vspace{-0.4cm}
\centering
\begin{tabular}{|c|c|}
\hline
 \multicolumn{2}{|c|} {\bf Time taken to converge} \\
\hline
\includegraphics[width=0.46\columnwidth]{fig2/speedup.eps} &
\includegraphics[width=0.46\columnwidth]{fig2/speedup2.eps} \\\hline
\end{tabular}
\vspace{-0.3cm}
\caption{\small Time taken by all methods to converge on the
three ML models, on an absolute scale (left) as well as a relative scale (right).
The methods plateau at these values in the respective plots shown
in Figure~\ref{fig:results}. The bar for \psgd is absent in the figure as it never reaches $0.059$ and stops around 
objective value $0.092$.  }
\label{fig:speed}
\end{figure}

\begin{figure}[t]
\vspace{-0.1cm}
\centering
\begin{tabular}{|c|c|}
\hline
 \multicolumn{2}{|c|} {\bf Break up of time take to finish tasks in reducer} \\
\hline
\includegraphics[width=0.46\columnwidth]{fig2/lda_piechart.eps} &
\includegraphics[width=0.46\columnwidth]{fig2/mmsb_piechart.eps} \\\hline
\end{tabular}
\vspace{-0.3cm}
\caption{\small Proportional breakup of time taken during different tasks in the 
reduce phase. The left hand graph is for \lda with 32 processors and \snytimes{32} 
dataset	and the right hand plot is for \mmsb with 32 processos and \swebgraph{8}
dataset. ``Waiting Read" is time a reducer waits to synchronize with other 
reducer to finishing 
writing its updates. ``Normalize" is the time taken to ensure the normalization constraints
for \lda and \mmsb, 
``Write" is the time taken to write the intermediate factors and output files to 
hdfs, ``Data Updates" is the time takes to perform computations on the 
data, and ``Extra Updates" is the time taken to perform extra updates while waiting 
for to synchronize with other reducers.
We can see that time for waiting to read is more than time for data updates 
in case of \lda due to distributed normalization. }
\label{fig:piechart}
\end{figure}

All four learning \schemes are evaluated on three criteria: 
1) their speed, 2) scalability (in data size as well as 
model size), and 3) quality of answers. We see that \ourmethod outperforms all 
other schemes with huge margins on all three \queries \abhi{we call the three models as 
queries} and over all three criteria.


\subsection{Scalability}
Columns two, three and four in Figure~\ref{fig:results} show the scalability of different
methods in model size, number of machines and data size for all three \queries. We see
that \ourmethod performs quite well across the spectrum. 

%\vspace{-0.4cm}
%\section{Experiments}
%\vspace{-0.3cm}
%
%\begin{figure*}[t]
%\vspace{-0.4cm}
%\centering
%\begin{tabular}{|c|c|c|c|}
%\hline
%\multicolumn{2}{|c|}{\bf Topic Modeling} & {\bf Dictionary Learning} & {\bf MMND} \\
%\hline
%Convergence Plots & Scaling in \# Cores & Convergence Plots &  Convergence Plots \\
%%\multicolumn{4}{|c|}{\bf Topic Modeling} \\
%%\hline
%%Convergence Plots & \# of Topics & \# of Processors & \# of Docs \\
%\hline
%\includegraphics[width=0.23\textwidth]{fig2/lda_convergence.eps} 
%& \includegraphics[width=0.23\textwidth]{fig2/lda_machines.eps} 
%& \includegraphics[width=0.23\textwidth]{fig2/dict_convergence.eps} 
%& \includegraphics[width=0.23\textwidth]{fig2/mmsb_convergence.eps}  
%\\
%\hline
%%{\bf Topic Modeling} & {\bf Dictionary Learning} & \multicolumn{2}{|c|}{\bf Mixed Membership Network Decomposition} \\
%\multicolumn{4}{|c|}{\bf Topic Modeling} \\
%\hline
%Machines Needed & Scaling in \# Docs & \multicolumn{2}{|c|}{Scaling in \# Topics} \\
%\hline
%\includegraphics[width=0.23\textwidth]{fig2/lda_machines_failing.eps}
%& \includegraphics[width=0.23\textwidth]{fig2/lda_datasize.eps}
%& \multicolumn{2}{|c|}{\includegraphics[width=0.46\textwidth]{fig2/lda_rankv2.eps}} \\
%\hline
%\end{tabular}
%\vspace{-0.3cm}
%\caption{\small Convergence and scalability plots for the three models (\lda, \dl, \mmsb),
%under our \ourmethod{} and baselines (\dsgd, \psgd, \graphlab). Unless otherwise stated in the plot, all methods were run with 16 cores
%and rank $K=25$. For all topic modeling plots except ``\# of Docs" and ``Machines Required", we used the NyTimes4 dataset (Table \ref{tab:dataset}).
%The convergence plots reveal the objective trajectory and final value of each method,
%while the scalability plots show how each method fares (on topic modeling) as we increase the problem rank, number of processor cores, and data size.
%In the bottom left, we also show the minimum number of machines required for a given topic modeling dataset size, for
%\ourmethod{} and \graphlab.}
%\vspace{-0.5cm}
%\label{fig:results}
%\end{figure*}

\paragraph{Dataset size}
For data size scalability experiment, we fix the number of processors as 32 
and rank as 25 for all three \allmethods and all three. 
Column four of Figure~\ref{fig:results} shows the convergence time taken by the different
\schemes with respect to increasing data size. We vary the data size in two ways: 1) by 
making the data denser e.g. increasing the number of edges in the graph (or 
entries in the adjacency matrix ) for  
\mmsb instead of increasing the number of vertices, 
and 2) by increasing the dimensions of the data e.g.
increasing the 
number of documents. In this set of experiments we vary the \lda and \sdl data 
size by increasing the number of documents. For \mmsb we increase the data by making it 
denser with inclusion of more edges as explained in section~\ref{subsec:data}. 
A subtle distinction between the two ways is that increasing the dimension of 
the data also increases the model size e.g. for the document by word input count matrix 
for \lda increasing the columns (number of words) increases the size of 
topic-word model parameter. 
This has bad repercussions for an approach
that is only scalable in data e.g. \psgd. We observe that \psgd runs out of memory very early 
in the data-size experiments for \lda and \sdl since , but keeps running with increase in data for a 
while in case of \mmsb (although with poorer result quality and slower convergence) ~\abhi{get
\psgd results for \mmsb and \sdl asap}. Our \ourmethod scales well for both ways of increasing 
data size. It never runs out of memory in any of the \query types for sizes shown in the 
figure as well as is faster by at least a factor of 2 usually. Figure~
\ref{fig:ldaMachinesNeeded} shows a comparison of number of machines needed for \ourmethod
and \graphlab for \lda. Figures~\ref{fig:results} and \ref{fig:ldaMachinesNeeded} show that 
not only is \ourmethod faster but also needs fewer processors for a given data size 
even though like rest of the three \allmethods it needs to store all its variables in 
memory for computation. This speed and economy stems from the fact that \ourmethod 
is distributed over both data and model, unlike \graphlab which is distributed over 
vertices that may not corresspond to data and model distribution. Or unlike 
\psgd that only is distributed over data and not model.  
%\alex{we need to be more concrete here and explain that we are all memory based
%systems}


\paragraph{Model parameters}
The model parameter are the types of variables that arise as a construct of the model (
Section~\ref{subsec:abstractProblem}). For example, the number of topics 
in \lda or the number of basis vectors in \sdl is a construct of the model.  
Column two shows the variations for different models for increase in rank (topic, bases or
network role number). For this set of experiments we fix number of processors as 16 for all 
\allmethods and \queries. 
The datasets used are \snytimes{4}, \swebgraph{1}, and \imagenet for \lda,
\mmsb and \sdl respectively.
Here again we see that \ourmethod is faster to converge than all the other
\allmethods. The difference in convergence rate of \graphlab and \ourmethod is especially stark
for \sdl. The reason for this is that \graphlab oscillates by big margins for this \query as
seen in Figure~\ref{fig:results}, second row, first column. 
\psgd runs out of memory quite early as expected for \lda and \sdl.
Since the dimensions of the \mmsb data are smaller (281,904) that \lda (1,200,000) or \sdl
(1,261,406), \psgd runs out of memory a little later for this \mmsb \query. 

\paragraph{Processors}
Column three shows the effects of varying number of processors for a fixed data and model size.
We have set rank as 25 and used \snytimes{32}, \swebgraph{8}, and \simagenet{4}
as data sets for \lda, \mmsb, and \sdl. 
We scale well in terms of processors needed for larger data sets for \lda compared to \graphlab.
We observe in our experiments that for smaller data sizes the \ourmethod slowly starts showing 
diminishing returns with increasing processors in case of \lda and its curve
is worse than \graphlab. But for larger datasets \ourmethod
has the best curve. This happens since for smaller dataset time taken to synchronize 
among the reducers dominates time taken to perform actual computations on the data. 
This fact is also illustrated in figure~\ref{fig:piechart}. 
Figure~\ref{fig:piechart} shows a break up of the percentage of time each reducer
spends on different tasks. For the \lda tasks every reducer has to perform 
distributed projections defined in \ref{sub:Computation}. Due to this it has to poll 
all the other reducers at the end of every subepoch \abhi{alex: we should verify this}.
This leads to extra waits compared to a non-dstributed projection.  
But compared to \ourmethod, \dsgd performs much worse (see \lda graph, column three,
row one). Our method due to its provably correct (\abhi{citations?}) Always-On property
mitigates the ill effects of sync-waits by doing more work.
For \mmsb its 
time spent on computation is the highest among all its tasks. As there is far fewer 
synchronization to be done. Though there is still synchronization going on between two 
consecutive blocks in a sub-epoch where reducer in block $i$ is waiting for reducer in 
block $i+1$ to finish so that it can move on to block $i+1$ in the next sub-epoch.
The actual number of datapoints updated in extra-updates are 950,760 (of $10.3*10^9$ 
of total data points) and 14,556,089 ($3.2*10^9$ of total data points) per 
reducer (in total 32 reducers) per epoch. 
In case of \dl and \mmsb \ourmethod performs significantly well compared to others.
For these set of experiments \psgd doesn't converge to the optimal point as more 
machines are added due to its bad averaging guarantees
on non-convex problems~\cite{zinkevich2010parallelized}. It is significantly slower too
since all the variable after independently being computed in the 
update phase on $d$ reducers ($d=32$ here) are passed through a single 
reducers for averaging and thus
creating a bottleneck.  

\subsection{Convergence speed}
Figure~\ref{fig:speed} shows convergence times 
for \ourmethod{}, \dsgd, \psgd, and \graphlab over the three 
models: \lda, \mmsb and \dl. \ourmethod{} is faster
by anywhere between $2.6\times$ (vs \graphlab on \lda) 
to $26.2\times$ (vs \graphlab on \sdl). This clearly is due to the Always-On 
property of \ourmethod.
The number of reducers used is 16 and the datasets are \snytimes{4}, \swebgraph{1} and 
\imagenet. 
%\begin{figure}[t]
%\vspace{-0.4cm}
%\centering
%\begin{tabular}{|c|c|}
%\hline
% \multicolumn{2}{|c|} {\bf Time taken to converge} \\
%\hline
%\includegraphics[width=0.46\columnwidth]{fig2/speedup.eps} &
%\includegraphics[width=0.46\columnwidth]{fig2/speedup2.eps} \\\hline
%\end{tabular}
%\vspace{-0.3cm}
%\caption{\small Time taken by all methods to converge on the
%three ML models, on an absolute scale (left) as well as a relative scale (right).
%The methods plateau at these values in the respective plots shown
%in figure~\ref{fig:results}. The bar for \psgd is absent in the figure as it never reaches $0.059$ and stops around 
%objective value $0.092$.  }
%\label{fig:speed}
%\end{figure}

\subsection{Convergence quality}
The number of processors used is 16 and rank is 25 for all three \allmethods and \queries.
The datasets used are \snytimes{4}, \swebgraph{1}, and \imagenet for \lda,
\mmsb and \sdl respectively.
Though graphlab is large scale and is a distributed work load system, it has no theoretical 
guarantees. This shows up in the quality of answers we get. \ourmethod is distributed, faster,
better scalable, and gives higher quality of answers. Figure~\ref{fig:results}, 
column one shows the convergence curve for all the approaches over all three \queries.
\ourmethod achieves better convergence value with faster speed compared to all other 
approaches. The higher quality of results stems from the fact that \ourmethod is 
theoretically sound. \graphlab oscillates quite a lot for \mmsb and \sdl. This is due to  
the fact that it is picking up arbitrary vertices and updating them without any guarantees
for the constraints (as discussed in Section~\ref{sec:complexQues})
%\graphlab oscillation reasons and patterns, \dsgd and \psgd converges to a poor
%quality,
\subsection{Why \ourmethod wins }
\ourmethod wins emphatically on all three criteria, and over all three \queries is due to 
follwoing salient properties:
\begin{itemize}
\item Unlike \psgd, it is distributed over data as well as model. This gives \ourmethod 
atleast twice the speed as well as scalability compared to \psgd 
\item Unlike \graphlab, it is theoretically grounded. This gives \ourmethod a guarantee 
for high quality answers
\item Unlike \dsgd it does not waste time while waiting to synchronize. This makes it 
more scalable (as we saw in case of machine scalability) and faster by several factors of
magnitude.
\end{itemize}
%Put a plot of waiting times in different concstraints case and argue why it
%helps here.
