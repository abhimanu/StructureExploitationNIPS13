We demonstrate that \method is a generic large scale machine learning system by
applying it over diverse set of real world problems that are non-trivial to
solve. We describe here a set of problems in machine learning from the sub-areas
of graphical models, natural language processing, computer vision and
computational social sciences. These applications are real world problems that
involve non-trivial complexity described in Section~\ref{sec:complexQues}.

\subsection{Latent dirichlet allocation (\lda)}
We described \lda in detail in Section~\ref{sec:mdAbstract}. We further
elaborate on this model and its application here. The assumption that there
are a fixed set of topics and each document is composed of topics with certain
probabilistic weights is helpful in search engine queries and information
retrieval. The search engine can use topics as part of their indices to keep
similar documents together. This helps in retrieving faster and accurate results for
search queries~\cite{Wei:LDM}. A similar strtegy is used by libraries for
efficient storage of documents~\cite{Newman:ETM} in digital form. Besides prevalent in
text mining and natural language processing, it is one of the most common
building blocks of complex graphical models such as nested
chinese restaurant process~\cite{Blei:NCR} used in genetics for clustering
micro-array data~\cite{Qin:CMG}, hierarchical dirichlet process~\cite{hdp:2006} 
used for tracking trending topics~\cite{Gao:TCT}, among other things.

This model uses non-negativity, simplex as well as distributed simplex
constraints defined in Section~\ref{par:Simplex Constraints}. We will see in
Section~\ref{sec:eval} as to how these constraints affect run time
and convergence quality.

\begin{table}
\centering
\scalebox{0.95}{
\begin{tabular}{c|c|c|c|} %\hline
Dataset  & Dimensions & Data points & Size(GB) \\ \hline
\nytimes  & $0.3*10^6\times$102,660 & $0.1*10^9$ &  1.49  \\ \hline
\snytimes{4} & $1,2*10^6\times$102,660 & $0.4*10^9$ &  6.08  \\ \hline
\snytimes{16} & $4.8*10^6\times$102,660 & $1.6*10^9$ &  25.12  \\ \hline
\snytimes{32} & $50.5*10^6\times$102,660 & $9.6*10^9$ &  50.5  \\ \hline
\snytimes{64} & $19.2*10^6\times$102,660 & $6.4*10^9$ &  103.4  \\ \hline
\snytimes{256} & $76.8*10^6\times$102,660 & $25.6*10^9$ &  421.42  \\ \hline
\hline
\imagenet & $1.26*10^6\times$1,000 & $1.25*10^9$ & 16.4  \\ \hline
\simagenet{0.5} & $0.63*10^6\times$1,000 & $0.63*10^9$ & 8.0  \\ \hline
\simagenet{1.5} & $1.89*10^6\times$1,000 & $1.89*10^9$ & 25.1  \\ \hline
\simagenet{2} & $2.51*10^6\times$1,000 & $2.51*10^9$ & 33.8  \\ \hline
\simagenet{2.5} & $3.15*10^6\times$1,000 & $3.15*10^9$ & 42.6  \\ \hline
\simagenet{3} & $3.77*10^6\times$1,000 & $3.77*10^9$ & 51.3  \\ \hline
\simagenet{3.5} & $4.40*10^6\times$1,000 & $4.40*10^9$ & 60.0  \\ \hline
\simagenet{4} & $5.03*10^6\times$1,000 & $5.03*10^9$ & 68.8  \\ \hline
\simagenet{8} & $10.03*10^6\times$1,000 & $10.03*10^9$ & 138.8  \\ \hline
\simagenet{16} & $20.13*10^6\times$1,000 & $20.13*10^9$ & 288.7  \\ \hline
\hline
\twitter & 281,903$\times$ 281,903 & $2.3*10^6$ & 0.03 \\ \hline
\swebgraph{1} & 281,903$\times$ 281,903 & $0.39*10^9$ & 6.0 \\ \hline
\swebgraph{2} & 281,903$\times$ 281,903 & $0.79*10^9$ & 12.1 \\ \hline
\swebgraph{3} & 281,903$\times$ 281,903 & $1.19*10^9$ & 18.1 \\ \hline
\swebgraph{4} & 281,903$\times$ 281,903 & $1.59*10^9$ & 24.2 \\ \hline
\swebgraph{5} & 281,903$\times$ 281,903 & $1.98*10^9$ & 30.2 \\ \hline
\swebgraph{6} & 281,903$\times$ 281,903 & $2.38*10^9$ & 36.3 \\ \hline
\swebgraph{7} & 281,903$\times$ 281,903 & $2.78*10^9$ & 42.3 \\ \hline
\swebgraph{8} & 281,903$\times$ 281,903 & $3.17*10^9$ & 48.3 \\ \hline
\swebgraph{16} & 281,903$\times$ 281,903 & $6.35*10^9$ & 96.7 \\ \hline
\hline
\end{tabular}
}
\caption{Dimension, size and data points statistics for different datasets. 
The data point is an edge in case of \mmsb, an entry in the pixel matrix 
for \dl and a word count in case of \lda input matrix.	
The exact figures are rounded off for simplicity. Size is the file size in
gigabytes. The biggest dataset (\snytimes{256}) is of size approximately 0.5
terabytes.}
\label{tab:dataset}
\end{table}

\subsection{Sparse Dictionary learning (\sdl)}
Dictionary Learning (\dl) is a classical model in computer vision used for image
denoising, restoration~\cite{Mairal07sparserepresentation} and
classification~\cite{RamirezSS10}. Given a signal matrix $Y_{m,n}$ where each
column of $Y$ is an observation of a signal, with $n$ such observation. We
would like represent each observed signal (column of $Y$) $Y_j$ as weighted
combination of $K$ basis vectors $D_k$ (or columns). The basis vectors $D_k$s
form the columns of the matrix $D$ called dictionary. The underlying assumption
is that there is an inherent set of basis vectors that is the building block
of any signal observed. This helps in image classification as the incoming
unlabeled image query can be matched with labled image's weights of basis vector
to predict a label. Mathematically we are solving the following query:

% {\small
\begin{align}
&\arg\min_{\alpha,D} L(Y,\alpha,D) =\frac{1}{2}||Y-D\alpha||_2^2 + \lambda||\alpha||_1
\label{eqn:dictL}\\
&\text{s.t.} \; \forall j, D_j^TD_j \leq 1 \nonumber
\end{align}
% }
$\alpha_j$, the $j$th column of $\alpha$, is the weight coefficient for basis
vectors in signal $j$. The $\lambda||\alpha||_1$ in equation~\ref{eqn:dictL} as
discussed in Section~\ref{par:Sparse Models}, helps in removing the noise from the
signal~\cite{Mairal07sparserepresentation}.
 
\subsection{Mixed network decomposition models (\mmsb)}
\mmsb models or multi-role models are very useful in social sciences. The premise
is that in a social network people are part of many communities simultaneously. 
This runs counter to traditional clustering techniques (spectral clustering, k-means) 
that assume non-overlapping clusters. This formulation is important in various social
settings where people play different roles in different communities. This model can 
find the affinity of a given person (probability that he is a member) to 
pre-defined set of communities. Given $K$ communities and Every person  

These probabilities can be used to infer like-dislikes
and various other social behavior of a person. 
%\alex{abhi: fill this in}
