\documentclass[english]{article}
\usepackage{times}
\usepackage[latin9]{inputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2cm,rmargin=2cm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{color}
\usepackage{bm}
\usepackage{hyperref} % Needed by xr package since main doc uses hyperref
\usepackage{xr}
\usepackage{xspace}
\externaldocument{paper}

\input{dfn.tex}

\def\toptitlebar{\hrule height4pt\vskip .25in\vskip-\parskip}

\def\bottomtitlebar{\vskip .29in\vskip-\parskip\hrule height1pt\vskip
.09in} %



% no indents
\parindent=0pt
%\newcommand{\Ind}{\ensuremath{\mathbb{I}}}

\newtheorem{observation}{Observation}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{algo}{Algorithm}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
%\newtheorem{question}{Question}
\newtheorem{example}{Example}
\newcommand{\pushright}[1]{\ifmeasuring@#1\else\omit\hfill$\displaystyle#1$\fi\ignorespaces}
\newcommand{\pushleft}[1]{\ifmeasuring@#1\else\omit$\displaystyle#1$\hfill\fi\ignorespaces}
%\newtheorem{answer}{Answer}
%\newtheorem{proof}{Proof}

\newcommand{\abhi}[1]{\textcolor{orange}{abhi-comment: #1}}
\newcommand{\alex}[1]{\textcolor{red}{\\ alex-comment: #1}}
\newcommand{\qirong}[1][1]{\textcolor{fuschia}{\\ qirong-comment: #1}}
\newcommand{\eric}[1][1]{\textcolor{blue}{\\ eric-comment: #1}}

\begin{document}


\appendix

{\linewidth\hsize \vskip 0.1in \toptitlebar \centering
	{\LARGE\bf
	Appendix \\[2mm]
	FlexiLearn: Slow-Worker-Agnostic Distributed Learning for Big Models on Big Data}
\bottomtitlebar}
% TODO: another title?

This appendix contains proof details for the main paper, as well as some experimental details. In this document, some of the
theorems and equations refer to the main paper; we put ``(main paper)" behind these references
to avoid confusion.


\section{Parameter Tuning for Experiments}

\begin{table}[h]
\centering
\begin{tabular}{c|c|c|c|}
{\bf Model}  & {\bf \lda} & {\bf \sdl} & {\bf \mmsb}  \\ \hline \hline
\ourmethod{}'s $\eta_0$ & 0.01 & 0.01 & 0.01    \\ \hline 
\ourmethod{}'s $\eta_{'}$ & 0.01 & 0.01 & 0.05 \\ \hline
\dsgd's $\eta_0$ & 0.01 & 0.01& 0.05\\ \hline
\graphlab's $\eta_0$ & 0.05 & 0.001 & 0.1  \\ \hline
\graphlab's step\_dec & 0.9 & 0.9 & 0.9   \\ \hline
\psgd's $\eta_0$ & 0.005& 0.01 & 0.1\\ \hline
\end{tabular}

\caption{Final tuned parameter values for \ourmethod, \dsgd, \graphlab and \psgd. All the methods are tuned to 
perform optimally. $\eta_0$ is the initial step size, where $\eta$ is defined in equation~\ref{equ:sgd-update-lda}. 
$\lambda$ is the \sdl $\ell_1$ penalty defined in equation~\ref{eqn:dictionary}. $\eta^{'}$ is parameter that modifies the 
learning rate when extra updates are executed while waiting for slow workers. step\_dec is a parameter for decreasing learning
rate for \graphlab used in their collaborative filtering library.
}
\label{tab:tunedParams}
\end{table}
Table~\ref{tab:tunedParams} shows the final parameter values for each problem,
after they have been tuned optimally for each method. 
$\eta^{'}$ controls the learning rate in additional updates by the worker when it is waiting 
for slower ones to finish. This makes it satisfy the condition in 
equation~\ref{eqn:varianceDecreaseCondition}. The modified learning 
rate $\eta_t^{'}=\eta_t*(\eta^{'})^{x+1}$ where $x$ is the number of extra update iterations the worker has perfomed 
while waiting in this sub-epoch. One iteration here means 
updating all the data points in the curent sub-epoch once. $\eta_t$ is the sub-epoch's original learning rate without any extra 
updates similar to \dsgd. The learning rate for epoch $t$ is given by $\eta_t=\frac{\eta_0}{t+1}$ for \ourmethod{} and \dsgd.  


\section{Convergence Proof: Theorem~\ref{theo:asymptotConverge} (main paper)}
From equation~\ref{equ:sgd-update-lda} (main paper) we have
\begin{eqnarray}
\itertO{\psi} &=& \itert{\psi} - \eta_t\delta
\itert{L}(\itert{V},\itert{\psi}) \nonumber\\ %\Longrightarrow \itertO{\psi} 
&=& \itert{\psi} - \eta_t\nabla
\mathcal{L}(\itert{\psi}) + \eta_t\left[ \nabla\mathcal{L}(\itert{\psi}) -
\delta \itert{L}(\itert{V},\itert{\psi})\right] \nonumber\\
 &=& \itert{\psi} - \eta_t\nabla
\mathcal{L}(\itert{\psi}) + \eta_t\varepsilon_t
\end{eqnarray} 
 
Using $n_i$ and $N_w$ as defined in equation~\ref{eqn:workerIpoints} (main paper)

\begin{align}
\psi^{(t+(\sum_1^w n_i)m)} = \itert{\psi} + \sum_{i=t}^{t+m(\sum_1^w n_i)}
-\eta_i\mathcal{\nabla L}(\iiter{\psi}) +  \sum_{i=t}^{t+m(\sum_1^w n_i)}
\eta_i\varepsilon_i \nonumber\\
\Longrightarrow \psi^{(t+mN_w)} = \itert{\psi} + \sum_{i=t}^{t+mN_w}
-\eta_i\mathcal{\nabla L}(\iiter{\psi}) + \sum_{i=t}^{t+mN_w}
\eta_i\varepsilon_i \nonumber\\ 
\text{assuming~$\sum_1^w n_i=N_w$} \nonumber \\
\Longrightarrow \psi^{(t+mN_w)} = \itert{\psi} + \sum_{i=t}^{t+mN_w}
-\eta_i\mathcal{\nabla L}(\iiter{\psi}) + M_{mN_w}
\end{align}

where $M_{mN_w}=\sum_{i=t}^{t+mN_w} \eta_i\varepsilon_i$ is a martingale
sequence since it is a sum of martingale difference sequence. $mN_w$
captures the $m$ whole sub-epochs of work done as a whole by all the workers combined.
From Doobs martingale inequality %(\cite{Friedman:1975}, ch. 1, Thm 3.8)
(Friedman, 1975, ch. 1, Thm 3.8)
\begin{equation}
P\left(\sup_{t+mN_w\geq r \geq t}|M_r|\geq c\right) \leq \frac{E\left[\left(
\sum_{i=t}^{t+mN_w}\eta_i\varepsilon_i\right)^2\right]}{c^2}
\label{eqn:doobs}
\end{equation}

where $M_r=\sum_{i=t}^r\eta_i\varepsilon_i$. Lets look at the RHS of
equation~\ref{eqn:doobs} above:
\begin{align}
E\left[\left(\sum_{i=t}^{t+mN_w}\eta_i\varepsilon_i\right)^2\right] &=
E\left[\sum_{i=1}^{mN_w}\left(\eta_i\varepsilon_i\right)^2\right]
\nonumber\\\text{(equation~\ref{eqn:martingaleAssumption} (main paper)
$\Longrightarrow~E[\varepsilon_i \varepsilon_j]=0$ if $i\neq j$)}\nonumber\\
=\sum_{i=1}^{mN_w}\eta_i^2E[\varepsilon_i^2] &\leq
\sum_{i=1}^{mN_w}\eta_i^2D \to 0 \nonumber\\
\text{where $E[\varepsilon_i^2]<D~\forall i$ and assuming $\sum\eta_i^2 <\infty$
} \nonumber\\ 
\lim_{t\to\infty}\Longrightarrow P\left(\sup_{i\geq t}|M_i|\geq c\right) =
0~~as~~t\to\infty
\label{eqn:asymptotoError}
\end{align}

From equation~\ref{eqn:asymptotoError} we have
\begin{align*}
\psi^{(t+mN_w)} = \itert{\psi} + \sum_{i=t}^{t+mN_w}
-\eta_i\mathcal{\nabla L}(\iiter{\psi})
% \label{eqn:asymptotConverge}
\end{align*}
asymptotically.

Note that we do a theoretical
analysis of the algorithm without projection steps. Extending the proof to include 
projection can be done by using Arzela-Ascoli theorem and the limits of
converging sub-sequence of our algorithm's SGD updates %~\cite{Kushner:yin}.
(Kushner and Yin, 2003).
\hfill$\blacksquare$
\label{append:convergenceProof}
\section{Intra sub-epoch variance}

\begin{align}
\atiter{\psi}{t+1} = \atiter{\psi}{t} -
\delta\atiter{\psi}{t}(\atiter{V}{t},\atiter{\psi}{t}) 
\label{eqn:iterDelta1} \\
%\end{eqnarray}
\text{where 
$\,\delta\atiter{\psi}{t}(\atiter{V}{t}, \atiter{\psi}{t}) =
\atiter{\eta}{t}\delta\atiter{L}{t}(\atiter{V}{t},\atiter{\psi}{t})\Rightarrow
\raisePsi{(t+1)} = \raisePsi{t} - \eta_{t} \delta L^{t} \! (V^{t},
\raisePsi{t})$} \nonumber\\
\text{Summing equation~\ref{eqn:iterDelta1} over $n_i$, the number of points
updated in block $i$ of a sub-epoch} \nonumber\\
\raisePsi{t+n_{i}} = \raisePsi{t} - \sum_{i=1}^{n_{i}} \! \eta_{t+i} \delta
L^{t+i}(V^{t+i}, \psi^{t+i})
\end{align}

As defined earlier, $V$ denotes the joint potential for all the $n_i$
points encountered in block $i$. The equation~\ref{eqn:potentialDef} (main paper) for $V$ is 

\begin{align}
p(\raisePsi{(t+n_i)}|\raisePsi{t}) d\raisePsi{(t+n_i)} &= 
p(V(\raisePsi{(t+n_i)}, \raisePsi{t})) dV \nonumber\\
\Rightarrow p(\raisePsi{(t+n_i)}) d\raisePsi{(t+n_i)}  
&= \int_{\raisePsi{t}} \!
p(\raisePsi{(t+n_i)}|\raisePsi{t}) p(\raisePsi{t}) d \raisePsi{t} 
d \raisePsi{(t+n_i)}
= \int_{\raisePsi{t}} \! p(V(\raisePsi{(t+n_i)}, \raisePsi{t})) dV 
p(\raisePsi{t}) d \raisePsi{t}
\label{eqn:newPotentialDef}
\end{align}

\begin{lemma}
	Let $u(\raisePsi{(t+n_i)})$ be a function of $\raisePsi{(t+n_i)}$ then
	
\begin{equation*}
\mathbb{E}^{\raisePsi{(t+n_i)}}[u(\raisePsi{(t+n_i)})] =
\mathbb{E}^{\raisePsi{t}}[\mathbb{E}^{V}[u(\raisePsi{(t+n_i)})]]
\end{equation*}
\label{theo:expectNewPsi}
\end{lemma}
\textbf{Proof.}
From equation~\ref{eqn:newPotentialDef}
\begin{align}
\mathbb{E}^{\raisePsi{(t+n_i)}}[u(\raisePsi{(t+n_i)})] &= 
\int_{\raisePsi{(t+n_i)}} \! u(\raisePsi{(t+n_i)}) p(\raisePsi{(t+n_i)}) d 
\raisePsi{(t+n_i)} \nonumber\\
&= \int_{\raisePsi{t+i}} 
\! u(\raisePsi{(t+n_i)}) P(\raisePsi{(t+n_i)}) d\raisePsi{(t+n_i)}\nonumber\\
&= \int_{V} \! \int_{\raisePsi{t}} \! u(\raisePsi{(t+n_i)}) P(V(\raisePsi{(t+n_i)}, \psi)) 
dV P(\raisePsi{t}) d\raisePsi{t}\nonumber\\
&= \mathbb{E}^{\raisePsi{t}}[\mathbb{E}^{V}[u(\raisePsi{(t+n_i)})]]
\nonumber
\end{align}
\hfill$\blacksquare$
\begin{lemma}
\begin{equation*}
\mathbb{E}^{V}[\delta L^{t+i} (v^{t+i}, \psi^{t+i})] =
\frac{d\mathbb{E}^{V}[L^{t+i} (v^{t+i}, \psi^{t+i})]}{d\psi^{t+i}}
\end{equation*}
\label{theo:derivativeExpect}
\end{lemma}
\textbf{Proof.}
Due to randomness in picking the point to be updated in iteration $t+i$ We have 
\begin{align*}
\mathbb{E}^{V}[L^{t+i} (v^{t+i}, \psi^{t+i})] &= \int L(y,\psi^{t+i}) dy\\ 
\Rightarrow \frac{d\mathbb{E}^{V}[L^{t+i} (v^{t+i},
\psi^{t+i})]}{d\psi^{t+i}} &=
\mathbb{E}^{V}[\frac{dL^{t+i} (v^{t+i},
\psi^{t+i})}{d\psi^{t+i}}]=\mathbb{E}^{V}[\delta L^{t+i} (v^{t+i}, \psi^{t+i})]
\end{align*}
\hfill$\blacksquare$

\begin{lemma}
\begin{equation*}
\mathbb{E}^{V}[L^{t+i}
(v^{t+i}, \psi^{t+i})] = \mathbb{E}^{v^{t+i}}[L^{t+1} (v^{t+i},
\psi^{t+i})]
\end{equation*}
\label{theo:independentPotentialExpectation}
\end{lemma}
\textbf{Proof.}

Using the definition of $V^{t+i}$ in equation~\ref{eqn:potentialDef} (main paper), the
fact that $V$ is a joint variable of each $V^{t+i}$  and an any
iteration $t+i$ the chance of picking any data point is completely random and
indpendent of any other iteration.
\begin{equation*}
\mathbb{E}^{V}[L^{t+i}
(v^{t+i}, \psi^{t+i})] = \mathbb{E}^{v^{t+i}}[L^{t+1} (v^{t+i},
\psi^{t+i})]
\end{equation*}
\hfill $\blacksquare$

\begin{lemma}
\begin{equation*}
\mathbb{E}^{V}[\frac{dL^{t+i} (v^{t+i},
\psi^{t+i})}{d\psi^{t+i}}\frac{dL^{t+j} (v^{t+j}, \psi^{t+i})}
{d\psi^{t+j}}] =\frac{d\mathbb{E}^{v^{t+i}}[L^{t+i}
(v^{t+i}, \psi^{t+i})]}{d\psi^{t+i}}\frac{d\mathbb{E}^{v^{t+i}}[L^{t+i} (v^{t+i},
\psi^{t+i})]}{d\psi^{t+i}}
\end{equation*}
\label{theo:independentPotentialCovariance}
\end{lemma}
\textbf{Proof.}
Two different data
points picked at iteration $(t+i)$ and $(t+j)$ are independent of each other.
Using this fact and the definition of potetntial function $V$ in
equation~\ref{eqn:newPotentialDef}
\begin{align*}
\mathbb{E}^{V}[\frac{dL^{t+i} (v^{t+i},
\psi^{t+i})}{d\psi^{t+i}}\frac{dL^{t+j} (v^{t+j}, \psi^{t+i})}
{d\psi^{t+j}}] &= \mathbb{E}^{V}[\frac{dL^{t+i} (v^{t+i},
\psi^{t+i})]}{d\psi^{t+i}}]\mathbb{E}^{V}[\frac{dL^{t+j} (v^{t+j},
\psi^{t+i})} {d\psi^{t+j}}] \\
\text{\bigg(using lemma~\ref{theo:derivativeExpect}\bigg)}& =
\frac{d\mathbb{E}^{V}[L^{t+i} (v^{t+i}, \psi^{t+i})]}{d\psi^{t+i}}
\frac{d\mathbb{E}^{V}[L^{t+j} (v^{t+j}, \psi^{t+j})]}{d\psi^{t+j}} \\
\text{\bigg(using lemma~\ref{theo:independentPotentialExpectation}\bigg)}& =
\frac{d\mathbb{E}^{v^{t+i}}[L^{t+i} (v^{t+i}, \psi^{t+i})]}{d\psi^{t+i}}
\frac{d\mathbb{E}^{v^{t+j}}[L^{t+j} (v^{t+j}, \psi^{t+j})]}{d\psi^{t+j}}
\end{align*}
\hfill $\blacksquare$

\begin{theorem}
\label{theo:optimaGradient}
We define $\psi_*$ as the global optima and $\Omega_0$ as the hessian of the
loss at $\psi_*$ i.e. $\Omega_0 = \frac{d^2\mathbb{E}[L(\psi_*)]}{d\psi_*^2}$
(assuming that $\psi$ is univariate) then
\begin{align*}
\frac{d\mathbb{E}^{v^{t+i}}[L^{t+i} (v^{t+i}, \psi^{t+i})]}{d\psi^{t+i}} =
\Omega_0(\psi_t-\psi_* + \delta_i) + \order{\rho_t^2}
\end{align*}
where $\order{\rho_t^2} = \order{|\psi_{t+i}-\psi_*|^2}$ with the assumption 
that $\order{\rho_{t+i}}$ is small $\forall i\geq 0$ and $\delta_i =
\psi_{t+i}-\psi_t$.
\end{theorem}
\textbf{Proof.}
Lets define $\phi(\psi_{t+i}) = \mathbb{E}^{v^{t+i}}[L^{t+i} (v^{t+i},
\psi^{t+i})]$ 
Using Taylor's theorem and expanding around $\psi_*$
\begin{align*}
\phi(\psi^{t+i}) = \phi(\psi_*) + \frac{d\phi(\psi_*)}{d\psi_*}
(\psi^{t+i}-\psi_*) + \frac{(\psi^{t+1}-\psi_*)^2}{2}
\frac{d^2\phi(\psi_*)}{d\psi_*^2} + \order{(\psi^{t+i}-\psi_*)^3}\\
= \phi(\psi_*) + \frac{(\psi^{t+i}-\psi_*)^2}{2}
\frac{d^2\phi(\psi_*)}{d\psi_*^2} + \order{(\psi^{t+i}-\psi_*)^3}
\,\text{\bigg(as $\frac{d\phi(\psi_*)}{d\psi_*} = 0$ at optima\bigg) } \\
\Rightarrow \frac{d\phi(\psi^{t+i})}{d\psi^{t+i}} =
(\psi^{t+i}-\psi_*)\frac{d^2\phi(\psi_*)}{d\psi_*^2} +
\order{(\psi^{t+i}-\psi_*)^2} \\
\Rightarrow \frac{d\mathbb{E}^{v^{t+i}}[L^{t+i} (v^{t+i},
\psi^{t+i})]}{d\psi^{t+i}} = \Omega_0(\psi^t-\psi_* + \delta_i) +
\order{\rho_t^2} \text{\bigg( with the assumption that $\order{\rho_t}$ is
small} \\ \text{we have $\order{\rho_{t+i}^2} = \order{\rho_t^2} $ 
\bigg)}
\end{align*}
\hfill $\blacksquare$

\begin{theorem}
With $\psi_*$ as defined in theorem~\ref{theo:optimaGradient} and assuming that $\psi$ is 
univariate we have
\begin{align*}
\mathbb{E}^{v^{t+i}}[(\frac{dL^{t+i} (v^{t+i},
\psi^{t+i})} {d\psi^{t+i}})^2] = \Omega_1 +
\order{\mathbb{E}[\order{\rho_t}]} + \order{\rho_t^2}
\end{align*}
where $\order{\rho_t^2}$ and $\delta_i$ are as defined
in theorem~\ref{theo:optimaGradient} and $\Omega_1=\mathbb{E}^{v^{t+i}}[
(\frac{dL^{t+i}(v^{t+i},\psi_*)}{d\psi_*})^2]$
\label{theo:optimaGradientSquare}
\end{theorem}
\textbf{Proof.}
Expanding $L^{t+i} (v^{t+i},\psi^{t+i})$ around $\psi_*$ using Taylor's
theorem 
\begin{align*}
L^{t+i} (v^{t+i},\psi^{t+i}) = L^{t+i} (v^{t+i},\psi_*) + \frac{dL^{t+i}
(v^{t+i},\psi_*)}{d\psi_*}(\psi^{t+i}-\psi_*) \\
+ \frac{1}{2}\frac{d^2L^{t+i}
(v^{t+i},\psi_*)}{d\psi_*^2}(\psi^{t+i}-\psi_*)^2 +
\order{(\psi^{t+i}-\psi_*)^3}\\
\Rightarrow \frac{dL^{t+i} (v^{t+i},\psi^{t+i})} {d\psi^{t+i}} = \frac{dL^{t+i}
(v^{t+i},\psi_*)}{d\psi_*} + \frac{d^2L^{t+i}
(v^{t+i},\psi_*)}{d\psi_*^2}(\psi^{t+i}-\psi_*) + \order{(\psi^{t+i}-\psi_*)^2}
\\
\Rightarrow \mathbb{E}^{v^{t+i}}[(\frac{dL^{t+i} (v^{t+i},\psi^{t+i})}
{d\psi^{t+i}})^2] = \mathbb{E}^{v^{t+i}}[
(\frac{dL^{t+i}(v^{t+i},\psi_*)}{d\psi_*})^2  \\+ 2
\frac{dL^{t+i}(v^{t+i},\psi_*)}{d\psi_*} \frac{d^2L^{t+i}
(v^{t+i},\psi_*)}{d\psi_*^2}(\psi^{t+i}-\psi_*) +
\order{(\psi^{t+i}-\psi_*)^2}]\\
\Rightarrow \mathbb{E}^{v^{t+i}}[(\frac{dL^{t+i} (v^{t+i},
\psi^{t+i})} {d\psi^{t+i}})^2] = \Omega_1 +
\order{\mathbb{E}[(\psi_{t+i}-\psi_*]} + \order{\rho_t^2}\\
= \Omega_1 +
\order{\mathbb{E}[\order{\rho_t}]} + \order{\rho_t^2}
\end{align*}
\hfill $\blacksquare$
\subsection{Within block variance bound}
\label{append:intra-variance}
\begin{theorem}
The variance of the parameter $\psi$ at the end of a sub-epoch $S$ in block
$S_i$ which updated $n_i$ points as defined in equation~\ref{eqn:workerIpoints} (main paper)
is
\begin{align*}
Var(\raisePsi{t+n_i}) &= Var(\psi^t)- 2\eta_tn_i\Omega_0(Var(\psi^t))
-2\eta_tn_i\Omega_0CoVar(\psi_t,\bar{\delta_t}) + \eta_t^2n_i\Omega_1 \nonumber\\ 
&+ \underbrace{\order{\eta_t^2\rho_t} +
\order{\eta_t\rho_t^2} +
\order{\eta_t^3}+ \order{\eta_t^2\rho_t^2}}_{\Delta_t}
\end{align*}
Constants $\Omega_0$ and $\Omega_1$ are defined in
theorems~\ref{theo:optimaGradient} and theorems~\ref{theo:optimaGradientSquare}
respectively. 
\end{theorem}
\textbf{Proof.}
We start with analysing $\mathbb{E}^{V}[u(\psi^{(t+ni)})]$ term from
lemma~\ref{theo:expectNewPsi}
\begin{align}
&\mathbb{E}^{V}[u(\psi^{(t+ni)})] = \mathbb{E}^{V}[u(\psi^{t} +
(-\underbrace{\sum_{i=1}^{n_{i}} \! \eta_{t+i} \delta L^{t+i} (v^{t+i},
\psi^{t+i})}_{\nabla}))] \nonumber\\
& = \mathbb{E}^{V}[u(\psi^{t}) - \frac{du(\psi^{t})}{d\psi^{t}}\nabla +
\frac{1}{2}\frac{du^2(\psi^{t})}{d(\psi^{t})^2}\nabla^2 + \order{\eta_t^3}]
\nonumber\\
& = u(\psi^{t}) -
\eta_t\frac{du(\psi^{t})}{d\psi^{t}}\mathbb{E}^{V}[\sum_{i=1}^{n_{i}} \!
\delta L^{t+i} (v^{t+i}, \psi^{t+i})] %\nonumber \\
%& 
+ \eta_t^2\frac{1}{2}\frac{du^2(\psi^{t})}{d(\psi^{t})^2}
\mathbb{E}^{V}[(\sum_{i=1}^{n_{i}} \! \delta L^{t+i} (v^{t+i}, \psi^{t+i}))^2]
\nonumber \\ &\,+ \order{\eta_t^3} %\nonumber\\
%& 
~~~~~~~~~~\text{\bigg(since $\eta_t=\eta_{t+i}$ within a block and
expanding $\nabla$ \bigg) } \nonumber\\
&= u(\psi^{t}) -
\eta_t\frac{du(\psi^{t})}{d\psi^{t}} \sum_{i=1}^{n_{i}}
\frac{d\mathbb{E}^{V}[L^{t+i} (v^{t+i},
\psi^{t+i})]}{d\psi^{t+i}} %\nonumber\\
%& 
+ \eta_t^2\frac{1}{2}\frac{du^2(\psi^{t})}{d(\psi^{t})^2}
\mathbb{E}^{V}[(\sum_{i=1}^{n_{i}} \! \frac{dL^{t+i} (v^{t+i},
\psi^{t+i})}{d\psi^{t+i}})^2] \nonumber\\ 
&\,+ \order{\eta_t^3}\nonumber\\
& = u(\psi^{t}) - \eta_t\frac{du(\psi^{t})}{d\psi^{t}}
(\sum_{i=1}^{n_{i}}\frac{d\mathbb{E}^{v^{t+i}}[L^{t+i} (v^{t+i},
\psi^{t+i})]}{d\psi^{t+i}})
%\nonumber\\
%&
\nonumber\\
&\text{\bigg(using
Lemma~\ref{theo:independentPotentialExpectation}\bigg)}\nonumber\\
& + \eta_t^2\frac{1}{2}\frac{du^2(\psi^{t})}{d(\psi^{t})^2}
\bigg[\mathbb{E}^{V}[\sum_{i=1}^{n_{i}} (\frac{dL^{t+i} (v^{t+i},
\psi^{t+i})}{d\psi^{t+i}})^2] %\nonumber \\ 
%&
+ \mathbb{E}^{V}[\sum_{i\neq
j}\! \frac{dL^{t+i} (v^{t+i}, \psi^{t+i})}{d\psi^{t+i}}\frac{dL^{t+j} (v^{t+j},
\psi^{t+j})}{d\psi^{t+j}}]\bigg]\nonumber \\ 
&\, 
+\order{\eta_t^3} \nonumber \\
& = u(\psi^{t}) - \eta_t\frac{du(\psi^{t})}{d\psi^{t}}
(\sum_{i=1}^{n_{i}}\frac{d\mathbb{E}^{v^{t+i}}[L^{t+i} (v^{t+i},
\psi^{t+i})]}{d\psi^{t+i}})
%\nonumber\\
%& 
+ \eta_t^2\frac{1}{2}\frac{du^2(\psi^{t})}{d(\psi^{t})^2}
\bigg[\sum_{i=1}^{n_{i}}\mathbb{E}^{v^{t+i}}[(\frac{dL^{t+i} (v^{t+i},
\psi^{t+i})} {d\psi^{t+i}})^2] \nonumber \\
&+ (\sum_{i\neq j}\frac{d\mathbb{E}^{v^{t+i}}[L^{t+i}
(v^{t+i}, \psi^{t+i})]}{d\psi^{t+i}}\frac{d\mathbb{E}^{v^{t+j}}[L^{t+j} (v^{t+j},
\psi^{t+j})]}{d\psi^{t+j}})\bigg] +
\order{\eta_t^3} \nonumber \\
&\text{\bigg(using
Lemma~\ref{theo:independentPotentialCovariance}\bigg)}
\label{eqn:expectV}
\end{align}

% Last statement in equation~\ref{eqn:expectV} holds because two different data
% points picked at iteration $(t+i)$ and $(t+j)$ are independent of each other, by
% expectation property
% \begin{equation}
% \mathbb{E}^{V}[\frac{dL^{t+i} (v^{t+i},
% \psi^{t+i})}{d\psi^{t+i}}\frac{dL^{t+j} (v^{t+j}, \psi^{t+i})}
% {d\psi^{t+j}}] = \mathbb{E}^{v^{t+i}}[\frac{dL^{t+i} (v^{t+i},
% \psi^{t+i})]}{d\psi^{t+i}}]\mathbb{E}^{v^{t+j}}[\frac{dL^{t+j} (v^{t+j},
% \psi^{t+i})} {d\psi^{t+j}}]
% \label{eqn:covIndependent} 
% \end{equation}

From equation~\ref{eqn:expectV} and lemma~\ref{theo:expectNewPsi}
\begin{align}
\mathbb{E}^{\raisePsi{(t+n_i)}}[u(\raisePsi{(t+n_i)})] &=
\mathbb{E}^{\raisePsi{(t)}}\bigg[ u(\psi^{t}) - \eta_t\frac{du(\psi^{t})}{d\psi^{t}}
(\sum_{i=1}^{n_{i}}\frac{d\mathbb{E}^{v^{t+i}}[L^{t+i} (v^{t+i},
\psi^{t+i})]}{d\psi^{t+i}})
\nonumber\\
& 
+ \eta_t^2\frac{1}{2}\frac{du^2(\psi^{t})}{d(\psi^{t})^2}
\bigg[\sum_{i=1}^{n_{i}}\mathbb{E}^{v^{t+i}}[(\frac{dL^{t+i} (v^{t+i},
\psi^{t+i})} {d\psi^{t+i}})^2] \nonumber \\
&+ (\sum_{i\neq j}\frac{d\mathbb{E}^{v^{t+i}}[L^{t+i}
(v^{t+i}, \psi^{t+i})]}{d\psi^{t+i}}\frac{d\mathbb{E}^{v^{t+j}}[L^{t+j} (v^{t+j},
\psi^{t+j})]}{d\psi^{t+j}})\bigg]  \bigg] +
\order{\eta_t^3}
\end{align}


From equation above the variance of $\raisePsi{t+n_i}$ is
\begin{align}
&Var(\raisePsi{t+n_i}) =
\mathbb{E}^{\raisePsi{(t+n_i)}}[(\raisePsi{(t+n_i)})^2] - \left(\mathbb{E}^{\raisePsi{(t+n_i)}}[\raisePsi{(t+n_i)}]\right)^2
\nonumber\\ 
&= \mathbb{E}^{\psi^t}[(\psi^t)^2] - \eta_t n_i
\mathbb{E}^{\psi^t}[2\psi^t(\Omega_0(\psi^t-\psi_*+\bar{\delta_t}) +
\order{\rho_t^2})] \nonumber\\
&\text{\bigg(using theorem~\ref{theo:optimaGradient} and defining
$\bar{\delta_t}= \frac{\sum_{i=1}^{n_i} \delta_i}{n_i}$\bigg)} \nonumber \\
&+
\eta_t^2\frac{1}{2}\mathbb{E}^{\psi^t}[2\{n_i(\Omega_1+\order{\mathbb{E}[\rho_t]}+\order{\rho_t^2})
\nonumber\\ 
&+ \sum_{i\neq
j}(\Omega_0(\psi^{t+i}-\psi_*)+\order{\rho_t^2})
(\Omega_0(\psi^{t+j}-\psi_*)+\order{\rho_t^2})\}] \nonumber\\
&- \left(\mathbb{E}^{\psi^t}[\psi^t] - \eta_t n_i
\mathbb{E}^{\psi^t}[(\Omega_0(\psi^t-\psi_* + \bar{\delta_t}) +
\order{|\psi^t-\psi_*|^2})]\right)^2 \nonumber\\
& = \mathbb{E}^{\psi^t}[(\psi^t)^2] - 2\Omega_0\eta_t n_i
\mathbb{E}^{\psi^t}[(\psi^t)^2] +
2\Omega_0\eta_tn_i\psi_*\mathbb{E}^{\psi^t}[\psi^t] - 2\Omega_0\eta_t n_i
\mathbb{E}^{\psi^t}[\psi^t \bar{\delta_t}] -
\order{\eta_t\rho_t^2} \nonumber\\ 
&+ \eta_t^2n_i\Omega_1 +
\order{\eta_t^2\rho_t} + \order{\eta_t^2\rho_t^2} +
\order{\eta_t^2\rho_t^3} + \order{\eta_t^2\rho_t^4}\nonumber \\
&-\left(\mathbb{E}^{\psi^t}[\psi^t]\right)^2 +
2n_i\eta_t\mathbb{E}^{\psi^t}[\psi^t](\mathbb{E}^{\psi^t}[\Omega_0\psi^t]
-\Omega_0\psi_*+ \mathbb{E}^{\psi^t}[\Omega_0\bar{\delta_t}]
+\order{\rho_t^2}) - \order{\eta_t^2\rho_t^2} +\order{\eta_t^3} \nonumber \\
&=Var(\psi^t)- 2\eta_tn_i\Omega_0(Var(\psi^t))
-2\eta_tn_i\Omega_0CoVar(\psi_t,\bar{\delta_t}) + \eta_t^2n_i\Omega_1 \nonumber\\ 
&+ \underbrace{\order{\eta_t^2\rho_t} +
\order{\eta_t\rho_t^2} +
\order{\eta_t^3}+ \order{\eta_t^2\rho_t^2}}_{\Delta_t}
\label{eqn:intraVar}
\end{align}


\hfill $\blacksquare$




\end{document}
