For each reference specific points in the paper


Reviewer 1:

- Scaling in a number of cores:
	1) Explain on big datasets this is not an issue
	2) With Always-On, this is even less of an issue
- Compare against single processor:
	We focus on truly big data, but given we are using hadoop for small scale problems there is non-negligent overhead
- The IDEA is obvious:
	1) The idea is very simple, that is a strength not a weakness
	2) Contextualize the research: Lots of current work in distributed computing and trying to handle systems issues and bottlenecks.  Most recent work focuses on asynchronicity, parameter staleness, etc and often solves it by letting workers continue on NEW data and then combining parameters later.  This is the first work to offer the simple idea of using extra time to run on older data.
	3) Proofs are non-trivial and necessary: Queueing theory is relatively new in ML literature.  More importantly, proving the convergence and variance bounds is necessary to set step sizes and other tunable parameters appropriately for optimal convergence.  We see in the case of GraphLab that without rigorous proofs the system has difficulty scaling to big data.

Reviewer 2:

- SGD only?: The idea of course can be extended (to sampling for example), but the proof is focused on an SGD setting.  We believe because of the ideas simplicity it will be interesting to see if the technique works across other methods.
- [8] focuses on the partitioning scheme but is vulnerable to stragglers.  Our contribution focuses on enabling faster workers to do extra updates rather than waste time.
- Is this a systems paper?:
	1) We only describe the system so as to explain our experimental set up and because there were no open source alternatives for this problem
	2) The focus of the paper is on the idea to do extra repeated updates while waiting at barrier and the complex statistical proofs of our methods.  This attacks, through math, an important issue that real world distributed machine learning systems face.
	3) Experiments are included to motivate the issue and demonstrate our success.
		3a) GraphLab is just to show our system is fast overall.  However, we need to show that we beat DSGD by a significant margin and thus this single idea provides great benefits to the system.

Reviewer 3:

- First statements clarify
- Extra SGD steps beyond barrier shown in proof to be not beneficial after a while
X Simple / how different from gemulla
- Why does DSGD do so bad on MMND
X Number of cores issue (compared to graphlab)
- Compare against Hogwild?: Explain partitions model only, not data.  Also multithreaded and not across clusters.  May still be worth running to compare.
- Class of ML problems addressed




Experiments to run:
	1) Number of cores on Huge Data with LDA bug fixed - we want to see DSGD be concave and us asymptote
    2) DSGD vs us
	3) Explain huge gains on MMND
	4) Hogwild
