\documentclass[twoside]{article}
\usepackage{aistats2014}

\input{dfn.tex}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{subfigure,grffile,placeins}
\usepackage{graphicx} 
\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage[lined,boxed]{algorithm2e}
\usepackage{url}
\usepackage{color}

\newtheorem{observation}{Observation}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{algo}{Algorithm}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
%\newtheorem{question}{Question}
\newtheorem{example}{Example}
\newcommand{\pushright}[1]{\ifmeasuring@#1\else\omit\hfill$\displaystyle#1$\fi\ignorespaces}
\newcommand{\pushleft}[1]{\ifmeasuring@#1\else\omit$\displaystyle#1$\hfill\fi\ignorespaces}
%\newtheorem{answer}{Answer}
%\newtheorem{proof}{Proof}

\newcommand{\abhi}[1]{\textcolor{orange}{abhi-comment: #1}}
\newcommand{\alex}[1]{\textcolor{red}{alex-comment: #1}}
\newcommand{\qirong}[1]{\textcolor{magenta}{qirong-comment: #1}}
\newcommand{\eric}[1]{\textcolor{blue}{eric-comment: #1}}

% If your paper is accepted, change the options for the package
% aistats2014 as follows:
%
%\usepackage[accepted]{aistats2014}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Slow-Worker-Agnostic Distributed Learning for Big Models on Big Data}

%\aistatsauthor{Abhimanu Kumar \And Alex Beutel 2 \And Qirong Ho \And Eric P. Xing}
\aistatsauthor{Anonymous Authors}
\aistatsaddress{ Unknown Institution} ]

\begin{abstract}
We present a scheme for fast, distributed learning on
big (i.e. high-dimensional) models applied to big datasets.
Unlike algorithms that focus on distributed learning in either the big data or big model setting
(but not both), our scheme partitions both the data and model variables
simultaneously. This not only leads to faster learning on distributed clusters,
but also enabes machine learning applications where both data
and model are too large to fit within the memory of a single machine. Furthermore, our scheme
allows worker machines to perform additional updates while waiting for slow workers to finish,
which provides users with a tunable synchronization strategy that can
be set based on learning needs and cluster conditions.
We prove the correctness of such strategies, as well as provide
bounds on the variance of the model variables under our scheme.
Finally, we present empirical results for latent space models such
as topic models, which demonstrate that our method
scales well with large data and model sizes, while beating
learning strategies that fail to take model partitioning into account.
\end{abstract}

\vspace{-0.3cm}
\section{Introduction}
\vspace{-0.2cm}

Machine Learning applications continue to grow rapidly, in terms of both input
data size (big data) as well as model complexity (big models). The big data challenge
is already well-known --- with some estimates putting the amount of data generated on the internet
at 5 exabytes every two days~\footnote{\url{http://techcrunch.com/2010/08/04/schmidt-data/}} ---
and much effort has been devoted towards learning models on big datasets, particularly through
stochastic optimization techniques that randomly partition the data over different machines. Such techniques
have been the subject of much theoretical
scrutiny~\cite{langford2009slow,zinkevich2010parallelized,agarwal2012distributed,hoffman2012stochastic}.
On the other hand, the big model issue is about learning models with an extremely large number of variables
and/or parameters --- such as the Google Brain deep network with over 1B parameters~\cite{dean2012large} ---
and recent papers on this subject have focused on intelligent partitioning of the model variables in
order to minimize network synchronization costs~\cite{low2012distributed,dean2012large}, albeit not always
with theoretical backing.

Although big data and big models are both crucial research foci, there are few
distributed machine learning papers that explicitly consider both aspects in conjunction, with a notable example
being the partitioned matrix factorization algorithm of Gemulla {\it et al.}~\cite{Gemulla:2011:LMF:2020408.2020426}. In the big data,
big model setting, the model variables may not all fit into a single machine's memory, which in turn imposes
additional constraints on how the data is partitioned. Moroever, careless partitioning of model variables
across distributed machines imposes significant network synchronization costs~\cite{low2012distributed}, which
are required whenever dependent variables or datapoints are placed on separate machines.
If we are to effectively partition both data and model, it follows that we must carefully
examine and exploit the interdependencies between data and model variables.

In this paper, we address learning big latent space models on big data over a distributed cluster.
We develop and theoretically analyze a stochastic optimization algorithm for learning latent space models
expressed in matrix form, such as topic modeling or dictionary learning. Our algorithm exploits the
model structure to partition the data and model variables over a distributed cluster, in a manner
that automatically balances inter-machine network synchronization costs with performing useful computational work,
even when the worker machines are not equally capable (e.g. because of different hardware or other
concurrently running programs), or the data cannot evenly partitioned (e.g. due to the structure of the model).

As a result, our algorithm solves the ``last reducer" or ``straggler" issue distributed systems, in which some worker machines
can be much slower than others (because of cluster heterogeneity, or because other jobs are running on the same
machine), causing faster workers to waste computational cycles waiting for them.
Instead, we ensure that faster workers will continue to perform
useful work until the last reducer finishes, and our theoretical and experimental analysis confirms that this
strategy leads to faster algorithm convergence. Furthermore, our theory shows that careful control of inter-worker
synchronization can lead to even faster convergence, which opens the door to intelligent exploitation
of distributed computation systems with fine-grained synchronization schemes, such
as parameter servers~\cite{ho2013scalable,cipar2013solving,ahmed2012scalable,power2010piccolo}.

\vspace{-0.3cm}
\section{Related Work}
\vspace{-0.2cm}

Most existing literature is focused on learning under either big data or big model conditions, but
rarely both together. Of the papers that focus on big data, most of them exploit data point
indepedence to construct stochastic distributed optimization schemes with little need
for inter-machine synchronization. For example, the PSGD algorithm~\cite{zinkevich2010parallelized}
is completely data parallel and requires absolutely no inter-machine communication,
therefore making it trivial to implement. However, in practice,
one can almost always obtain faster convergence with some inter-machine communication, as our experiments will show.
Another important class of methods are the fixed-delay algorithms~\cite{agarwal2012distributed,langford2009slow} in which machines communicate
with a central server (or each other) in a fixed ordering. This fixed ordering is a serious practical limitation,
because all machines will be held up by slowdowns or failures in just one machine.
In contrast, our algorithm ensures that all machines continue to do useful work even under such conditions.
Most importantly, unlike these big data algorithms, our algorithm can partition model variables
(and not just datapoints) across machines, which is absolutely critical for massive models that cannot fit onto a single machine.

For learning on big models, the general strategy is to exploit the fact that each model variable usually
depends on only a small number of other variables, and then partition model variables
in a way that limits the number of dependencies that cross machines. The GraphLab system~\cite{low2012distributed}
is a good example of this concept, but it requires machine learning algorithms to be rewritten into
``vertex programs", which can be awkward or even difficult for some algorithms. Furthermore,
there has been little theoretical analysis on machine learning algorithms running under GraphLab.
The Google Brain project~\cite{dean2012large} provides another example of the need for model partitioning,
this time for a custom-built deep network meant for image and video feature extraction. However, the
paper does not provide general partitioning strategies for arbitrary models. Finally, we note that
the Hogwild paper~\cite{niu2011hogwild} provides a theoretical analysis of certain issues related to model partitioning ---
specifically, the effect of errors produced when two worker threads update the same variable simultaneously.
Aside from that, the paper does not provide or analyze model partitioning strategies, while their
experiments only cover the shared-memory, single-machine setting.

Finally, there are papers that tackle big data and big model issues together, such as the partitioned matrix factorization
algorithm of Gemulla {\it et al.}~\cite{Gemulla:2011:LMF:2020408.2020426}, to which our work is most closely related. Unlike Gemulla {\it et al.},
our algorithm allows worker machines to perform useful variable updates continuously, without blocking or waiting
for any other machine to complete its assigned task. This property is exceptionally beneficial
on very large clusters, where machines often fail or slow down for any number of reasons. Thus, our algorithm
is not bottlenecked by the slowest machine, unlike~\cite{Gemulla:2011:LMF:2020408.2020426}. Furthermore, we provide substantially
richer theoretical analysis, including variance bounds and analysis of the effect of non-blocking workers.
In particular, work by Murata~\cite{Murata98astatistical} lays the foundation for
variance analysis of SGD algorithms, by providing variance bounds over datapoint selection.

\vspace{-0.3cm}
\section{Slow-Worker Agnostic Learning for Big Models on Big Data}
\vspace{-0.2cm}

Our approach to learning big models on big data relies on
exploiting independent blocks of data, variables and parameters.
For example, a probabilistic graphical model can contain massive
numbers of latent variables and parameters,
so as to capture the modeler's generative assumptions about large datasets.
In order to tackle problems of such scale, we need to exploit independence structures
present in the data and model, so as to partition both over a distributed cluster.

Before we describe our partitioning strategy, we shall present examples of latent space models in ML.
Consider {\bf Topic Modeling}~\cite{blei2009topic}: given a \emph{ document by vocabulary} data matrix $Y$ (with the
rows normalized to sum to 1),
we want to decompose it into two matrices: \emph{ documents by topics} $\pi$ (which are model variables) and
\emph{ topics by vocabulary} $\beta$ (which are parameters). We formulate this task
as an optimization problem with simplex and non-negativity constraints:
{\small
\begin{align}
&\argmin_{\pi,\beta}L(Y,\pi,\beta) =||Y-\pi\beta||_p^p
%= \sum_{i,j}(Y_{i,j}-\sum_k \pi_{i,k}\beta_{k,j})_p^p
\label{eqn:LDA}\\
&\text{s.t.} \; \forall  i,j,k \quad
\sum_k\pi_{i,k}=1,
\sum_j\beta_{k,j}=1, \quad
\pi_{i,k}\geq 0,
\beta_{k,j}\geq 0,
\nonumber
\end{align}}
where $\Vert \cdot \Vert^P_P$ is an $\ell_p$ norm, typically $\ell_2$. We note
that other matrix-decomposition-based algorithms for topic modeling also exist, such
as the spectral decomposition method of Anima {\it et al.}~\cite{anandkumar2012two}.
%In our experiments section, we will show that our learning algorithm outperforms recent state-of-the-art baselines on the Topic Modeling problem.
Another example is {\bf Dictionary Learning}~\cite{Kreutz-Delgado:2003:DLA},
in which the goal is to decompose a signal matrix $Y$ into a dictionary $D$
and a sparse reconstruction $\alpha$:
{\small
\begin{align}
&\argmin_{\alpha,D} L(Y,\alpha,D) =\frac{1}{2}||Y-D\alpha||_2^2 + \lambda||\alpha||_1
\label{eqn:dictionary} \\
&\text{s.t.} \; \forall j, D_j^TD_j \leq 1
\nonumber
\end{align}}
A third example is Multi-Role or {\bf Mixed-Membership Network Decomposition}, where
an $N \times N$ adjacency matrix $Y$ is decomposed into an $N\times K$ matrix $\theta$,
whose $i$-th row is the normalized role-vector for node $i$,
and a $K \times K$ role matrix $B$. Together, $\theta,B$ characterize the behavior of every node in the network,
and the optimization problem is:
{\small
\begin{align}
&\argmin_{\theta,B} L(Y,\theta,B) =\frac{1}{2}||Y-\theta B \theta^\top||_2^2
\label{eqn:mm_network} \\
&\text{s.t.} \; \forall i, \quad \sum_j \theta_{ij} = 1, \quad \theta_{i,j}\ge 0
\nonumber
\end{align}}
The main difference between such latent space models and matrix factorization problems (unconstrained or non-negative)
is that latent space models usually have more constraints: simplex constraints in the case of topic modeling
and mixed-membership network decomposition, and
bounded inner product in the case of dictionary learning. To handle these,
our distributed learning algorithm supports projection steps to ensure the final solution always
satisfies the constraints.
We note that while many recent distributed algorithms have theoretical guarantees
under projection~\cite{langford2009slow,agarwal2012distributed,Gemulla:2011:LMF:2020408.2020426},
some algorithms such as PSGD~\cite{zinkevich2010parallelized} do not explicitly handle projections,
and are thus unsuitable for the latent space models defined above.

\vspace{-0.3cm}
\subsection{Partitioning Strategy and Algorithm}
\vspace{-0.2cm}

\begin{figure*}[t]
\vspace{-0.3cm}
\centering
\includegraphics[width=\textwidth]{fig/partition.pdf}
\vspace{-0.5cm}
\caption{\small Partitioning strategy for data $Y$, model variables $\pi$, and parameters $\beta$.
We show one epoch broken into multiple sub-epochs (3 in this case).
Each sub-epoch is further divided into (colored) blocks, such that the data $Y_{i,j}$ (with its
associated variables $\pi_{i,\cdot}$ and parameters $\beta_{\cdot,j}$) from one block do not
share rows/columns with data $Y_{a,b}$ from another block. Taken together, all blocks from
all sub-epochs cover every element of $Y,\pi,\beta$.
}
\vspace{-0.3cm}
\label{fig:para-div}
\end{figure*}

\paragraph{High-level overview.}
In order to learn latent space models effectively on a distributed cluster,
we need to exploit the interdependence of parameters and variables.
As a running example, consider the topic modeling objective $L(Y,\pi,\beta)$:
we can divide the data matrix $Y$ into a sequence of sub-epochs, where
each epoch consists of blocks that do not overlap on parameters $\beta$ and variables $\pi$,
and where the union over all epochs covers the entire matrix (Figure~\ref{fig:para-div}).
This blocking strategy is attributed to Gemulla {\it et al.}~\cite{Gemulla:2011:LMF:2020408.2020426}, and it
permits multiple machines to perform stochastic gradient descent (SGD) on different blocks in parallel,
on a Hadoop cluster. However, it requires all workers to process a roughly equal number of datapoints
per block, which leads to problems with slow workers. Our algorithm and theoretical analysis removes this limitation,
allowing faster workers to process extra datapoints in their assigned block, which maximizes
the cluster's computational efficiency.

At a high level, our algorithm proceeds one epoch at a time,
performing SGD on all blocks within an epoch in parallel.
In order to satisfy the problem constraints, we must interleave projection steps with
the SGD algorithm. In this respect, the parameters $\beta$ and variables $\pi$ 
must be handled differently: while the simplex projection for variables $\pi$ can
be performed by each worker independently of others,
the simplex projection for the parameters $\beta$ requires workers to repeatedly synchronize with each other.
This cannot be done through the MapReduce programming model, so our Hadoop implementation allows workers
to write to the Hadoop Distributed File System (HDFS) in order to communicate projection information with each other.
We find that this scheme works well in practice, while dedicated, memory-based synchronization systems such as
parameter servers~\cite{cipar2013solving,ahmed2012scalable,power2010piccolo} have the potential to perform even better.

\vspace{-0.2cm}
\paragraph{Partitioning strategy.}
More formally, let $\bold{\Psi}$ collectively refer to the variables and parameters
$\mathbf{\pi, \beta}$, and let $\psi$ refer to individual elements of $\bold{\Psi}$.
These definitions will make the subsequent analysis easier to understand.
Thus, we rewrite the topic modeling objective $L$ as: 
\begin{align}
\psi^{(t+1)}&= \psi^{(t)} - \eta_t \nabla
\loss_{Y_{i,j}}(\psi^{(t)}),
\label{equ:sgd-update-lda}
\end{align}
and we apply parameter/variable projections each time we execute Eq.~(\ref{equ:sgd-update-lda}).
Assuming that we use the $\ell_2$ norm, the differential of $\psi$ with respect
to $\pi$ at a single data point $Y_{i,j}$ is
{\scriptsize
\begin{align}
	(\nabla L_{Y_{i,j}}(\psi))_\sigma &= 
	\left\{
	\begin{array}{ll}
		-2(Y_{i,j}-\sum_k \pi_{i,k}\beta_{k,j}
		)\beta_{\ell,j}  & \mbox{if } \sigma = \pi_{i,\ell} \\
		0 & \mbox{if } \sigma = \pi_{i',\ell},\ i\neq i'\\
	\end{array}
	\label{eqn:diff-lda}
\right.
\end{align}}
where $\sigma$ is the element of $\pi$ being differentiated, and $(\nabla L_{Y_{i,j}}(\psi))_\sigma = \frac{\partial L_{Y_{i,j}}}{\partial \sigma}$.
The differentials with respect to $\sigma = \beta_{j,\ell}$
are similar.
From these equations, we observe that the SGD update for $\pi_{i,\ell}$ at a particular datapoint
$Y_{i,j}$ depends only on a small subset of the variables and parameters: specifically,
$\pi_{i,k}, \beta_{k,j}$ where $k\in {1,\ldots,K}$ and $K$ is the number of topics we chose.
Notice that the $\pi$ all come from the same row $i$ as $Y_{i,j}$, while the $\beta$ all come
from the same column $j$.
Furthermore, the SGD updates for $\pi_{i,\ell}$ are zero for any datapoint $Y_{a,b}$ where $a \ne i$.
A similar observation holds for the parameters: the SGD update for $\beta_{r,j}$ is zero for
any datapoint $Y_{a,b}$ where $b\ne j$.

These observations lead to the following key insight: we can perform SGD on two datapoints $Y_{i,j}$ and $Y_{a,b}$ at the same
time, provided $i \ne a$ and $j \ne b$ --- in other words, as long as the datapoints do not
overlap on their rows or columns. An intuitive proof goes like this: SGD updates on $Y_{i,j}$ only touch the variable row $\pi_{i,\cdot}$
and the parameter column $\beta_{\cdot,j}$, and both of them do not overlap with $Y_{a,b}$'s variable row $\pi_{a,\cdot}$ and
parameter column $\beta_{\cdot,b}$. In other words, the SGD updates on $Y_{i,j}$ and $Y_{a,b}$
touch disjoint sets of variables $\pi$ and parameters $\beta$.
Furthermore, each $\pi_{i,\cdot}$ in row $i$ only ever depends on other $\pi$ in the same row $i$,
and similarly for $\beta_{\cdot,j}$ and other $\beta$ in column $j$. We therefore conclude that
all quantities associated with row $i$ and column $j$, namely $Y_{ij},\pi_{i\cdot},\beta_{\cdot j}$, are completely
independent of the quantities from row $a$ and column $b$, namely $Y_{ab},\pi_{a\cdot},\beta_{\cdot b}$.
Hence, datapoints with disjoint rows and columns can be simultaneously used for SGD updates~\cite{Gemulla:2011:LMF:2020408.2020426}.

From the perspective of data, model and parameter partitioning, we have essentially partitioned datapoints
$Y$, model variables $\pi$ and parameters $\beta$ into independent collections $(i,j)$ where
$i$ is a row index and $j$ is a column index. In other words, the collection $(i,j)$ contains $Y_{ij},\pi_{i\cdot},\beta_{\cdot j}$,
and can be ``processed" (meaning that we run SGD on $Y_{ij}$ to update $\pi_{i\cdot}$ and $\beta_{\cdot j}$)
in parallel with any other collection $(a,b)$ where $a\ne i$, $b \ne j$.

We note that the above scheme applies to Dictionary Learning with only slight modification. For Mixed-Membership
Network Decomposition, the presence of the symmetric term $\theta B\theta^T$ presents additional challenges.
Instead, we replace $\theta B\theta^\top$ with $\theta C$ where $C := B\theta^\top$, and recover $B$ post-optimization
via pseudoinversion: $B = C\theta(\theta^\top \theta)^{-1}$. The inversion cost is reasonable since $\theta^\top \theta$ is $K\times K$,
while $K$ is rarely $\ge 1000$ in practice.

\vspace{-0.2cm}
\paragraph{Distributed algorithm.}
Since collections $(i,j)$ with disjoint rows/columns can be processed
simultaneously, let us consider grouping
them into multiple blocks $S_b \subseteq Y$, such that the blocks have disjoint rows/columns (Figure~\ref{fig:para-div}).
While we cannot process collections $(i,j)$
within the same block in parallel (because they might share rows/columns), we can process collections from
{\it different} blocks in parallel, as they are guaranteed to be non-overlapping. Thus, if we managed to
construct $P$ non-overlapping blocks, we can spawn $P$ workers to perform SGD in parallel. Although it
is impossible to find a set of non-overlapping blocks that covers all of $Y$, we can find multiple disjoint sets of
non-overlapping blocks that, taken together, cover $Y$ completely (Figure~\ref{fig:para-div}). We call
these sets {\it sub-epochs}, which are processed sequentially (while the blocks within a sub-epoch
are processed in parallel). An {\it epoch} is a sequence of sub-epochs that fully covers $Y$.

Our algorithm differs from Gemulla~{\it et al.} in that
within a sub-epoch, we allow different worker-blocks to perform varying numbers of SGD updates per collection $(i,j)$
(whereas Gemulla~{\it et al.} require equal numbers of updates per worker).
This makes our algorithm much more efficient whenever there are slow
worker machines (which are common in large clusters),
since faster workers can keep running until synchronization, rather than
wasting computational time waiting for slower workers to catch up. The full algorithm is shown in Algorithm \ref{algo:lda},
and we shall prove that it converges, along with several other important properties.

We note that while the row/column-wise partitioning strategy for data/variables/parameters
works well for the examples we have presented, it does not apply to all possible ML models:
for example, graphical models and deep networks can have arbitrary structure between parameters and variables,
while problems on time-series data will have sequential or autoregressive dependencies between datapoints.
In such cases, a row/column-wise partitioning will not work. Nevertheless, the {\it idea} and basic theoretical analysis of
grouping data, variables and parameters into independent collections still holds; only the partitioning
strategy needs to be changed. This opens up rich possibilities for future work on general-purpose partitioning algorithms
under our framework.

\IncMargin{1em}
\begin{algorithm}[t]
\small
\SetAlgoLined
Input : $Y,\beta, \pi$,~sub-epoch~size~$d$\\
$\pi\leftarrow \pi_0$, $\beta\leftarrow \beta_0$\\
Block $Y,\pi,\beta$ into corresponding $w$ blocks\\
\While{not converged}{
Pick step size $\eta_S$\\
		Pick $w$ blocks($S_{1},...,S_{w}$) to form sub-epoch $S$\\
		\For{$b=0,\ldots,w-1$ \textbf{in parallel}}{
			Run SGD on the training points $Y_{ij}\in S_{b}$\\
			// (until every block is ready to synchronize)\\
			// Each worker-block $S_{b}$ can touch datapoints\\
			// multiple times if other blocks are slow.\\
			Apply appropriate projections\\
			// (e.g. on variables $\pi$ in topic modeling)\\
		}
		Apply appropriate projections\\
		// (e.g. on parameters $\beta$ in topic modeling)\\
}
\caption{\small Our slow-worker agnostic learning algorithm, as applied to topic modeling.}
\label{algo:lda}
\end{algorithm}

\vspace{-0.3cm}
\section{Theoretical Analysis}
\vspace{-0.2cm}

We analyse algorithm~\ref{algo:lda}, and prove
that our strategy of allowing multiple SGD iterations per data/variable/parameter
collection $(i,j) = \{Y_{ij},\pi_{i\cdot},\beta_{\cdot j}\}$
leads to faster convergence. Furthermore, we bound the variance in the final solution
caused by two aspects of our partitioning strategy: (1) the variance induced by running
two blocks within a sub-epoch in parallel, and (2) the variance due to splitting
the data matrix into a sequence of sub-epochs. These variance bounds
distinguish our analysis from Gemulla {\it et al.}~\cite{Gemulla:2011:LMF:2020408.2020426}, who did not provide
such bounds for their blocking strategy.
Our ultimate goal is to show that allowing additional iterations on fast workers
is better than simply waiting for the slowest worker.

Assume we have $w$ worker processors
(algorithm~\ref{algo:lda}), and that in each sub-epoch, every processor
is assigned to a distinct block $i$ --- henceforth, we shall use the index $i$ to
refer interchangeably to processors or blocks. We now define the following terms:
\vspace{-0.2cm}
\begin{definition}
\end{definition}
\begin{itemize}
    \setlength{\itemsep}{0.6pt}
    \setlength{\parskip}{1pt}
  \item $n_i$, $\kappa_i$ and $N_w$: Let $n_i$ be the number of datapoints
that worker $i$ touches (with repetition) in its assigned block, before
transitioning to the next sub-epoch. In other words, if worker $i$ was assigned $n$
datapoints, and touches each point $\kappa_i \geq 1$ times on average, then
\begin{align}
n_i = \kappa_i n \qquad \text{and} \qquad N_w = \sum_{i=1}^w n_i 
\label{eqn:workerIpoints}
\end{align}
  \item $\eta_t$: SGD step size at iteration $t$. An iteration is defined as one
SGD update on one datapoint.
  \item $ \nabla\mathcal{L}(\itert{\psi})$: Exact gradient at iteration
$t$.
  \item $\delta \itert{L}(\itert{V},\itert{\psi})$: Stochastic gradient at
  iteration $t$, i.e. $\nabla \loss_{Y_{i,j}}(\psi^{(t)})$ for some $i,j$.
  \item  $\varepsilon_t$: Error due to stochastic update at iteration $t$, $\left[
\nabla\mathcal{L}(\itert{\psi}) - \delta
\itert{L}(\itert{V},\itert{\psi})\right]$.
  \item $\atiter{\psi}{t}$ : Model state $\psi$ (see
  Eq.~\ref{equ:sgd-update-lda}) at iteration $t$.
\end{itemize}
We now introduce
$\atiter{V}{t}(\atiter{\psi}{t+1},\atiter{\psi}{t})$, a state potential
function defined over a previous state $\atiter{\psi}{t}$, a future state
$\atiter{\psi}{t+1}$, and the data points $\atiter{y}{t}$ picked at
iteration $t$.

\vspace{-0.2cm}
\begin{definition}
\textbf{State Potential Function ($V$)}:  $V^{(t)}$ encodes the probability
that $\atiter{\psi}{t}$ will be updated to $\atiter{\psi}{t+1}$ when the algorithm
performs the stochastic update over datapoint $y^{(t)}$. We also define an $n_i$-dimensional
state potential $V = \left(\atiter{V}{t+1}, \atiter{V}{t+2}\ldots \atiter{V}{t+n_i} \right)$,
which encodes the probability distribution of updates caused by all $n_i$ iterations in block $i$ of
a sub-epoch (assuming that block $i$ starts at iteration $t+1$).
\end{definition}

Next, we make assumptions on the error terms $\varepsilon_t$ and step sizes $\eta_i$:
\vspace{-0.2cm}
\begin{assumption}
\end{assumption}
\begin{itemize}
    \setlength{\itemsep}{0.6pt}
    \setlength{\parskip}{1pt}
  \item \textbf{Martingale difference error $\varepsilon_t$}:
  The error terms $\varepsilon_t$ form a martingale difference sequence. 
  \item \textbf{Variance bound on errors $\varepsilon_t$}:
  For all $t$, we have $E[\varepsilon_t^2]<D$.
  \item \textbf{Step size $\eta_t$ assumption}: $\sum\eta_t^2 <\infty$.
\end{itemize}
We note that assuming error terms are a martingale
difference sequence is weaker (easier to satisfy) than assuming error terms
$\varepsilon_t$ are independent of each other. The martingale difference
assumption means that the stochastic gradient $\delta \itert{L}(\itert{V},\itert{\psi})$,
conditioned on the initial model state $\psi^{(0)}$ and previous gradients $\delta
\iiter{L}(\iiter{V},\iiter{\psi})$ for all $i<t$, depends only on the current model
stae $\itert{\psi}$. This is because our blocking strategy
ensures that parallel parameter updates (from different blocks)
never overlap on the same elements of $\psi$.

\vspace{-0.2cm}
\subsection{Convergence of our algorithm}
\vspace{-0.1cm}
First, using the definition of $V^t$, we obtain the relation
{\small
\begin{equation}
p(\atiter{\psi}{t+1}|\atiter{\psi}{t}) d\atiter{\psi}{t} =
p(V^{(t)}(\atiter{\psi}{t+1},\atiter{\psi}{t}))
dV^{(t)}(\atiter{\psi}{t+1},\atiter{\psi}{t}).
\label{eqn:potentialDef}
\end{equation}}
We can interpret this equation as follows:
fix an particular update event
$\atiter{\psi}{t} \rightarrow \atiter{\psi}{t+1}$, then
$V^{(t)}(\atiter{\psi}{t+1},\atiter{\psi}{t})$
represents the event that some datapoint $\atiter{y}{t}$ gets chosen,
while $dV^{(t)}(\atiter{\psi}{t+1},\atiter{\psi}{t})$ is the
probability that said choice leads to the update event $\atiter{\psi}{t} \rightarrow \atiter{\psi}{t+1}$.
The intuition here is that $\itert{V}=\itert{V}(\itertO{\psi},\itert{\psi})$ is a 
function that keeps track of the state of $\itert{\psi}$ and $\itertO{\psi}$,
and that depends on the datapoint $\itert{Y_{i,j}}$ chosen by the SGD update.

Next, from the definition of a martingale difference sequence:
\begin{eqnarray}
E\left[\nabla\mathcal{L}(\itert{\psi}) - \delta
\itert{L}(\itert{V},\itert{\psi}) \;\vert\; \right. \qquad &&\nonumber \\
\left. \delta
\iiter{L}(\iiter{V},\iiter{\psi}),\iiter{\psi},i<t,\itert{\psi}\right] &=&
0, \nonumber \\
E\left[\varepsilon_t|\varepsilon_i,i<t\right] &=& 0.
\label{eqn:martingaleAssumption}
\end{eqnarray}
In other words, the error term $\varepsilon_t$ has zero expectation when
conditioned on previous errors.
We now have the necessary tools to provide a convergence guarantee for our
slow-worker agnostic algorithm:
\vspace{-0.2cm}
\begin{theorem}
	The stochastic updates 
	$\psi^{(t+1)}= \psi^{(t)} - \eta_t \nabla \loss_{Y_{i,j}}(\psi^{(t)})$ as
	described in algorithm~\ref{algo:lda} and the exact updates
	$\psi^{(t+1)}=\itert{\psi} - \eta_t\nabla \mathcal{L}(\itert{\psi})$ (for
	an exact gradient descent) converge to the same set of limit points
	asymptotically, given that the error terms $\varepsilon_t$ are a martingale
	difference sequence, and $E[\varepsilon_i^2]<D$ (bounded variance), and
	$\sum\eta_i^2 <\infty$.
	\label{theo:asymptotConverge}
\end{theorem}
We defer the proof to the appendix.
The above theorem says that, asymptotically, the error terms cancel each other out,
and therefore our stochastic algorithm will find the same set of optima as an
exact gradient aglrotihm.

\vspace{-0.2cm}
\subsection{Variance of $\psi$ within a sub-epoch}
\vspace{-0.1cm}
We now bound the variance of the model state $\psi$, when
it is updated inside block $i$ in a sub-epoch.
\vspace{-0.2cm}
\begin{assumption}
Assume for simplicity that the parameter being updated in block $i$ is univariate. 
The analysis can be easily extended to multivariate updates.
\end{assumption}
\vspace{-0.2cm}
\begin{theorem}
\label{thm:psi_variance_within_subepoch}
Within block $i$, suppose we update the model state $\psi$ using $n_i$ datapoints (Eq.~\ref{eqn:workerIpoints}).
Then the variance of $\psi$ after those $n_i$ updates is
\begin{align*}
Var(\raisePsi{t+n_i}) =& Var(\psi^t)- 2\eta_tn_i\Omega_0(Var(\psi^t)) \nonumber\\
&-2\eta_tn_i\Omega_0CoVar(\psi_t,\bar{\delta_t}) + \eta_t^2n_i\Omega_1 \nonumber\\ 
&+ \underbrace{\order{\eta_t^2\rho_t} +
\order{\eta_t\rho_t^2} +
\order{\eta_t^3}+ \order{\eta_t^2\rho_t^2}}_{\Delta_t}
\end{align*}
Constants $\Omega_0$ and $\Omega_1$ are defined in
Theorems 1 and 2 in 
the Appendix.
\end{theorem}
The proof is left to the Appendix.
Note that the constants $\Omega_0$ and $\Omega_1$ are independent of the datapoints picked or
the iteration number; they only depend on the global optima of the problem.
The above theorem consists of 4 important terms on the first line, plus another 4 cubic (or higher order)
terms $\Delta_t$ that quickly converge to zero and can be ignored. The basic intuition is as follows:
when the algorithm is far from an optimum, $Var(\atiter{\psi}{t})$ and $CoVar(\atiter{\psi}{t},\bar{\delta}_t)$
are large, so the 2nd and 3rd terms dominate and the variance of $\psi$ decreases quickly. However, when
close to an optimum, the constant 4th term with $\Omega_1$ dominates, causing the algorithm
to oscillate unless the step size $\eta^2_t$ is small --- which we have ensured via the shrinking step size assumption $\sum \eta^2_t < \infty$.

\vspace{-0.2cm}
\subsection{Variance of $\psi$ between sub-epochs}
\vspace{-0.1cm}
Let us consider the variance of $\psi$ across entire sub-epochs. First, note that within
a sub-epoch, two blocks $Z_i$ and $Z_{i^\prime}$ are independent if for
each datapoint $y \in Z_i$ and $y^{'} \in Z_{i^\prime}$, the following holds:
\begin{align}
\nabla L_{y}(\psi) = \nabla L_{y}(\psi -
\eta \nabla L_{y_{'}}(\psi)) \nonumber\\ 
{\rm and}\quad \nabla L_{y_{'}}(\psi) =
\nabla L_{y_{'}}(\psi - \eta \nabla L_{y}(\psi))
\label{eqn:blockIndependence}
\end{align}
In other words, even when the model state $\psi$ is perturbed by the stochastic gradient
on $y^\prime$, the stochastic gradient on $y$ must not change (and vice versa).
By our earlier argument on collections $(i,j) = \{ Y_{ij},\pi_{i\cdot},\beta_{\cdot j} \}$,
this condition holds true for any pair of points from distinct blocks.
Thus, within a sub-epoch, distinct blocks $Z_i$ and $Z_{i^\prime}$ operate
on disjoint subsets of $\Psi$, hence their update equations are independent of each other.
At the end of a sub-epoch $S_{n+1}$, the algorithm synchronizes the model state
$\Psi_{S_{n+1}}$ by aggregating the non-overlapping updates
$\delta\psi^i_{S_{n+1}}$ from all blocks $S^i_{n+1}$. Therefore, we can write the variance
$Var(\Psi_{S_{n+1}})$ at the end of sub-epoch $S_{n+1}$ as
{\scriptsize
\begin{align}
&Var(\Psi_{S_{n+1}}) = \sum_{i=1}^{w}Var(\psi^i_{S_{n+1}}) \label{eqn:interVar} \\
=& \sum_{i=1}^{w}\bigg[Var(\psi^i_{S_{n}})  -
2\eta_{S_n}n_i\Omega_0^i(Var(\psi^i_{S_{n}})) \nonumber\\
&-2\eta_{S_n}n_i\Omega_0^i(CoVar(\psi^i_{S_{n}},\bar{\delta}_{S_n}^i))
+ \eta_{S_n}^2n_i\Omega_1^i + \Delta_{S^i_{n}}\bigg] \nonumber \\
=& Var(\Psi_{S_{n}}) -
2\eta_{S_n}\sum_{i=1}^{w}n_i\Omega_0^iVar(\psi^i_{S_{n}}) \nonumber \\
&-2\eta_{S_n}\sum_{i=1}^{w}n_i\Omega_0^iCoVar(\psi^i_{S_{n}}
,\bar{\delta}_{S_n}^i)
+ \eta_{S_n}^2\sum_{i=1}^{w}n_i\Omega_1^i + \order{\Delta_{S_{n}}},
\nonumber
\end{align}}
where the 2nd line is proven in the Appendix.
This equation carries the same interpretation as
Theorem \ref{thm:psi_variance_within_subepoch}: when far from an optimum,
the negative terms dominate and the variance shrinks, but when close to an optimum,
the positive $\Omega_1^i$ term dominates unless the step size $\eta^2_{S_n}$ is small.
Again, we can ignore the higher-order terms $\order{\Delta_{S_{n}}}$.

\vspace{-0.2cm}
\subsection{Slow worker agnosticism}
\vspace{-0.1cm}
We now explain why allowing fast processors to do extra updates is beneficial.
In Eq.~\ref{eqn:interVar}, we saw that the variance after each sub-epoch $S$ 
depends on the number of datapoints touched $n_i$ and the step size $\eta_{S_n}$.
Let us choose $\eta_{S_{n}}$ small enough so that the variance-decreasing terms dominate, i.e.
{\small
\begin{align}
&2\eta_{S_n}\sum_{i=1}^{w}n_i\Omega_0^iVar(\psi^i_{S_{n}}) +
2\eta_{S_n}\sum_{i=1}^{w}n_i\Omega_0^iCoVar(\psi^i_{S_{n}},\bar{\delta}_{S_n}^i) \nonumber \\
&\quad> \eta_{S_n}^2\sum_{i=1}^{w}n_i\Omega_1^{i}.
\label{eqn:varianceDecreaseCondition}
\end{align}}
This implies $Var(\Psi_{S_{n+1}}) < Var(\Psi_{S_{n}})$.
Hence, using more datapoints $n_i$ decreases the variance of the model state $\psi$,
provided that we choose $\eta_{S_n}$ so that Eq.~\ref{eqn:varianceDecreaseCondition} holds.
This is easy to satisfy: the RHS of Eq.~\ref{eqn:varianceDecreaseCondition} is $\order{\eta_{S_n}^2}$
while the LHS is $\order{\eta_{S_n}}$, so we only need to set $\eta_{S_n}$ small enough.

Let us now take stock:
Eq.~\ref{eqn:varianceDecreaseCondition} tells us that using additional datapoints
in any block can decrease the variance of $\Psi$, while Theorem~\ref{theo:asymptotConverge}
tells us that the algorithm converges asymptotically, regardless of the number of processors
and the number of datapoints assigned to each processor.
Because the algorithm converges, decreasing the variance will only move $\Psi$ towards an optimum,
and therefore it makes sense for faster processors to perform more updates
(with appropriate choice of step size $\eta_{S_n}$) and decrease the variance of $\Psi$,
rather than wait for slow processors to finish.
However, if the step size $\eta_{S_n}$ is set incorrectly, then using
too many updates $n_i$ could increase the variance of $\Psi$.
 
\vspace{-0.3cm}
\section{Experiments}
\vspace{-0.2cm}


\begin{figure*}[t]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\bf Topic Modeling} \\
\hline
Convergence Plots & \# of Topics & \# of Processors & \# of Docs \\
\hline
\includegraphics[width=0.23\textwidth]{results/tm_cvg.pdf} &
\includegraphics[width=0.23\textwidth]{results/tm_rank.pdf} &
\includegraphics[width=0.23\textwidth]{results/tm_cores.pdf} &
\includegraphics[width=0.23\textwidth]{results/tm_data.pdf} \\
\hline
\multicolumn{4}{|c|}{\bf Dictionary Learning} \\
\hline
Convergence Plots & \# of Dictionary Bases & \# of Processors & \# of Images \\
\hline
TODO&&&\\
\hline
\multicolumn{4}{|c|}{\bf Mixed Membership Network Decomposition} \\
\hline
Convergence Plots & \# of Network Roles & \# of Processors & \# of Network Nodes \\
\hline
TODO&&&\\
\hline
\end{tabular}
\caption{\small Convergence (Left) and scalability (in rank, processor cores and data size)
of all methods, on topic modeling, dictionary learning and mixed-membership network decomposition.
The convergence plot reveals the solution trajectory of each method, revealing pathological behavior such as oscillation.
The scalability plots show how each method fares as the problem rank, number of processor cores, and data
size is increased.}
\label{fig:results}
\end{figure*}

We compare our slow-worker agnostic algorithm implemented on Hadoop
to three baselines: (a) DSGD (Gemulla {\it et al.}~\cite{Gemulla:2011:LMF:2020408.2020426})
on Hadoop, (b) PSGD (Zinkevich {\it et al.}~\cite{zinkevich2010parallelized}) on Hadoop,
and (c) our own distributed GraphLab~\cite{low2012distributed} implementation, which
modifies the default Matrix Factorization toolkit in order to regularly project variables and thus maintain constraints
(the graph structure is unaltered). We test all 4 methods
on our 3 latent space models: topic modeling, dictionary learning, and mixed-membership network decomposition.
We also tune all methods to their optimum parameters, before taking results.

Compared to the baselines, our method has several theoretical and practical advantages: unlike DSGD, our algorithm allows fast workers
to continue doing work while waiting for slow workers to synchronize, and unlike PSGD, we explicitly partition the
data/variables/parameters into collections $(i,j) = \{Y_{ij},\pi_{i\cdot},\beta_{\cdot j}\}$ instead of averaging updates
over all data points. Finally, we note that GraphLab is poorly-suited for implementing the simplex and inner-product constraints
required by our topic modeling, dictionary learning and mixed-membership network decomposition models. This is because
the constraints are over entire matrix rows, creating dependencies over all row variables --- which is
especially problematic for topic modeling, because the vocabulary matrix $\beta$ has $V$ columns, and
$V$ can be $\ge 100K$ words in practice. Such long-range dependencies significantly hurt GraphLab's
performance, because it picks sets of variables for updating without regard to the constraints --- whereas
the optimal strategy is to schedule as many dependent elements together as possible.

\vspace{-0.2cm}
\paragraph{Cluster Hardware}
The Hadoop algorithms (ours, DSGD, PSGD) were run on the Yahoo OCC-Y Starlight cluster,
with the following machine specifications: 1x Intel Xeon E5410 @ 2.33Ghz (4 cores per machine),
16GB RAM, 1Gbit Ethernet.
Because the Yahoo Hadoop cluster does not support MPI programs, we ran GraphLab on a
much newer cluster with the following machine specifications: 2x Intel Xeon E5-2450 @ 2.1-2.9GHz
(16 cores per machine), 128GB RAM, 10Gbit Ethernet. As we shall see, our method significantly outperforms
GraphLab, even when the latter is given far more processor cores (in addition to much
more RAM and a much faster network interface).

\vspace{-0.2cm}
\paragraph{Datasets}
For topic modeling, we simulated datasets of various sizes, using the 300K-document
NY Times bag-of-words dataset as a building block. The resulting $\beta$ matrices contain
102,660 columns (words), and between 1.2M to 76.8M rows (documents). The number of
topics was set to $K=25$ unless otherwise stated.

For dictionary learning ... \qirong{todo}

For mixed-membership network decomposition ... \qirong{todo}

\vspace{-0.2cm}
\paragraph{Evaluation Criteria}
Unless otherwise stated, we stop each method when it has reached convergence, defined
as when its objective function does not change by more than $\pm 0.0005$ in successive iterations.

\vspace{-0.2cm}
\paragraph{Results}
All our results are shown and explained in Figure \ref{fig:results}. In general, our method converges faster
than all other baselines, particularly GraphLab, which tends to oscillate because its design
prevents it from applying projections immediately as the variables are updated. The convergence
plots (left) reveal that, in addition to oscillating, GraphLab does not come close to the same
solution quality (measured via the loss function) as our method and the other baselines.
Even when GraphLab is given significantly more cores, it still fails to beat all other methods,
and worse, it ends up consuming most of the RAM on our 128GB machines. In contrast,
our method and the other 2 baselines use Hadoop, and hence store most of their intermediate state to disk
through HDFS (i.e. out-of-core support), allowing them to run on even 16GB machines.

In terms of scalability, our method consistently maintains its lead as the problem rank (number of topics,
dictionary bases, or network roles), number of processors used, or data size increases. Compared to
DSGD and PSGD, we attribute our method's success to having a data/model variable partitioning strategy
that accounts for dependencies (which PSGD lacks), as well as the ability to allow faster workers
to do more work before synchronization (which DSGD lacks). Additionally, the relatively poor performance
of GraphLab highlights the weakness of graph-based platforms at dealing with constrained optimization
problems --- such graph platforms perform optimally when updating small, locally-connected sets of variables,
but falter on models that contain many dependencies (e.g. normalization constraints over 100K variables).

In conclusion, we have developed a slow-worker agnostic algorithm for distributed learning
of latent space models such as topic modeling, dictionary learning and mixed-membership network
decomposition. Our method takes advantage of both
data/model partitioning and extra computational time on fast workers to achieve faster empirical convergence,
and is validated by our theoretical analysis of variance bounds. In terms of empirical performance,
our method combines the best ideas from DSGD (data and model variable partitioning) and PSGD (additional
work before synchronization) to yield performance better than either baseline, and is able to
tackle highly constrained problems, which GraphLab falters on. Although our method is implemented
in Hadoop, we plan to implement it on newly-emerging Big ML systems, such as
parameter servers with bounded consistency~\cite{ho2013scalable}, to reach
higher performance and data scales.

%We implemented all algorithms on Hadoop, and ran them on the topic
%modeling problem (Eq. \ref{eqn:LDA}), using the NIPS (1.2k docs and 12k terms) and
%New York Times datasets (100m docs and 100k terms). We used $K=20$ topics and
%25 Hadoop workers for all experiments.

%\paragraph{Convergence rate on real data.}
%Figure~\ref{fig:convergence_speed} shows convergence (measured by RMSE) vs either
%execution time or the number of data passes (epochs) for all three algorithms.
%For both the NIPS and NYtimes datasets, our algorithm converges to a better RMSE value
%than DSGD or PSGD. The difference is that our algorithm has both a data/model partitioning strategy
%{\it and} allows fast workers to do more work without synchronization, whereas DSGD only has the former,
%while PSGD only has the latter. In Table~\ref{tab:nips_lda}, we show the top words (reweighed by IDF)
%from 10 NIPS topics $\beta_{k,\cdot}$.

%\begin{figure}
%  \begin{center}
%    \includegraphics[width=0.5\textwidth]{fig/synth.pdf}
%  \end{center}
%  \caption{\small Scalability plot for synthetic topic model data}
%\label{fig:scalability}
%\end{figure}

%\paragraph{Scalability experiments.}
%In Figure~\ref{fig:scalability}, we show how our algorithm scales with data
%that has increasing nonzeros (simulated from a topic model). We achive a roughly linear
%increase in runtime versus data size, demonstrating that our implementation scales linearly to
%larger datasets (without introducing additional overheads or costs due to the larger data).


%\begin{table*}[t]
%\centering
%\scriptsize
%    \setlength{\tabcolsep}{4.5pt}
%\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
%\hline
%Topic 1 & Topic 2 & Topic 3 & Topic 4 & Topic 5 & Topic 6 & Topic 7 & Topic 8 & Topic 9 & Topic 10\\
%\hline
%micchelli & kirchoff & iris & texture & maintained & foveal & neglect & microchip & microchip & indexed \\
%phased & endmember & chou & creasing & cknn & cotterill & rho & ecc & sondik & originate \\
%alike & sander & consideration & descending & spotlight & endmember & micchelli & stabilizing & unfolded & release \\
%umli & fre & retinotopic & locate & subgoal & retinotopic & cepstrum & manifold & eeg & occurred \\
%saliencies & triangulated & falling & sociated & treatment & easiest & interference & anton & shortest & multicollinearity \\
%spotlight & sep & pca & perceptual & robot & bbn & indiveri & tour & resembling & multitask \\
%gammon & assumption & eval & saund & hitting & chi & sharpe & unconditional & inhibition & dendritic \\
%sdti & hole & winner & onset & normalization & lesson & leaning & reside & cell & helpful \\
%viewer & suzanna & segmentation & lrta & fish & refractory & packet & patient & nearest & depth \\
%riken & denote & cole & mary & searching & chip & sensitivity & arethor & neighbor & partic \\
%\hline
%\end{tabular}
%\caption{\small A selection of topics from the NIPS corpus.
%}
%\label{tab:nips_lda}
%\end{table*}

%\subsubsection*{Acknowledgements}

{
\small
\bibliography{biblio}
\bibliographystyle{plain}
}

\end{document}
